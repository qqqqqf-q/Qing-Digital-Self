<!DOCTYPE html>
<html lang="en-US" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Configure the Environment | Qing-Digital-Self</title>
    <meta name="description" content="Qing's digital avatar with complete setup tutorial">
    <meta name="generator" content="VitePress v1.6.3">
    <link rel="preload stylesheet" href="/Qing-Digital-Self/assets/style.saCJ7iHT.css" as="style">
    <link rel="preload stylesheet" href="/Qing-Digital-Self/vp-icons.css" as="style">
    
    <script type="module" src="/Qing-Digital-Self/assets/app.D1rCpgyH.js"></script>
    <link rel="preload" href="/Qing-Digital-Self/assets/inter-roman-latin.Di8DUHzh.woff2" as="font" type="font/woff2" crossorigin="">
    <link rel="modulepreload" href="/Qing-Digital-Self/assets/chunks/theme.CB5kl2g0.js">
    <link rel="modulepreload" href="/Qing-Digital-Self/assets/chunks/framework.DUP9kEI5.js">
    <link rel="modulepreload" href="/Qing-Digital-Self/assets/en_guide_fine-tune-model.md.Q1pOqDtF.lean.js">
    <script id="check-dark-mode">(()=>{const e=localStorage.getItem("vitepress-theme-appearance")||"auto",a=window.matchMedia("(prefers-color-scheme: dark)").matches;(!e||e==="auto"?a:e==="dark")&&document.documentElement.classList.add("dark")})();</script>
    <script id="check-mac-os">document.documentElement.classList.toggle("mac",/Mac|iPhone|iPod|iPad/i.test(navigator.platform));</script>
  </head>
  <body>
    <div id="app"><div class="Layout" data-v-d8b57b2d><!--[--><!--]--><!--[--><span tabindex="-1" data-v-fcbfc0e0></span><a href="#VPContent" class="VPSkipLink visually-hidden" data-v-fcbfc0e0>Skip to content</a><!--]--><!----><header class="VPNav" data-v-d8b57b2d data-v-7ad780c2><div class="VPNavBar" data-v-7ad780c2 data-v-9fd4d1dd><div class="wrapper" data-v-9fd4d1dd><div class="container" data-v-9fd4d1dd><div class="title" data-v-9fd4d1dd><div class="VPNavBarTitle has-sidebar" data-v-9fd4d1dd data-v-9f43907a><a class="title" href="/Qing-Digital-Self/en/" data-v-9f43907a><!--[--><!--]--><!----><span data-v-9f43907a>Qing-Digital-Self</span><!--[--><!--]--></a></div></div><div class="content" data-v-9fd4d1dd><div class="content-body" data-v-9fd4d1dd><!--[--><!--]--><div class="VPNavBarSearch search" data-v-9fd4d1dd><!----></div><nav aria-labelledby="main-nav-aria-label" class="VPNavBarMenu menu" data-v-9fd4d1dd data-v-afb2845e><span id="main-nav-aria-label" class="visually-hidden" data-v-afb2845e> Main Navigation </span><!--[--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/Qing-Digital-Self/en/" tabindex="0" data-v-afb2845e data-v-815115f5><!--[--><span data-v-815115f5>Home</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/Qing-Digital-Self/en/guide/index.html" tabindex="0" data-v-afb2845e data-v-815115f5><!--[--><span data-v-815115f5>Quick Start</span><!--]--></a><!--]--><!--]--></nav><div class="VPFlyout VPNavBarTranslations translations" data-v-9fd4d1dd data-v-acee064b data-v-bfe7971f><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="Change language" data-v-bfe7971f><span class="text" data-v-bfe7971f><span class="vpi-languages option-icon" data-v-bfe7971f></span><!----><span class="vpi-chevron-down text-icon" data-v-bfe7971f></span></span></button><div class="menu" data-v-bfe7971f><div class="VPMenu" data-v-bfe7971f data-v-20ed86d6><!----><!--[--><!--[--><div class="items" data-v-acee064b><p class="title" data-v-acee064b>English</p><!--[--><div class="VPMenuLink" data-v-acee064b data-v-7eeeb2dc><a class="VPLink link" href="/Qing-Digital-Self/guide/fine-tune-model.html" data-v-7eeeb2dc><!--[--><span data-v-7eeeb2dc>中文</span><!--]--></a></div><!--]--></div><!--]--><!--]--></div></div></div><div class="VPNavBarAppearance appearance" data-v-9fd4d1dd data-v-3f90c1a5><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-3f90c1a5 data-v-be9742d9 data-v-b4ccac88><span class="check" data-v-b4ccac88><span class="icon" data-v-b4ccac88><!--[--><span class="vpi-sun sun" data-v-be9742d9></span><span class="vpi-moon moon" data-v-be9742d9></span><!--]--></span></span></button></div><div class="VPSocialLinks VPNavBarSocialLinks social-links" data-v-9fd4d1dd data-v-ef6192dc data-v-e71e869c><!--[--><a class="VPSocialLink no-icon" href="https://github.com/qqqqqf-q/Qing-Digital-Self" aria-label="github" target="_blank" rel="noopener" data-v-e71e869c data-v-60a9a2d3><span class="vpi-social-github"></span></a><a class="VPSocialLink no-icon" href="https.x.com/qqqqqf5" aria-label target="_blank" rel="noopener" data-v-e71e869c data-v-60a9a2d3><svg role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><title>X</title><path d="M18.901 1.153h3.68l-8.04 9.19L24 22.846h-7.406l-5.8-7.584-6.638 7.584H.474l8.6-9.83L0 1.154h7.594l5.243 7.184L18.901 1.153Zm-1.653 19.57h2.608L6.852 3.24H4.21l13.038 17.484Z"/></svg></a><!--]--></div><div class="VPFlyout VPNavBarExtra extra" data-v-9fd4d1dd data-v-f953d92f data-v-bfe7971f><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-bfe7971f><span class="vpi-more-horizontal icon" data-v-bfe7971f></span></button><div class="menu" data-v-bfe7971f><div class="VPMenu" data-v-bfe7971f data-v-20ed86d6><!----><!--[--><!--[--><div class="group translations" data-v-f953d92f><p class="trans-title" data-v-f953d92f>English</p><!--[--><div class="VPMenuLink" data-v-f953d92f data-v-7eeeb2dc><a class="VPLink link" href="/Qing-Digital-Self/guide/fine-tune-model.html" data-v-7eeeb2dc><!--[--><span data-v-7eeeb2dc>中文</span><!--]--></a></div><!--]--></div><div class="group" data-v-f953d92f><div class="item appearance" data-v-f953d92f><p class="label" data-v-f953d92f>Appearance</p><div class="appearance-action" data-v-f953d92f><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-f953d92f data-v-be9742d9 data-v-b4ccac88><span class="check" data-v-b4ccac88><span class="icon" data-v-b4ccac88><!--[--><span class="vpi-sun sun" data-v-be9742d9></span><span class="vpi-moon moon" data-v-be9742d9></span><!--]--></span></span></button></div></div></div><div class="group" data-v-f953d92f><div class="item social-links" data-v-f953d92f><div class="VPSocialLinks social-links-list" data-v-f953d92f data-v-e71e869c><!--[--><a class="VPSocialLink no-icon" href="https://github.com/qqqqqf-q/Qing-Digital-Self" aria-label="github" target="_blank" rel="noopener" data-v-e71e869c data-v-60a9a2d3><span class="vpi-social-github"></span></a><a class="VPSocialLink no-icon" href="https.x.com/qqqqqf5" aria-label target="_blank" rel="noopener" data-v-e71e869c data-v-60a9a2d3><svg role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><title>X</title><path d="M18.901 1.153h3.68l-8.04 9.19L24 22.846h-7.406l-5.8-7.584-6.638 7.584H.474l8.6-9.83L0 1.154h7.594l5.243 7.184L18.901 1.153Zm-1.653 19.57h2.608L6.852 3.24H4.21l13.038 17.484Z"/></svg></a><!--]--></div></div></div><!--]--><!--]--></div></div></div><!--[--><!--]--><button type="button" class="VPNavBarHamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="VPNavScreen" data-v-9fd4d1dd data-v-6bee1efd><span class="container" data-v-6bee1efd><span class="top" data-v-6bee1efd></span><span class="middle" data-v-6bee1efd></span><span class="bottom" data-v-6bee1efd></span></span></button></div></div></div></div><div class="divider" data-v-9fd4d1dd><div class="divider-line" data-v-9fd4d1dd></div></div></div><!----></header><div class="VPLocalNav has-sidebar empty" data-v-d8b57b2d data-v-2488c25a><div class="container" data-v-2488c25a><button class="menu" aria-expanded="false" aria-controls="VPSidebarNav" data-v-2488c25a><span class="vpi-align-left menu-icon" data-v-2488c25a></span><span class="menu-text" data-v-2488c25a>Menu</span></button><div class="VPLocalNavOutlineDropdown" style="--vp-vh:0px;" data-v-2488c25a data-v-6b867909><button data-v-6b867909>Return to top</button><!----></div></div></div><aside class="VPSidebar" data-v-d8b57b2d data-v-42c4c606><div class="curtain" data-v-42c4c606></div><nav class="nav" id="VPSidebarNav" aria-labelledby="sidebar-aria-label" tabindex="-1" data-v-42c4c606><span class="visually-hidden" id="sidebar-aria-label" data-v-42c4c606> Sidebar Navigation </span><!--[--><!--]--><!--[--><div class="no-transition group" data-v-51288d80><section class="VPSidebarItem level-0" data-v-51288d80 data-v-0009425e><div class="item" role="button" tabindex="0" data-v-0009425e><div class="indicator" data-v-0009425e></div><h2 class="text" data-v-0009425e>Getting Started</h2><!----></div><div class="items" data-v-0009425e><!--[--><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/Qing-Digital-Self/en/guide/index.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Introduction</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><div class="no-transition group" data-v-51288d80><section class="VPSidebarItem level-0 has-active" data-v-51288d80 data-v-0009425e><div class="item" role="button" tabindex="0" data-v-0009425e><div class="indicator" data-v-0009425e></div><h2 class="text" data-v-0009425e>Quick Start</h2><!----></div><div class="items" data-v-0009425e><!--[--><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/Qing-Digital-Self/en/guide/prepare-data.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>1. QQ/TG/Other Data Acquisition</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/Qing-Digital-Self/en/guide/clean-data.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>2. Clean Data</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/Qing-Digital-Self/en/guide/mix-data.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>3. (Optional) Mix Data</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/Qing-Digital-Self/en/guide/prepare-model.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>4. Prepare Model</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/Qing-Digital-Self/en/guide/fine-tune-model.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>5. Fine-tune Model</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/Qing-Digital-Self/en/guide/run-full-model.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>6. (Recommended to Skip) Run Full Model After Fine-tuning</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/Qing-Digital-Self/en/guide/convert-model.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>7. Convert GGUF and Quantize Model</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/Qing-Digital-Self/en/guide/run-model.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>8. Run Model</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><div class="no-transition group" data-v-51288d80><section class="VPSidebarItem level-0" data-v-51288d80 data-v-0009425e><div class="item" role="button" tabindex="0" data-v-0009425e><div class="indicator" data-v-0009425e></div><h2 class="text" data-v-0009425e>Extra</h2><!----></div><div class="items" data-v-0009425e><!--[--><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/Qing-Digital-Self/en/guide/change-logger-language.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Change Logger Language</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/Qing-Digital-Self/en/guide/fine-tune-model-exp.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Fine-tuning Experience with Different Models (Qwen, Llama, Gemma, etc.)</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/Qing-Digital-Self/en/guide/save-vram.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>VRAM Optimization</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><div class="no-transition group" data-v-51288d80><section class="VPSidebarItem level-0" data-v-51288d80 data-v-0009425e><div class="item" role="button" tabindex="0" data-v-0009425e><div class="indicator" data-v-0009425e></div><h2 class="text" data-v-0009425e>Summary</h2><!----></div><div class="items" data-v-0009425e><!--[--><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/Qing-Digital-Self/en/guide/summary.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>9. summary</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><!--]--><!--[--><!--]--></nav></aside><div class="VPContent has-sidebar" id="VPContent" data-v-d8b57b2d data-v-9a6c75ad><div class="VPDoc has-sidebar has-aside" data-v-9a6c75ad data-v-e6f2a212><!--[--><!--]--><div class="container" data-v-e6f2a212><div class="aside" data-v-e6f2a212><div class="aside-curtain" data-v-e6f2a212></div><div class="aside-container" data-v-e6f2a212><div class="aside-content" data-v-e6f2a212><div class="VPDocAside" data-v-e6f2a212 data-v-cb998dce><!--[--><!--]--><!--[--><!--]--><nav aria-labelledby="doc-outline-aria-label" class="VPDocAsideOutline" data-v-cb998dce data-v-f610f197><div class="content" data-v-f610f197><div class="outline-marker" data-v-f610f197></div><div aria-level="2" class="outline-title" id="doc-outline-aria-label" role="heading" data-v-f610f197>On this page</div><ul class="VPDocOutlineItem root" data-v-f610f197 data-v-53c99d69><!--[--><!--]--></ul></div></nav><!--[--><!--]--><div class="spacer" data-v-cb998dce></div><!--[--><!--]--><!----><!--[--><!--]--><!--[--><!--]--></div></div></div></div><div class="content" data-v-e6f2a212><div class="content-container" data-v-e6f2a212><!--[--><!--]--><main class="main" data-v-e6f2a212><div style="position:relative;" class="vp-doc _Qing-Digital-Self_en_guide_fine-tune-model" data-v-e6f2a212><div><h2 id="fine-tuning-the-model" tabindex="-1">Fine-tuning the Model <a class="header-anchor" href="#fine-tuning-the-model" aria-label="Permalink to &quot;Fine-tuning the Model&quot;">​</a></h2><blockquote><p>It is recommended to use a CPU with high single-core performance for fine-tuning. Otherwise, there may be a CPU bottleneck (cause not yet identified — PRs to fix are welcome).</p></blockquote><h2 id="before-you-begin-—-environment-setup" tabindex="-1">Before You Begin — Environment Setup <a class="header-anchor" href="#before-you-begin-—-environment-setup" aria-label="Permalink to &quot;Before You Begin — Environment Setup&quot;">​</a></h2><blockquote><p>It&#39;s simple, don&#39;t worry.</p></blockquote><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">git</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> clone</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> https://github.com/qqqqqf-q/Qing-Digital-Self.git</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --depth</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 1</span></span></code></pre></div><p>Or use a mirror (China mainland acceleration):</p><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">git</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> clone</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> https://hk.gh-proxy.com/https://github.com/qqqqqf-q/Qing-Digital-Self.git</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">  --depth</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 1</span></span></code></pre></div><h1 id="configure-the-environment" tabindex="-1">Configure the Environment <a class="header-anchor" href="#configure-the-environment" aria-label="Permalink to &quot;Configure the Environment&quot;">​</a></h1><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">python3</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> environment/setup_env.py</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --install</span></span></code></pre></div><p>Just follow the default process. Installation includes built-in checks. You can also use:</p><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">python3</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> environment/setup_env.py</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --check</span></span></code></pre></div><p>To check the environment.</p><p>If you encounter issues with Unsloth installation, please install it manually. First run the following command:</p><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">wget</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -qO-</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> https://raw.githubusercontent.com/unslothai/unsloth/main/unsloth/_auto_install.py</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> |</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> python</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> -</span></span></code></pre></div><p>It will output a pip command — copy it and run it in your shell. For example:</p><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">pip</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> install</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --upgrade</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> pip</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> &amp;&amp; </span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">pip</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> install</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;unsloth[cu126-ampere-torch270] @ git+https://github.com/unslothai/unsloth.git&quot;</span></span></code></pre></div><p>If you encounter issues with flash attention installation, You can try visiting <a href="https://github.com/Dao-AILab/flash-attention/releases/" target="_blank" rel="noreferrer">this GitHub repository</a> To find the offline installation package you need (this doesn&#39;t require compilation and will be much faster). Commands are similar to:</p><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">wget</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> https://github.com/Dao-AILab/flash-attention/releases/download/v2.8.3/flash_attn-2.8.3+cu12torch2.4cxx11abiTRUE-cp312-cp312-linux_x86_64.whl&#39;</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">pip install flash_attn-2.8.3+cu12torch2.4cxx11abiTRUE-cp312-cp312-linux_x86_64.whl</span></span></code></pre></div><hr><h1 id="now-the-actual-fine-tuning" tabindex="-1">Now the Actual Fine-tuning <a class="header-anchor" href="#now-the-actual-fine-tuning" aria-label="Permalink to &quot;Now the Actual Fine-tuning&quot;">​</a></h1><blockquote><p>Parameters can actually be left empty during testing, as defaults are provided. By default, it seems to use 8-bit quantization (this needs modification).</p></blockquote><p>Run the fine-tuning script:</p><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">python</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> run_finetune.py</span></span></code></pre></div><h3 id="model-related-parameters-4-columns-scroll-to-view-all" tabindex="-1">Model-related Parameters (4 columns, scroll to view all) <a class="header-anchor" href="#model-related-parameters-4-columns-scroll-to-view-all" aria-label="Permalink to &quot;Model-related Parameters (4 columns, scroll to view all)&quot;">​</a></h3><table tabindex="0"><thead><tr><th>Parameter Name</th><th>Type</th><th>Default Value</th><th>Description</th></tr></thead><tbody><tr><td><code>--repo_id</code></td><td>str</td><td><code>&#39;Qwen/Qwen3-30B-A3B-Instruct-2507&#39;</code></td><td>HF repository ID</td></tr><tr><td><code>--local_dir</code></td><td>str</td><td><code>&#39;qwen3-30b-a3b-instruct&#39;</code></td><td>Local model directory</td></tr><tr><td><code>--use_unsloth</code></td><td>str</td><td><code>&#39;false&#39;</code></td><td>Whether to use Unsloth</td></tr><tr><td><code>--use_qlora</code></td><td>str</td><td><code>&#39;true&#39;</code></td><td>Whether to use QLoRA</td></tr><tr><td><code>--data_path</code></td><td>str</td><td><code>&#39;training_data.jsonl&#39;</code></td><td>Training data path</td></tr><tr><td><code>--eval_data_path</code></td><td>str</td><td><code>None</code></td><td>Evaluation data path</td></tr><tr><td><code>--max_samples</code></td><td>str</td><td><code>None</code></td><td>Maximum number of training samples</td></tr><tr><td><code>--max_eval_samples</code></td><td>str</td><td><code>None</code></td><td>Maximum number of evaluation samples</td></tr><tr><td><code>--model_max_length</code></td><td>str</td><td><code>&#39;2048&#39;</code></td><td>Maximum sequence length</td></tr><tr><td><code>--output_dir</code></td><td>str</td><td><code>&#39;finetune/models/qwen3-30b-a3b-qlora&#39;</code></td><td>Output directory</td></tr><tr><td><code>--seed</code></td><td>str</td><td><code>&#39;42&#39;</code></td><td>Random seed</td></tr><tr><td><code>--per_device_train_batch_size</code></td><td>str</td><td><code>&#39;1&#39;</code></td><td>Per-device training batch size</td></tr><tr><td><code>--per_device_eval_batch_size</code></td><td>str</td><td><code>&#39;1&#39;</code></td><td>Per-device evaluation batch size</td></tr><tr><td><code>--gradient_accumulation_steps</code></td><td>str</td><td><code>&#39;16&#39;</code></td><td>Gradient accumulation steps</td></tr><tr><td><code>--learning_rate</code></td><td>str</td><td><code>&#39;2e-4&#39;</code></td><td>Learning rate</td></tr><tr><td><code>--num_train_epochs</code></td><td>str</td><td><code>&#39;3&#39;</code></td><td>Number of training epochs</td></tr><tr><td><code>--max_steps</code></td><td>str</td><td><code>&#39;-1&#39;</code></td><td>Maximum steps (-1 means unlimited)</td></tr><tr><td><code>--lora_r</code></td><td>str</td><td><code>&#39;16&#39;</code></td><td>LoRA rank</td></tr><tr><td><code>--lora_alpha</code></td><td>str</td><td><code>&#39;32&#39;</code></td><td>LoRA alpha</td></tr><tr><td><code>--lora_dropout</code></td><td>str</td><td><code>&#39;0.05&#39;</code></td><td>LoRA dropout rate</td></tr><tr><td><code>--target_modules</code></td><td>str</td><td><code>&#39;too long, check file&#39;</code></td><td>LoRA target modules</td></tr><tr><td><code>--weight_decay</code></td><td>str</td><td><code>&#39;0.0&#39;</code></td><td>Weight decay</td></tr><tr><td><code>--moe_enable</code></td><td>str</td><td><code>&#39;false&#39;</code></td><td>Whether to enable MoE injection logic</td></tr><tr><td><code>--moe_lora_scope</code></td><td>str</td><td><code>&#39;expert_only&#39;</code></td><td>LoRA injection scope</td></tr><tr><td><code>--moe_expert_patterns</code></td><td>str</td><td><code>&#39;too long to include here, check file&#39;</code></td><td>Expert linear layer patterns</td></tr><tr><td><code>--moe_router_patterns</code></td><td>str</td><td><code>&#39;markdown would parse it, check file&#39;</code></td><td>Router/gating linear layer patterns</td></tr><tr><td><code>--moe_max_experts_lora</code></td><td>str</td><td><code>&#39;-1&#39;</code></td><td>Max number of LoRA experts per layer</td></tr><tr><td><code>--moe_dry_run</code></td><td>str</td><td><code>&#39;false&#39;</code></td><td>Whether to do a dry run only</td></tr><tr><td><code>--load_precision</code></td><td>str</td><td><code>&#39;fp16&#39;</code></td><td>Model load precision: <code>int8</code> / <code>int4</code> / <code>fp16</code></td></tr><tr><td><code>--logging_steps</code></td><td>str</td><td><code>&#39;1&#39;</code></td><td>Logging interval (steps)</td></tr><tr><td><code>--eval_steps</code></td><td>str</td><td><code>&#39;50&#39;</code></td><td>Evaluation interval (steps)</td></tr><tr><td><code>--save_steps</code></td><td>str</td><td><code>&#39;200&#39;</code></td><td>Model save interval (steps)</td></tr><tr><td><code>--save_total_limit</code></td><td>str</td><td><code>&#39;2&#39;</code></td><td>Maximum number of saved models</td></tr><tr><td><code>--warmup_ratio</code></td><td>str</td><td><code>&#39;0.05&#39;</code></td><td>Learning rate warmup ratio</td></tr><tr><td><code>--lr_scheduler_type</code></td><td>str</td><td><code>&#39;cosine&#39;</code></td><td>Learning rate scheduler type</td></tr><tr><td><code>--resume_from_checkpoint</code></td><td>str</td><td><code>None</code></td><td>Path to resume training from checkpoint</td></tr><tr><td><code>--no-gradient_checkpointing</code></td><td>flag</td><td><code>False</code></td><td>Disable gradient checkpointing (enable by adding this flag)</td></tr><tr><td><code>--no-merge_and_save</code></td><td>flag</td><td><code>False</code></td><td>Do not merge and save model (enable by adding this flag)</td></tr><tr><td><code>--fp16</code></td><td>str</td><td><code>&#39;true&#39;</code></td><td>Whether to use fp16</td></tr><tr><td><code>--optim</code></td><td>str</td><td><code>&#39;adamw_torch_fused&#39;</code></td><td>Optimizer name</td></tr><tr><td><code>--dataloader_pin_memory</code></td><td>str</td><td><code>&#39;false&#39;</code></td><td>Whether to pin DataLoader memory</td></tr><tr><td><code>--dataloader_num_workers</code></td><td>str</td><td><code>&#39;0&#39;</code></td><td>Number of DataLoader workers</td></tr><tr><td><code>--dataloader_prefetch_factor</code></td><td>str</td><td><code>&#39;2&#39;</code></td><td>DataLoader prefetch factor</td></tr><tr><td><code>--use_flash_attention_2</code></td><td>str</td><td><code>&#39;false&#39;</code></td><td>Use FlashAttention2 (not effective for Unsloth) (enable by adding this flag)</td></tr></tbody></table><hr><blockquote><p>The parameters are still quite complex — it’s best to consult an AI for help. Below is an example of fine-tuning <code>qwen2.5-7b-instruct</code> on an RTX 4090:</p></blockquote><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">python3</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> run_finetune.py</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --output_dir</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> /root/autodl-fs/qwen2.5-7b-qing-v1</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --local_dir</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> ./model/Qwen2.5-7B-Instruct</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --data_path</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> ./dataset/sft.jsonl</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --use_qlora</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> true</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --lora_dropout</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 0.1</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --num_train_epochs</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 8</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --per_device_train_batch_size</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 4</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --per_device_eval_batch_size</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 4</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --gradient_accumulation_steps</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 8</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --learning_rate</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> 2e-5</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --lr_scheduler</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> cosine</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --logging_steps</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 5</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --eval_steps</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 40</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --save_steps</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 200</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --warmup_ratio</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 0.05</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --dataloader_num_workers</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 16</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --fp16</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> true</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --use_unsloth</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> true</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --no-gradient_checkpointing</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">  --load_precision</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> int8</span></span></code></pre></div><h3 id="evaluation-set-not-working" tabindex="-1">Evaluation Set Not Working <a class="header-anchor" href="#evaluation-set-not-working" aria-label="Permalink to &quot;Evaluation Set Not Working&quot;">​</a></h3><ul><li>Check that <code>--eval_data_path</code> is correct.</li><li>Ensure evaluation data format matches training data.</li><li>Look for console output saying “no evaluation data path provided”.</li></ul><h3 id="gpu-out-of-memory" tabindex="-1">GPU Out of Memory <a class="header-anchor" href="#gpu-out-of-memory" aria-label="Permalink to &quot;GPU Out of Memory&quot;">​</a></h3><ul><li>Reduce <code>--per_device_eval_batch_size</code>.</li><li>Reduce <code>--max_eval_samples</code>.</li><li>Increase the <code>--eval_steps</code> interval.</li></ul><h3 id="dev-notes" tabindex="-1">Dev Notes <a class="header-anchor" href="#dev-notes" aria-label="Permalink to &quot;Dev Notes&quot;">​</a></h3><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">python3</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> cli.py</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> train</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> start</span></span></code></pre></div><p>This parameter still seems unusable, with many bugs that need fixing.</p></div></div></main><footer class="VPDocFooter" data-v-e6f2a212 data-v-1bcd8184><!--[--><!--]--><!----><nav class="prev-next" aria-labelledby="doc-footer-aria-label" data-v-1bcd8184><span class="visually-hidden" id="doc-footer-aria-label" data-v-1bcd8184>Pager</span><div class="pager" data-v-1bcd8184><a class="VPLink link pager-link prev" href="/Qing-Digital-Self/en/guide/prepare-model.html" data-v-1bcd8184><!--[--><span class="desc" data-v-1bcd8184>Previous page</span><span class="title" data-v-1bcd8184>4. Prepare Model</span><!--]--></a></div><div class="pager" data-v-1bcd8184><a class="VPLink link pager-link next" href="/Qing-Digital-Self/en/guide/run-full-model.html" data-v-1bcd8184><!--[--><span class="desc" data-v-1bcd8184>Next page</span><span class="title" data-v-1bcd8184>6. (Recommended to Skip) Run Full Model After Fine-tuning</span><!--]--></a></div></nav></footer><!--[--><!--]--></div></div></div><!--[--><!--]--></div></div><!----><!--[--><!--]--></div></div>
    <script>window.__VP_HASH_MAP__=JSON.parse("{\"en_guide_change-logger-language.md\":\"V3sO9rI4\",\"en_guide_clean-data.md\":\"Bx8PWuvn\",\"en_guide_convert-model.md\":\"Ct8-b_O3\",\"en_guide_fine-tune-model-exp.md\":\"DJj3sgLC\",\"en_guide_fine-tune-model.md\":\"Q1pOqDtF\",\"en_guide_index.md\":\"09IUFwXB\",\"en_guide_media-chat-data.md\":\"8G-QBXri\",\"en_guide_mix-data.md\":\"--GkcxzO\",\"en_guide_prepare-data.md\":\"DiZvDIlX\",\"en_guide_prepare-model.md\":\"Bltb3cm9\",\"en_guide_run-full-model.md\":\"BSMoeFhI\",\"en_guide_run-model.md\":\"Bw-ni02b\",\"en_guide_save-vram.md\":\"CZln7i03\",\"en_guide_summary.md\":\"dRwu-xWw\",\"en_index.md\":\"C_vA3p6U\",\"guide_change-logger-language.md\":\"CpOF6gL7\",\"guide_clean-data.md\":\"DxBTT7_o\",\"guide_convert-model.md\":\"yD3VhmRz\",\"guide_fine-tune-model-exp.md\":\"BosNsuHj\",\"guide_fine-tune-model.md\":\"D5bg_Glq\",\"guide_index.md\":\"rYyXbAPJ\",\"guide_media-chat-data.md\":\"CbL7cPQu\",\"guide_mix-data.md\":\"CbQJ0N42\",\"guide_prepare-data.md\":\"C7d9gmOG\",\"guide_prepare-model.md\":\"DDeZJUf5\",\"guide_run-full-model.md\":\"BUFp91Z2\",\"guide_run-model.md\":\"PCe7hpah\",\"guide_save-vram.md\":\"CoXg1WGx\",\"guide_summary.md\":\"BcXDteQC\",\"index.md\":\"DPh3RsME\"}");window.__VP_SITE_DATA__=JSON.parse("{\"lang\":\"en-US\",\"dir\":\"ltr\",\"title\":\"VitePress\",\"description\":\"A VitePress site\",\"base\":\"/Qing-Digital-Self/\",\"head\":[],\"router\":{\"prefetchLinks\":true},\"appearance\":true,\"themeConfig\":{\"socialLinks\":[{\"icon\":\"github\",\"link\":\"https://github.com/qqqqqf-q/Qing-Digital-Self\"},{\"icon\":{\"svg\":\"<svg role=\\\"img\\\" viewBox=\\\"0 0 24 24\\\" xmlns=\\\"http://www.w3.org/2000/svg\\\"><title>X</title><path d=\\\"M18.901 1.153h3.68l-8.04 9.19L24 22.846h-7.406l-5.8-7.584-6.638 7.584H.474l8.6-9.83L0 1.154h7.594l5.243 7.184L18.901 1.153Zm-1.653 19.57h2.608L6.852 3.24H4.21l13.038 17.484Z\\\"/></svg>\"},\"link\":\"https.x.com/qqqqqf5\"}]},\"locales\":{\"root\":{\"label\":\"中文\",\"lang\":\"zh-CN\",\"title\":\"Qing-Digital-Self\",\"description\":\"清凤的数字分身,并且包含了搭建教程\",\"themeConfig\":{\"nav\":[{\"text\":\"开始\",\"link\":\"/\"},{\"text\":\"快速上手\",\"link\":\"/guide/index\"}],\"sidebar\":[{\"text\":\"开始\",\"items\":[{\"text\":\"简介\",\"link\":\"/guide/index\"}]},{\"text\":\"快速上手\",\"items\":[{\"text\":\"1. QQ/TG/WX其他数据的获取\",\"link\":\"/guide/prepare-data\"},{\"text\":\"2. 清洗数据\",\"link\":\"/guide/clean-data\"},{\"text\":\"3. (可选) 混合数据\",\"link\":\"/guide/mix-data\"},{\"text\":\"4. 准备模型\",\"link\":\"/guide/prepare-model\"},{\"text\":\"5. 微调模型\",\"link\":\"/guide/fine-tune-model\"},{\"text\":\"6. (建议跳过)微调后直接运行全量模型\",\"link\":\"/guide/run-full-model\"},{\"text\":\"7. 转换GUFF和量化模型\",\"link\":\"/guide/convert-model\"},{\"text\":\"8. 运行模型\",\"link\":\"/guide/run-model\"}]},{\"text\":\"补充\",\"items\":[{\"text\":\"修改Logger的语言\",\"link\":\"/guide/change-logger-language\"},{\"text\":\"不同种类模型的微调经验(例如Qwen,Llama,Gemma等)\",\"link\":\"/guide/fine-tune-model-exp\"},{\"text\":\"节省显存\",\"link\":\"/guide/save-vram\"}]},{\"text\":\"总结\",\"items\":[{\"text\":\"9. 总结\",\"link\":\"/guide/summary\"}]}]}},\"en\":{\"label\":\"English\",\"lang\":\"en-US\",\"title\":\"Qing-Digital-Self\",\"description\":\"Qing's digital avatar with complete setup tutorial\",\"themeConfig\":{\"nav\":[{\"text\":\"Home\",\"link\":\"/en/\"},{\"text\":\"Quick Start\",\"link\":\"/en/guide/index\"}],\"sidebar\":[{\"text\":\"Getting Started\",\"items\":[{\"text\":\"Introduction\",\"link\":\"/en/guide/index\"}]},{\"text\":\"Quick Start\",\"items\":[{\"text\":\"1. QQ/TG/Other Data Acquisition\",\"link\":\"/en/guide/prepare-data\"},{\"text\":\"2. Clean Data\",\"link\":\"/en/guide/clean-data\"},{\"text\":\"3. (Optional) Mix Data\",\"link\":\"/en/guide/mix-data\"},{\"text\":\"4. Prepare Model\",\"link\":\"/en/guide/prepare-model\"},{\"text\":\"5. Fine-tune Model\",\"link\":\"/en/guide/fine-tune-model\"},{\"text\":\"6. (Recommended to Skip) Run Full Model After Fine-tuning\",\"link\":\"/en/guide/run-full-model\"},{\"text\":\"7. Convert GGUF and Quantize Model\",\"link\":\"/en/guide/convert-model\"},{\"text\":\"8. Run Model\",\"link\":\"/en/guide/run-model\"}]},{\"text\":\"Extra\",\"items\":[{\"text\":\"Change Logger Language\",\"link\":\"/en/guide/change-logger-language\"},{\"text\":\"Fine-tuning Experience with Different Models (Qwen, Llama, Gemma, etc.)\",\"link\":\"/en/guide/fine-tune-model-exp\"},{\"text\":\"VRAM Optimization\",\"link\":\"/en/guide/save-vram\"}]},{\"text\":\"Summary\",\"items\":[{\"text\":\"9. summary\",\"link\":\"/en/guide/summary\"}]}]}}},\"scrollOffset\":134,\"cleanUrls\":false}");</script>
    
  </body>
</html>