import{_ as d,c as e,o as s,ae as i}from"./chunks/framework.DUP9kEI5.js";const u=JSON.parse('{"title":"Extra Chapter: Fine-tuning the OpenAI OSS Model","description":"","frontmatter":{},"headers":[],"relativePath":"en/guide/fine-tune-openai-oss-model.md","filePath":"en/guide/fine-tune-openai-oss-model.md"}'),o={name:"en/guide/fine-tune-openai-oss-model.md"};function a(r,t,n,c,h,l){return s(),e("div",null,t[0]||(t[0]=[i('<h1 id="extra-chapter-fine-tuning-the-openai-oss-model" tabindex="-1">Extra Chapter: Fine-tuning the OpenAI OSS Model <a class="header-anchor" href="#extra-chapter-fine-tuning-the-openai-oss-model" aria-label="Permalink to &quot;Extra Chapter: Fine-tuning the OpenAI OSS Model&quot;">​</a></h1><h2 id="this-guide-has-not-been-tested-in-practice-my-3080-doesn-t-have-enough-vram-to-run-it" tabindex="-1">This guide has <strong>not</strong> been tested in practice (my 3080 doesn’t have enough VRAM to run it) <a class="header-anchor" href="#this-guide-has-not-been-tested-in-practice-my-3080-doesn-t-have-enough-vram-to-run-it" aria-label="Permalink to &quot;This guide has **not** been tested in practice (my 3080 doesn’t have enough VRAM to run it)&quot;">​</a></h2><h3 id="if-you-can-test-it-please-share-your-results-or-bugs-in-the-issues-section" tabindex="-1">If you can test it, please share your results or bugs in the Issues section <a class="header-anchor" href="#if-you-can-test-it-please-share-your-results-or-bugs-in-the-issues-section" aria-label="Permalink to &quot;If you can test it, please share your results or bugs in the Issues section&quot;">​</a></h3><h3 id="even-better-if-you-can-submit-a-pr-fix-directly" tabindex="-1"><s>Even better if you can submit a PR fix directly</s> <a class="header-anchor" href="#even-better-if-you-can-submit-a-pr-fix-directly" aria-label="Permalink to &quot;~~Even better if you can submit a PR fix directly~~&quot;">​</a></h3><hr><blockquote><p>Due to the OSS release timing, fine-tuning OSS and Qwen may not be interchangeable. It’s also best to use the latest versions of <code>unsloth</code>, <code>torch</code>, <code>transformers</code>, and related libraries.</p></blockquote><p><a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/gpt-oss-%2820B%29-Fine-tuning.ipynb#scrollTo=WQSmUBxXx2r-" target="_blank" rel="noreferrer">Here’s Unsloth’s OSS fine-tuning tutorial</a></p><h2 id="quick-fine-tuning-guide" tabindex="-1">Quick Fine-tuning Guide <a class="header-anchor" href="#quick-fine-tuning-guide" aria-label="Permalink to &quot;Quick Fine-tuning Guide&quot;">​</a></h2><blockquote><p>It’s recommended to use a <strong>new virtual environment</strong>, separate from your Qwen fine-tuning environment.</p></blockquote><p>Make sure you have: <code>torch&gt;=2.8.0</code> <code>triton&gt;=3.4.0</code><strong>And importantly — ensure <code>unsloth</code> and <code>unsloth_zoo</code> are up to date.</strong></p><blockquote><p>The original <code>requirements.txt</code> for unsloth only supports up to version 2025.8.1, which does not work for OSS fine-tuning.</p></blockquote><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">pip</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> install</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;unsloth_zoo[base] @ git+https://github.com/unslothai/unsloth-zoo&quot;</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;unsloth[base] @ git+https://github.com/unslothai/unsloth&quot;</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> torchvision</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> bitsandbytes</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> git+https://github.com/huggingface/transformers</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> git+https://github.com/triton-lang/triton.git@main#subdirectory=python/triton_kernels</span></span></code></pre></div><h2 id="start-fine-tuning" tabindex="-1">Start Fine-tuning <a class="header-anchor" href="#start-fine-tuning" aria-label="Permalink to &quot;Start Fine-tuning&quot;">​</a></h2><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">python3</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> run_finetune_oss.py</span></span></code></pre></div><hr><table tabindex="0"><thead><tr><th>Parameter</th><th>Type</th><th>Default</th><th>Options</th><th>Description</th></tr></thead><tbody><tr><td><code>--repo_id</code></td><td>str</td><td><code>unsloth/gpt-oss-20b-unsloth-bnb-4bit</code></td><td>-</td><td>Hugging Face repository ID</td></tr><tr><td><code>--local_dir</code></td><td>str</td><td><code>gpt-oss-20b-unsloth-bnb-4bit</code></td><td>-</td><td>Local model directory</td></tr><tr><td><code>--use_unsloth</code></td><td>str</td><td><code>false</code></td><td><code>true</code>, <code>false</code></td><td>Whether to use Unsloth</td></tr><tr><td><code>--use_qlora</code></td><td>str</td><td><code>true</code></td><td><code>true</code>, <code>false</code></td><td>Whether to use QLoRA</td></tr><tr><td><code>--data_path</code></td><td>str</td><td><code>training_data.jsonl</code></td><td>-</td><td>Training data path</td></tr><tr><td><code>--eval_data_path</code></td><td>str / None</td><td>None</td><td>-</td><td>Evaluation data path</td></tr><tr><td><code>--max_samples</code></td><td>str / None</td><td>None</td><td>-</td><td>Maximum number of training samples</td></tr><tr><td><code>--max_eval_samples</code></td><td>str / None</td><td>None</td><td>-</td><td>Maximum number of evaluation samples</td></tr><tr><td><code>--model_max_length</code></td><td>str</td><td><code>2048</code></td><td>-</td><td>Max sequence length</td></tr><tr><td><code>--output_dir</code></td><td>str</td><td><code>finetune/models/qwen3-30b-a3b-qlora</code></td><td>-</td><td>Output directory</td></tr><tr><td><code>--seed</code></td><td>str</td><td><code>42</code></td><td>-</td><td>Random seed</td></tr><tr><td><code>--per_device_train_batch_size</code></td><td>str</td><td><code>1</code></td><td>-</td><td>Training batch size per device</td></tr><tr><td><code>--per_device_eval_batch_size</code></td><td>str</td><td><code>1</code></td><td>-</td><td>Evaluation batch size per device</td></tr><tr><td><code>--gradient_accumulation_steps</code></td><td>str</td><td><code>16</code></td><td>-</td><td>Gradient accumulation steps</td></tr><tr><td><code>--learning_rate</code></td><td>str</td><td><code>2e-4</code></td><td>-</td><td>Learning rate</td></tr><tr><td><code>--num_train_epochs</code></td><td>str</td><td><code>3</code></td><td>-</td><td>Number of training epochs</td></tr><tr><td><code>--max_steps</code></td><td>str</td><td><code>-1</code></td><td>-</td><td>Max steps (-1 for unlimited)</td></tr><tr><td><code>--lora_r</code></td><td>str</td><td><code>16</code></td><td>-</td><td>LoRA rank</td></tr><tr><td><code>--lora_alpha</code></td><td>str</td><td><code>32</code></td><td>-</td><td>LoRA alpha</td></tr><tr><td><code>--lora_dropout</code></td><td>str</td><td><code>0.05</code></td><td>-</td><td>LoRA dropout rate</td></tr><tr><td><code>--target_modules</code></td><td>str</td><td><code>q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj</code></td><td>-</td><td>LoRA target modules</td></tr><tr><td><code>--weight_decay</code></td><td>str</td><td><code>0.0</code></td><td>-</td><td>Weight decay</td></tr><tr><td><code>--moe_enable</code></td><td>str</td><td><code>false</code></td><td><code>true</code>, <code>false</code></td><td>Enable MoE</td></tr><tr><td><code>--moe_lora_scope</code></td><td>str</td><td><code>expert_only</code></td><td><code>expert_only</code>, <code>router_only</code>, <code>all</code></td><td>LoRA injection scope</td></tr><tr><td><code>--moe_expert_patterns</code></td><td>str</td><td><code>experts.ffn.(gate_proj|up_proj|down_proj),layers.[0-9]+.mlp.experts.[0-9]+.(w1|w2|w3)</code></td><td>-</td><td>Expert linear layer regex patterns</td></tr><tr><td><code>--moe_router_patterns</code></td><td>str</td><td><code>router.(gate|dense)</code></td><td>-</td><td>Router/gating layer regex patterns</td></tr><tr><td><code>--moe_max_experts_lora</code></td><td>str</td><td><code>-1</code></td><td>-</td><td>Max LoRA-injected experts per layer</td></tr><tr><td><code>--moe_dry_run</code></td><td>str</td><td><code>false</code></td><td><code>true</code>, <code>false</code></td><td>Print matched modules and exit</td></tr><tr><td><code>--load_precision</code></td><td>str</td><td><code>fp16</code></td><td><code>int8</code>, <code>int4</code>, <code>fp16</code></td><td>Model load precision</td></tr><tr><td><code>--use_flash_attention_2</code></td><td>str</td><td><code>false</code></td><td><code>true</code>, <code>false</code></td><td>Enable FlashAttention 2</td></tr><tr><td><code>--logging_steps</code></td><td>str</td><td><code>1</code></td><td>-</td><td>Logging interval in steps</td></tr><tr><td><code>--eval_steps</code></td><td>str</td><td><code>50</code></td><td>-</td><td>Evaluation interval in steps</td></tr><tr><td><code>--save_steps</code></td><td>str</td><td><code>200</code></td><td>-</td><td>Model save interval in steps</td></tr><tr><td><code>--save_total_limit</code></td><td>str</td><td><code>2</code></td><td>-</td><td>Max number of saved models</td></tr><tr><td><code>--warmup_ratio</code></td><td>str</td><td><code>0.05</code></td><td>-</td><td>Warmup ratio</td></tr><tr><td><code>--lr_scheduler_type</code></td><td>str</td><td><code>cosine</code></td><td>-</td><td>Learning rate scheduler type</td></tr><tr><td><code>--resume_from_checkpoint</code></td><td>str / None</td><td>None</td><td>-</td><td>Resume from checkpoint</td></tr><tr><td><code>--no-gradient_checkpointing</code></td><td>flag</td><td>False</td><td>-</td><td>Disable gradient checkpointing</td></tr><tr><td><code>--no-merge_and_save</code></td><td>flag</td><td>False</td><td>-</td><td>Skip merge and save</td></tr><tr><td><code>--fp16</code></td><td>str</td><td><code>true</code></td><td><code>true</code>, <code>false</code></td><td>Use fp16</td></tr><tr><td><code>--optim</code></td><td>str</td><td><code>adamw_torch_fused</code></td><td>-</td><td>Optimizer</td></tr><tr><td><code>--dataloader_pin_memory</code></td><td>str</td><td><code>false</code></td><td><code>true</code>, <code>false</code></td><td>Pin DataLoader memory</td></tr><tr><td><code>--dataloader_num_workers</code></td><td>str</td><td><code>0</code></td><td>-</td><td>DataLoader workers</td></tr><tr><td><code>--dataloader_prefetch_factor</code></td><td>str</td><td><code>2</code></td><td>-</td><td>DataLoader prefetch factor</td></tr><tr><td><code>--use_gradient_checkpointing</code></td><td>str</td><td><code>true</code></td><td><code>true</code>, <code>false</code>, <code>unsloth</code></td><td>Gradient checkpointing mode</td></tr><tr><td><code>--full_finetuning</code></td><td>str</td><td><code>false</code></td><td><code>true</code>, <code>false</code></td><td>Enable full fine-tuning</td></tr></tbody></table><hr><blockquote><p>Example: Fine-tuning <code>gpt-oss-20b-unsloth-bnb-4bit</code> on an RTX 4090:</p></blockquote><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">python3</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> run_finetune_oss.py</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --output_dir</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> /root/autodl-fs/gpt-oss-20b-unsloth-bnb-4bit</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --local_dir</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> gpt-oss-20b-bnb-4bit</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --data_path</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> ./training_data_ruozhi.jsonl</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --eval_data_path</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> ./training_data_ruozhi_eval.jsonl</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --use_qlora</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> true</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --lora_dropout</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 0.05</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --num_train_epochs</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 8</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --per_device_train_batch_size</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 4</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --per_device_eval_batch_size</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 4</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --gradient_accumulation_steps</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 8</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --learning_rate</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> 2e-5</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --lr_scheduler</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> cosine</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --logging_steps</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 5</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --eval_steps</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 40</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --save_steps</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 200</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --warmup_ratio</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 0.05</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --dataloader_num_workers</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 16</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --fp16</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> true</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --use_unsloth</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> true</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --no-gradient_checkpointing</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --dataloader_prefetch_factor</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 4</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --load_precision</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> int4</span></span></code></pre></div>',19)]))}const k=d(o,[["render",a]]);export{u as __pageData,k as default};
