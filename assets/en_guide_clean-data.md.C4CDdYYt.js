import{_ as i,c as a,o as s,ae as t}from"./chunks/framework.DUP9kEI5.js";const k=JSON.parse('{"title":"","description":"","frontmatter":{},"headers":[],"relativePath":"en/guide/clean-data.md","filePath":"en/guide/clean-data.md"}'),n={name:"en/guide/clean-data.md"};function l(o,e,r,h,p,d){return s(),a("div",null,e[0]||(e[0]=[t(`<h2 id="_2-1-clean-data-regular-version-llm-cleaning-version-in-next-chapter" tabindex="-1">2.1 Clean Data (Regular Version, LLM cleaning version in next chapter) <a class="header-anchor" href="#_2-1-clean-data-regular-version-llm-cleaning-version-in-next-chapter" aria-label="Permalink to &quot;2.1 Clean Data (Regular Version, LLM cleaning version in next chapter)&quot;">​</a></h2><blockquote><p>This method is much faster than LLM cleaning, 300k messages done in half a minute But correspondingly the quality is also lower This part is recommended to be optimized on Windows first, then uploaded to GPU server Not sure if there are compatibility issues on Linux</p></blockquote><ul><li><p>Modify the database path and related parameters in the <code>.env</code> file (please note the required fields)</p></li><li><p>Run the cleaning script:</p><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">python</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> generate_training_data.py</span></span></code></pre></div></li></ul><h2 id="_2-2-clean-data-llm-cleaning" tabindex="-1">2.2 Clean Data (LLM Cleaning) <a class="header-anchor" href="#_2-2-clean-data-llm-cleaning" aria-label="Permalink to &quot;2.2 Clean Data (LLM Cleaning)&quot;">​</a></h2><blockquote><p>Need to configure an OpenAI-compatible API For example: LM Studio or vLLM (faster, but more complicated to set up, requires Linux environment)</p></blockquote><blockquote><p>This part is also recommended to be optimized on Windows first, then uploaded to GPU server Not sure if there are compatibility issues on Linux</p></blockquote><h2 id="lm-studio-setup-tutorial" tabindex="-1">LM Studio Setup Tutorial <a class="header-anchor" href="#lm-studio-setup-tutorial" aria-label="Permalink to &quot;LM Studio Setup Tutorial&quot;">​</a></h2><ul><li><ol><li>Go to <a href="https://lmstudio.ai/" target="_blank" rel="noreferrer">LM Studio</a> to download LM Studio</li></ol></li><li><ol start="2"><li>Install LM Studio</li></ol></li><li><ol start="3"><li>Open LM Studio, click <code>Search</code> -&gt; <code>Model Search</code> on the left</li></ol></li><li><ol start="4"><li>Search for <code>qwen3 8b</code> -&gt; <code>Complete Download</code></li></ol></li><li><ol start="5"><li>Choose a quantization version suitable for you <strong>recommend at least Q4, preferably Q6-Q8, depends on your device situation, ask AI if you don&#39;t know</strong></li></ol></li><li>Remember your <strong>model name</strong>, fill it in the <code>Openai_model</code> field in the <code>.env</code> file</li><li>If you don&#39;t know your model name, you can run test_openai.py, it will output all model names</li><li><ol start="6"><li>After installation, click the button next to <code>Status:Stopped</code> in the left <code>Developer</code> section</li></ol></li><li>If the log below shows port occupied, please click <code>settings</code> to change the <code>server port</code></li><li>Remember this <code>server port</code>, fill your configuration in the <code>.env</code> file</li></ul><h3 id="run" tabindex="-1">run! <a class="header-anchor" href="#run" aria-label="Permalink to &quot;run!&quot;">​</a></h3><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">python</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> generate_training_data_llm.py</span></span></code></pre></div><blockquote><p>If you encounter 400 error, it&#39;s most likely because the message is too large and was rejected by the model framework</p></blockquote><hr><h2 id="vllm-setup" tabindex="-1">vLLM Setup <a class="header-anchor" href="#vllm-setup" aria-label="Permalink to &quot;vLLM Setup&quot;">​</a></h2><blockquote><p>vLLM requires Linux environment! If your graphics card is decent (&gt;6800xt, &gt;3080) You can choose to use LM Studio, just wait a bit longer, and you can also play with the model The downside is that LM Studio cannot run HF models, and concurrency is terrible</p></blockquote><blockquote><p>vLLM consumes much more VRAM than LM Studio! LM Studio can run 8b_q6 but vLLM can only run 4b_Q6</p></blockquote><blockquote><p>However, the improvement in concurrency efficiency is real</p></blockquote><blockquote><p>But! The context is very short, if there are more than 500 messages in a day, it cannot handle them</p></blockquote><blockquote><p>RTX 3080 tested with 4b_q6 processing, the final jsonl rate is approximately <strong>300kb/minute</strong></p></blockquote><ul><li>Follow these steps to set up:</li></ul><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">sudo</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> apt</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> update</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">sudo</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> apt</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> install</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> python3.10-venv</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> git</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -y</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">python3</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -m</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> venv</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> vllm_env</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">source</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> vllm_env/bin/activate</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">pip</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> install</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -U</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> pip</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">pip</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> install</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> torch</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --index-url</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> https://download.pytorch.org/whl/cu121</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">  # If you use CUDA</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">pip</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> install</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> vllm</span></span></code></pre></div><h3 id="different-considerations-from-lm-studio" tabindex="-1">Different considerations from LM Studio <a class="header-anchor" href="#different-considerations-from-lm-studio" aria-label="Permalink to &quot;Different considerations from LM Studio&quot;">​</a></h3><ul><li><ol><li>The <code>Openai_model</code> in <code>.env</code> needs to be set to a path rather than just a folder name</li></ol></li></ul><blockquote><p>It should be <code>/home/vllm/qwen3-4b-int8</code> instead of <code>qwen3-4b-int8</code></p></blockquote><ul><li><ol start="2"><li>The <strong>api_server</strong> to run is <code>vllm.entrypoints.openai.api_server</code> not <code>vllm.entrypoints.api_server</code>, because the second one is not compatible with OpenAI API</li></ol></li></ul><h3 id="example-run-command" tabindex="-1">Example run command <a class="header-anchor" href="#example-run-command" aria-label="Permalink to &quot;Example run command&quot;">​</a></h3><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">python3</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -m</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> vllm.entrypoints.openai.api_server</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --model</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> /home/vllm/qwen3-4b-int8</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --gpu-memory-utilization</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 0.7</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --max-model-len</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 10240</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --max-num-seqs</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 4</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --max-num-batched-tokens</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 2048</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --dtype</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> auto</span></span></code></pre></div><blockquote><p>If you encounter 400 error, it&#39;s most likely because the message is too large and was rejected by the model framework</p></blockquote>`,27)]))}const u=i(n,[["render",l]]);export{k as __pageData,u as default};
