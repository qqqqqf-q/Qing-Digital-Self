import{_ as e,c as d,o as s,ae as i}from"./chunks/framework.DUP9kEI5.js";const k=JSON.parse('{"title":"The following is the real fine-tuning","description":"","frontmatter":{},"headers":[],"relativePath":"en/guide/fine-tune-model.md","filePath":"en/guide/fine-tune-model.md"}'),a={name:"en/guide/fine-tune-model.md"};function o(n,t,r,l,h,c){return s(),d("div",null,t[0]||(t[0]=[i('<h2 id="_3-fine-tune-model" tabindex="-1">3. Fine-tune Model <a class="header-anchor" href="#_3-fine-tune-model" aria-label="Permalink to &quot;3. Fine-tune Model&quot;">​</a></h2><h2 id="before-this-you-need-to-configure-the-environment" tabindex="-1">Before this, you need to configure the environment <a class="header-anchor" href="#before-this-you-need-to-configure-the-environment" aria-label="Permalink to &quot;Before this, you need to configure the environment&quot;">​</a></h2><blockquote><p>Very simple, don&#39;t worry</p></blockquote><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">git</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> clone</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> https://github.com/qqqqqf-q/Qing-Digital-Self.git</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --depth</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 1</span></span></code></pre></div><p>Activate virtual environment:</p><ul><li><p>On Linux/Mac:</p><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">source</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> venv/bin/activate</span></span></code></pre></div></li><li><p>On Windows:</p><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">.</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">\\</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">venv</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">\\</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">Scripts</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">\\</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">activate</span></span></code></pre></div></li><li><p>Install dependencies</p></li></ul><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">pip</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> install</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -r</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> requirements.txt</span></span></code></pre></div><blockquote><p>PS: <s>I tested for a long time on the dependency step, I don&#39;t know why there were a bunch of strange problems before()</s> But this requirements is the version I tested myself, <s>should be stable</s></p></blockquote><hr><h3 id="if-you-need-the-unsloth-torch-version-provided-by-unsloth-please-run-the-following-command" tabindex="-1">If you need the unsloth+torch version provided by Unsloth, please run the following command <a class="header-anchor" href="#if-you-need-the-unsloth-torch-version-provided-by-unsloth-please-run-the-following-command" aria-label="Permalink to &quot;If you need the unsloth+torch version provided by Unsloth, please run the following command&quot;">​</a></h3><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">wget</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -qO-</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> https://raw.githubusercontent.com/unslothai/unsloth/main/unsloth/_auto_install.py</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> |</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> python</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> -</span></span></code></pre></div><p>It will output a pip command, please copy it and run it in shell For example</p><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">pip</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> install</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --upgrade</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> pip</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> &amp;&amp; </span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">pip</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> install</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;unsloth[cu126-ampere-torch270] @ git+https://github.com/unslothai/unsloth.git&quot;</span></span></code></pre></div><h1 id="the-following-is-the-real-fine-tuning" tabindex="-1">The following is the real fine-tuning <a class="header-anchor" href="#the-following-is-the-real-fine-tuning" aria-label="Permalink to &quot;The following is the real fine-tuning&quot;">​</a></h1><blockquote><p>Parameters actually don&#39;t need to be filled during testing, they all have default values</p></blockquote><blockquote><p>Seems to default to 8bit quantization, needs modification</p></blockquote><ul><li><p>Run fine-tuning script:</p><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">python</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> run_finetune.py</span></span></code></pre></div></li></ul><h3 id="model-related-parameters-table-has-four-columns-please-scroll-to-view" tabindex="-1">Model Related Parameters (table has four columns, please scroll to view) <a class="header-anchor" href="#model-related-parameters-table-has-four-columns-please-scroll-to-view" aria-label="Permalink to &quot;Model Related Parameters (table has four columns, please scroll to view)&quot;">​</a></h3><table tabindex="0"><thead><tr><th>Parameter Name</th><th>Type</th><th>Default Value</th><th>Description</th></tr></thead><tbody><tr><td><code>--repo_id</code></td><td>str</td><td><code>&#39;Qwen/Qwen3-30B-A3B-Instruct-2507&#39;</code></td><td>HF repository ID</td></tr><tr><td><code>--local_dir</code></td><td>str</td><td><code>&#39;qwen3-30b-a3b-instruct&#39;</code></td><td>Local model directory</td></tr><tr><td><code>--use_unsloth</code></td><td>str</td><td><code>&#39;false&#39;</code></td><td>Whether to use unsloth</td></tr><tr><td><code>--use_qlora</code></td><td>str</td><td><code>&#39;true&#39;</code></td><td>Whether to use qlora</td></tr><tr><td><code>--data_path</code></td><td>str</td><td><code>&#39;training_data.jsonl&#39;</code></td><td>Training data path</td></tr><tr><td><code>--eval_data_path</code></td><td>str</td><td><code>None</code></td><td>Validation data file path</td></tr><tr><td><code>--max_samples</code></td><td>str</td><td><code>None</code></td><td>Maximum training samples</td></tr><tr><td><code>--max_eval_samples</code></td><td>str</td><td><code>None</code></td><td>Maximum validation samples</td></tr><tr><td><code>--model_max_length</code></td><td>str</td><td><code>&#39;2048&#39;</code></td><td>Maximum sequence length</td></tr><tr><td><code>--output_dir</code></td><td>str</td><td><code>&#39;finetune/models/qwen3-30b-a3b-qlora&#39;</code></td><td>Output directory</td></tr><tr><td><code>--seed</code></td><td>str</td><td><code>&#39;42&#39;</code></td><td>Random seed</td></tr><tr><td><code>--per_device_train_batch_size</code></td><td>str</td><td><code>&#39;1&#39;</code></td><td>Training batch size per device</td></tr><tr><td><code>--per_device_eval_batch_size</code></td><td>str</td><td><code>&#39;1&#39;</code></td><td>Validation batch size per device</td></tr><tr><td><code>--gradient_accumulation_steps</code></td><td>str</td><td><code>&#39;16&#39;</code></td><td>Gradient accumulation steps</td></tr><tr><td><code>--learning_rate</code></td><td>str</td><td><code>&#39;2e-4&#39;</code></td><td>Learning rate</td></tr><tr><td><code>--num_train_epochs</code></td><td>str</td><td><code>&#39;3&#39;</code></td><td>Number of training epochs</td></tr><tr><td><code>--max_steps</code></td><td>str</td><td><code>&#39;-1&#39;</code></td><td>Maximum steps, -1 means no limit</td></tr><tr><td><code>--lora_r</code></td><td>str</td><td><code>&#39;16&#39;</code></td><td>LoRA rank</td></tr><tr><td><code>--lora_alpha</code></td><td>str</td><td><code>&#39;32&#39;</code></td><td>LoRA alpha value</td></tr><tr><td><code>--lora_dropout</code></td><td>str</td><td><code>&#39;0.05&#39;</code></td><td>LoRA dropout rate</td></tr><tr><td><code>--target_modules</code></td><td>str</td><td><code>&#39;Too long, please check in file&#39;</code></td><td>LoRA target modules</td></tr><tr><td><code>--weight_decay</code></td><td>str</td><td><code>&#39;0.0&#39;</code></td><td>Weight decay</td></tr><tr><td><code>--moe_enable</code></td><td>str</td><td><code>&#39;false&#39;</code></td><td>Whether to enable MoE injection logic</td></tr><tr><td><code>--moe_lora_scope</code></td><td>str</td><td><code>&#39;expert_only&#39;</code></td><td>LoRA injection scope</td></tr><tr><td><code>--moe_expert_patterns</code></td><td>str</td><td><code>&#39;Too long, check in file&#39;</code></td><td>Expert linear layer patterns</td></tr><tr><td><code>--moe_router_patterns</code></td><td>str</td><td><code>&#39;Markdown converts, check in file&#39;</code></td><td>Router/gate linear layer patterns</td></tr><tr><td><code>--moe_max_experts_lora</code></td><td>str</td><td><code>&#39;-1&#39;</code></td><td>Max experts per layer for LoRA</td></tr><tr><td><code>--moe_dry_run</code></td><td>str</td><td><code>&#39;false&#39;</code></td><td>Whether it&#39;s a Dry-Run</td></tr><tr><td><code>--load_precision</code></td><td>str</td><td><code>&#39;fp16&#39;</code></td><td>Model loading precision: <code>int8</code>/<code>int4</code>/<code>fp16</code></td></tr><tr><td><code>--logging_steps</code></td><td>str</td><td><code>&#39;1&#39;</code></td><td>Logging steps</td></tr><tr><td><code>--eval_steps</code></td><td>str</td><td><code>&#39;50&#39;</code></td><td>Evaluation interval steps</td></tr><tr><td><code>--save_steps</code></td><td>str</td><td><code>&#39;200&#39;</code></td><td>Model saving steps</td></tr><tr><td><code>--save_total_limit</code></td><td>str</td><td><code>&#39;2&#39;</code></td><td>Maximum number of saved models</td></tr><tr><td><code>--warmup_ratio</code></td><td>str</td><td><code>&#39;0.05&#39;</code></td><td>Learning rate warmup ratio</td></tr><tr><td><code>--lr_scheduler_type</code></td><td>str</td><td><code>&#39;cosine&#39;</code></td><td>Learning rate scheduler type</td></tr><tr><td><code>--resume_from_checkpoint</code></td><td>str</td><td><code>None</code></td><td>Checkpoint path to resume training</td></tr><tr><td><code>--no-gradient_checkpointing</code></td><td>flag</td><td><code>False</code></td><td>Don&#39;t use gradient checkpointing</td></tr><tr><td><code>--no-merge_and_save</code></td><td>flag</td><td><code>False</code></td><td>Don&#39;t merge and save model</td></tr><tr><td><code>--fp16</code></td><td>str</td><td><code>&#39;true&#39;</code></td><td>Whether to use fp16</td></tr><tr><td><code>--optim</code></td><td>str</td><td><code>&#39;adamw_torch_fused&#39;</code></td><td>Optimizer name</td></tr><tr><td><code>--dataloader_pin_memory</code></td><td>str</td><td><code>&#39;false&#39;</code></td><td>Whether to pin DataLoader memory</td></tr><tr><td><code>--dataloader_num_workers</code></td><td>str</td><td><code>&#39;0&#39;</code></td><td>DataLoader worker threads</td></tr><tr><td><code>--dataloader_prefetch_factor</code></td><td>str</td><td><code>&#39;2&#39;</code></td><td>DataLoader prefetch factor</td></tr><tr><td><code>--use_flash_attention_2</code></td><td>str</td><td><code>&#39;false&#39;</code></td><td>Whether to use FlashAttention2</td></tr></tbody></table><hr><blockquote><p>Parameters are still too complex, suggest asking AI Below is an example of fine-tuning <code>qwen3-8b-base</code> on 4090</p></blockquote><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">python3</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> run_finetune.py</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --output_dir</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> /root/autodl-fs/qwen3-8b-qing-v4</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --local_dir</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> qwen3-8b-base</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --data_path</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> ./training_data_ruozhi.jsonl</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --eval_data_path</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> ./training_data_ruozhi_eval.jsonl</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --use_qlora</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> true</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --lora_dropout</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 0.05</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --num_train_epochs</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 8</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --per_device_train_batch_size</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 4</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --per_device_eval_batch_size</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 4</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --gradient_accumulation_steps</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 8</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --learning_rate</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> 2e-5</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --lr_scheduler</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> cosine</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --logging_steps</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 5</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --eval_steps</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 40</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --save_steps</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 200</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --warmup_ratio</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 0.05</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --dataloader_num_workers</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 16</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --fp16</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> true</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --use_unsloth</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> true</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --no-gradient_checkpointing</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --dataloader_prefetch_factor</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 4</span></span></code></pre></div><h3 id="validation-set-not-working" tabindex="-1">Validation Set Not Working <a class="header-anchor" href="#validation-set-not-working" aria-label="Permalink to &quot;Validation Set Not Working&quot;">​</a></h3><ul><li>Check if <code>--eval_data_path</code> path is correct</li><li>Confirm validation data file format matches training data</li><li>Check console output for &quot;validation data path not provided&quot; prompt</li></ul><h3 id="gpu-memory-insufficient" tabindex="-1">GPU Memory Insufficient <a class="header-anchor" href="#gpu-memory-insufficient" aria-label="Permalink to &quot;GPU Memory Insufficient&quot;">​</a></h3><ul><li>Reduce <code>--per_device_eval_batch_size</code></li><li>Reduce <code>--max_eval_samples</code></li><li>Increase <code>--eval_steps</code> interval</li></ul>',26)]))}const u=e(a,[["render",o]]);export{k as __pageData,u as default};
