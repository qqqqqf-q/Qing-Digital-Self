import{_ as t,c as i,o as a,ae as s}from"./chunks/framework.DUP9kEI5.js";const k=JSON.parse('{"title":"Develop version may temporarily not support this feature","description":"","frontmatter":{},"headers":[],"relativePath":"en/guide/clean-data.md","filePath":"en/guide/clean-data.md"}'),n={name:"en/guide/clean-data.md"};function o(l,e,r,d,h,p){return a(),i("div",null,e[0]||(e[0]=[s(`<h2 id="convert-data-to-csv" tabindex="-1">Convert Data to CSV <a class="header-anchor" href="#convert-data-to-csv" aria-label="Permalink to &quot;Convert Data to CSV&quot;">​</a></h2><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">python</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> cli.py</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> data</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> extract</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Or customize parser fields</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">python</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> cli.py</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> data</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> extract</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --qq-db-path</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> ./data/qq.db</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --qq-number-ai</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> 1234567890--output</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> ./dataset/csv</span></span></code></pre></div><table tabindex="0"><thead><tr><th>Parameter</th><th>Description</th><th>Default/Notes</th></tr></thead><tbody><tr><td><code>-h, --help</code></td><td>Show help information and exit</td><td>-</td></tr><tr><td><code>--source-type {qq,tg,telegram}</code></td><td>Specify data source type</td><td>Auto-detect if not specified</td></tr><tr><td><code>--data-dir DATA_DIR</code></td><td>Data directory path</td><td><code>./dataset/original/</code></td></tr><tr><td><code>--output OUTPUT</code></td><td>Output directory path</td><td><code>./dataset/csv/</code></td></tr><tr><td><code>--qq-db-path QQ_DB_PATH</code></td><td>QQ database file path</td><td>-</td></tr><tr><td><code>--qq-number-ai QQ_NUMBER_AI</code></td><td>AI&#39;s QQ number (to distinguish sender)</td><td>-</td></tr><tr><td><code>--telegram-chat-id TELEGRAM_CHAT_ID</code></td><td>AI&#39;s Telegram chat name (to distinguish sender)</td><td>-</td></tr><tr><td><code>--tg-data-dir TG_DATA_DIR</code></td><td>Telegram data directory</td><td>Use <code>--data-dir</code> if not specified</td></tr></tbody></table><h2 id="clean-data-regular-version-llm-cleaning-version-in-next-section" tabindex="-1">Clean Data (Regular Version, LLM cleaning version in next section) <a class="header-anchor" href="#clean-data-regular-version-llm-cleaning-version-in-next-section" aria-label="Permalink to &quot;Clean Data (Regular Version, LLM cleaning version in next section)&quot;">​</a></h2><blockquote><p>This method is much faster than LLM cleaning, 300k messages done in a few seconds But correspondingly the quality is also lower This part is recommended to be optimized on Windows first, then uploaded to GPU server</p></blockquote><ul><li>Modify the database path and related parameters in the <code>setting.jsonc</code> file (please note the required fields)</li><li>Some fields in <code>data_agrs</code> and the <code>system prompt</code> below</li><li>Run the cleaning script:</li></ul><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">python</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> cli.py</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> data</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> clean</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> raw</span></span></code></pre></div><hr><h2 id="clean-data-llm-cleaning" tabindex="-1">Clean Data (LLM Cleaning) <a class="header-anchor" href="#clean-data-llm-cleaning" aria-label="Permalink to &quot;Clean Data (LLM Cleaning)&quot;">​</a></h2><h1 id="develop-version-may-temporarily-not-support-this-feature" tabindex="-1">Develop version may temporarily not support this feature <a class="header-anchor" href="#develop-version-may-temporarily-not-support-this-feature" aria-label="Permalink to &quot;Develop version may temporarily not support this feature&quot;">​</a></h1><h1 id="please-prioritize-using-raw-version-or-wait-for-updates-to-new-cleaning-methods" tabindex="-1">Please prioritize using raw version or wait for updates to new cleaning methods <a class="header-anchor" href="#please-prioritize-using-raw-version-or-wait-for-updates-to-new-cleaning-methods" aria-label="Permalink to &quot;Please prioritize using raw version or wait for updates to new cleaning methods&quot;">​</a></h1><blockquote><p>Need to configure an OpenAI-compatible API For example: LM Studio or vLLM (faster, but more complicated to set up, requires Linux environment)</p></blockquote><blockquote><p>This part is also recommended to be optimized on Windows first, then uploaded to GPU server Not sure if there are compatibility issues on Linux</p></blockquote><h2 id="lm-studio-setup-tutorial" tabindex="-1">LM Studio Setup Tutorial <a class="header-anchor" href="#lm-studio-setup-tutorial" aria-label="Permalink to &quot;LM Studio Setup Tutorial&quot;">​</a></h2><ul><li><ol><li>Go to <a href="https://lmstudio.ai/" target="_blank" rel="noreferrer">LM Studio</a> to download LM Studio</li></ol></li><li><ol start="2"><li>Install LM Studio</li></ol></li><li><ol start="3"><li>Open LM Studio, click <code>Search</code> -&gt; <code>Model Search</code> on the left</li></ol></li><li><ol start="4"><li>Search for <code>qwen2.5-7b-instruct</code> -&gt; <code>Complete Download</code></li></ol></li><li><ol start="5"><li>Choose a quantization version suitable for you <strong>recommend at least Q4, preferably Q6-Q8, depends on your device situation, ask AI if you don&#39;t know</strong></li></ol></li><li>Remember your <strong>model name</strong>, fill it in the <code>Openai_model</code> field in the <code>.env</code> file</li><li>If you don&#39;t know your model name, you can run test_openai.py, it will output all model names</li><li><ol start="6"><li>After installation, click the button next to <code>Status:Stopped</code> in the left <code>Developer</code> section</li></ol></li><li>If the log below shows port occupied, please click <code>settings</code> to change the <code>server port</code></li><li>Remember this <code>server port</code>, fill your configuration in the <code>.env</code> file</li></ul><h3 id="run" tabindex="-1">run! <a class="header-anchor" href="#run" aria-label="Permalink to &quot;run!&quot;">​</a></h3><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">python</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> generate_training_data_llm.py</span></span></code></pre></div><blockquote><p>If you encounter 400 error, it&#39;s most likely because the message is too large and was rejected by the model framework</p></blockquote><hr><h2 id="vllm-setup" tabindex="-1">vLLM Setup <a class="header-anchor" href="#vllm-setup" aria-label="Permalink to &quot;vLLM Setup&quot;">​</a></h2><blockquote><p>vLLM requires Linux environment! If your graphics card is decent (&gt;6800xt, &gt;3080) You can choose to use LM Studio, just wait a bit longer, and you can also play with the model The downside is that LM Studio cannot run HF models, and concurrency is terrible</p></blockquote><blockquote><p>vLLM consumes much more VRAM than LM Studio! LM Studio can run 8b_q6 but vLLM can only run 4b_Q6</p></blockquote><blockquote><p>However, the improvement in concurrency efficiency is real</p></blockquote><blockquote><p>But! The context is very short, if there are more than 500 messages in a day, it cannot handle them</p></blockquote><blockquote><p>RTX 3080 tested with 4b_q6 processing, the final jsonl rate is approximately <strong>300kb/minute</strong></p></blockquote><ul><li>Follow these steps to set up:</li></ul><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">sudo</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> apt</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> update</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">sudo</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> apt</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> install</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> python3.10-venv</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> git</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -y</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">python3</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -m</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> venv</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> vllm_env</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">source</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> vllm_env/bin/activate</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">pip</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> install</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -U</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> pip</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">pip</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> install</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> torch</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --index-url</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> https://download.pytorch.org/whl/cu121</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">  # If you use CUDA</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">pip</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> install</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> vllm</span></span></code></pre></div><h3 id="different-considerations-from-lm-studio" tabindex="-1">Different considerations from LM Studio <a class="header-anchor" href="#different-considerations-from-lm-studio" aria-label="Permalink to &quot;Different considerations from LM Studio&quot;">​</a></h3><ul><li><ol><li>The <code>model_name</code> in <code>setting.jsonc</code> needs to be set to a path rather than just a folder name</li></ol></li></ul><blockquote><p>It should be <code>/home/vllm/qwen3-4b-int8</code> instead of <code>qwen3-4b-int8</code></p></blockquote><ul><li><ol start="2"><li>The <strong>api_server</strong> to run is <code>vllm.entrypoints.openai.api_server</code> not <code>vllm.entrypoints.api_server</code>, because the second one is not compatible with OpenAI API</li></ol></li></ul><h3 id="example-run-command" tabindex="-1">Example run command <a class="header-anchor" href="#example-run-command" aria-label="Permalink to &quot;Example run command&quot;">​</a></h3><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">python3</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -m</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> vllm.entrypoints.openai.api_server</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --model</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> /home/vllm/qwen3-4b-int8</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --gpu-memory-utilization</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 0.7</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --max-model-len</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 10240</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --max-num-seqs</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 4</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --max-num-batched-tokens</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 2048</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --dtype</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> auto</span></span></code></pre></div><blockquote><p>If you encounter 400 error, it&#39;s most likely because the message is too large and was rejected by the model framework</p></blockquote><h3 id="dev-notes" tabindex="-1">Dev Notes <a class="header-anchor" href="#dev-notes" aria-label="Permalink to &quot;Dev Notes&quot;">​</a></h3><blockquote><p>Currently, new LLM processing has not been implemented yet The <code>python cli.py data clean llm</code> command is available but equivalent to raw Also, the <code>python cli.py data extract</code> command now only supports QQ Parser, not optimized for multi-parser multi-data sources You can add <code>qq/tg/wx</code> etc. <code>metamodel</code> to support more parsers</p></blockquote>`,36)]))}const u=t(n,[["render",o]]);export{k as __pageData,u as default};
