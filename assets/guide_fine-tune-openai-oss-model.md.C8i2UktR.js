import{_ as d,c as e,o as s,ae as o}from"./chunks/framework.DUP9kEI5.js";const k=JSON.parse('{"title":"番外篇: 微调OpenAI OSS模型","description":"","frontmatter":{},"headers":[],"relativePath":"guide/fine-tune-openai-oss-model.md","filePath":"guide/fine-tune-openai-oss-model.md"}'),a={name:"guide/fine-tune-openai-oss-model.md"};function i(r,t,c,n,h,l){return s(),e("div",null,t[0]||(t[0]=[o('<h1 id="番外篇-微调openai-oss模型" tabindex="-1">番外篇: 微调OpenAI OSS模型 <a class="header-anchor" href="#番外篇-微调openai-oss模型" aria-label="Permalink to &quot;番外篇: 微调OpenAI OSS模型&quot;">​</a></h1><h2 id="此篇内容未经过实机测试-我的3080显存太小跑不动" tabindex="-1">此篇内容未经过实机测试(我的3080显存太小跑不动) <a class="header-anchor" href="#此篇内容未经过实机测试-我的3080显存太小跑不动" aria-label="Permalink to &quot;此篇内容未经过实机测试(我的3080显存太小跑不动)&quot;">​</a></h2><h3 id="欢迎能测试的朋友进行实机测试并将结果或bug发在issues上" tabindex="-1">欢迎能测试的朋友进行实机测试并将结果或Bug发在Issues上 <a class="header-anchor" href="#欢迎能测试的朋友进行实机测试并将结果或bug发在issues上" aria-label="Permalink to &quot;欢迎能测试的朋友进行实机测试并将结果或Bug发在Issues上&quot;">​</a></h3><h3 id="能直接pr修复那就更棒了" tabindex="-1"><s>能直接PR修复那就更棒了</s> <a class="header-anchor" href="#能直接pr修复那就更棒了" aria-label="Permalink to &quot;~~能直接PR修复那就更棒了~~&quot;">​</a></h3><hr><blockquote><p>因为OSS发布的时间,似乎微调OSS和Qwen的并不能通用<br> 并且最好使用新的<code>unsloth</code> <code>torch</code> <code>transformers</code> 等库</p></blockquote><p><a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/gpt-oss-(20B)-Fine-tuning.ipynb#scrollTo=WQSmUBxXx2r-" target="_blank" rel="noreferrer">这是Unsloth提供的OSS微调经验</a></p><h2 id="以下是快速微调指南" tabindex="-1">以下是快速微调指南 <a class="header-anchor" href="#以下是快速微调指南" aria-label="Permalink to &quot;以下是快速微调指南&quot;">​</a></h2><blockquote><p>建议使用新的虚拟环境<br> 和Qwen的微调环境分离</p></blockquote><p>请确保以下的库: <code>torch&gt;=2.8.0</code> <code>triton&gt;=3.4.0</code><strong>并且!请保证Unsloth和Unsloth_zoo的最新版</strong></p><blockquote><p>原requirements.txt的unsloth只支持到2025.8.1版本,并不能微调oss</p></blockquote><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">pip</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> install</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;unsloth_zoo[base] @ git+https://github.com/unslothai/unsloth-zoo&quot;</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;unsloth[base] @ git+https://github.com/unslothai/unsloth&quot;</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> torchvision</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> bitsandbytes</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> git+https://github.com/huggingface/transformers</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> git+https://github.com/triton-lang/triton.git@main#subdirectory=python/triton_kernels</span></span></code></pre></div><h2 id="开始微调" tabindex="-1">开始微调 <a class="header-anchor" href="#开始微调" aria-label="Permalink to &quot;开始微调&quot;">​</a></h2><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">python3</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> run_finetune_oss.py</span></span></code></pre></div><hr><table tabindex="0"><thead><tr><th>参数</th><th>类型</th><th>默认值</th><th>可选值</th><th>说明</th></tr></thead><tbody><tr><td><code>--repo_id</code></td><td>str</td><td><code>unsloth/gpt-oss-20b-unsloth-bnb-4bit</code></td><td>-</td><td>HF 仓库ID</td></tr><tr><td><code>--local_dir</code></td><td>str</td><td><code>gpt-oss-20b-unsloth-bnb-4bit</code></td><td>-</td><td>本地模型目录</td></tr><tr><td><code>--use_unsloth</code></td><td>str</td><td><code>false</code></td><td><code>true</code>, <code>false</code></td><td>是否使用unsloth</td></tr><tr><td><code>--use_qlora</code></td><td>str</td><td><code>true</code></td><td><code>true</code>, <code>false</code></td><td>是否使用QLoRA</td></tr><tr><td><code>--data_path</code></td><td>str</td><td><code>training_data.jsonl</code></td><td>-</td><td>训练数据路径</td></tr><tr><td><code>--eval_data_path</code></td><td>str / None</td><td>None</td><td>-</td><td>验证数据路径</td></tr><tr><td><code>--max_samples</code></td><td>str / None</td><td>None</td><td>-</td><td>最大训练样本数</td></tr><tr><td><code>--max_eval_samples</code></td><td>str / None</td><td>None</td><td>-</td><td>最大验证样本数</td></tr><tr><td><code>--model_max_length</code></td><td>str</td><td><code>2048</code></td><td>-</td><td>最大序列长度</td></tr><tr><td><code>--output_dir</code></td><td>str</td><td><code>finetune/models/qwen3-30b-a3b-qlora</code></td><td>-</td><td>输出目录</td></tr><tr><td><code>--seed</code></td><td>str</td><td><code>42</code></td><td>-</td><td>随机种子</td></tr><tr><td><code>--per_device_train_batch_size</code></td><td>str</td><td><code>1</code></td><td>-</td><td>每设备训练批次大小</td></tr><tr><td><code>--per_device_eval_batch_size</code></td><td>str</td><td><code>1</code></td><td>-</td><td>每设备验证批次大小</td></tr><tr><td><code>--gradient_accumulation_steps</code></td><td>str</td><td><code>16</code></td><td>-</td><td>梯度累积步数</td></tr><tr><td><code>--learning_rate</code></td><td>str</td><td><code>2e-4</code></td><td>-</td><td>学习率</td></tr><tr><td><code>--num_train_epochs</code></td><td>str</td><td><code>3</code></td><td>-</td><td>训练轮数</td></tr><tr><td><code>--max_steps</code></td><td>str</td><td><code>-1</code></td><td>-</td><td>最大步数（-1为不限）</td></tr><tr><td><code>--lora_r</code></td><td>str</td><td><code>16</code></td><td>-</td><td>LoRA 秩</td></tr><tr><td><code>--lora_alpha</code></td><td>str</td><td><code>32</code></td><td>-</td><td>LoRA alpha</td></tr><tr><td><code>--lora_dropout</code></td><td>str</td><td><code>0.05</code></td><td>-</td><td>LoRA dropout率</td></tr><tr><td><code>--target_modules</code></td><td>str</td><td><code>q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj</code></td><td>-</td><td>LoRA 目标模块</td></tr><tr><td><code>--weight_decay</code></td><td>str</td><td><code>0.0</code></td><td>-</td><td>权重衰减</td></tr><tr><td><code>--moe_enable</code></td><td>str</td><td><code>false</code></td><td><code>true</code>, <code>false</code></td><td>是否启用 MoE</td></tr><tr><td><code>--moe_lora_scope</code></td><td>str</td><td><code>expert_only</code></td><td><code>expert_only</code>, <code>router_only</code>, <code>all</code></td><td>LoRA 注入范围</td></tr><tr><td><code>--moe_expert_patterns</code></td><td>str</td><td><code>experts.ffn.(gate_proj|up_proj|down_proj),layers.[0-9]+.mlp.experts.[0-9]+.(w1|w2|w3)</code></td><td>-</td><td>专家线性层模式（正则）</td></tr><tr><td><code>--moe_router_patterns</code></td><td>str</td><td><code>router.(gate|dense)</code></td><td>-</td><td>路由/门控层模式（正则）</td></tr><tr><td><code>--moe_max_experts_lora</code></td><td>str</td><td><code>-1</code></td><td>-</td><td>每层最多注入 LoRA 的专家数</td></tr><tr><td><code>--moe_dry_run</code></td><td>str</td><td><code>false</code></td><td><code>true</code>, <code>false</code></td><td>仅打印匹配模块并退出</td></tr><tr><td><code>--load_precision</code></td><td>str</td><td><code>fp16</code></td><td><code>int8</code>, <code>int4</code>, <code>fp16</code></td><td>模型加载精度</td></tr><tr><td><code>--use_flash_attention_2</code></td><td>str</td><td><code>false</code></td><td><code>true</code>, <code>false</code></td><td>是否启用 FlashAttention2</td></tr><tr><td><code>--logging_steps</code></td><td>str</td><td><code>1</code></td><td>-</td><td>日志记录步数</td></tr><tr><td><code>--eval_steps</code></td><td>str</td><td><code>50</code></td><td>-</td><td>验证间隔步数</td></tr><tr><td><code>--save_steps</code></td><td>str</td><td><code>200</code></td><td>-</td><td>保存模型步数</td></tr><tr><td><code>--save_total_limit</code></td><td>str</td><td><code>2</code></td><td>-</td><td>最多保存数</td></tr><tr><td><code>--warmup_ratio</code></td><td>str</td><td><code>0.05</code></td><td>-</td><td>预热比例</td></tr><tr><td><code>--lr_scheduler_type</code></td><td>str</td><td><code>cosine</code></td><td>-</td><td>学习率调度器类型</td></tr><tr><td><code>--resume_from_checkpoint</code></td><td>str / None</td><td>None</td><td>-</td><td>从检查点恢复</td></tr><tr><td><code>--no-gradient_checkpointing</code></td><td>flag</td><td>False</td><td>-</td><td>不使用梯度检查点</td></tr><tr><td><code>--no-merge_and_save</code></td><td>flag</td><td>False</td><td>-</td><td>不合并并保存模型</td></tr><tr><td><code>--fp16</code></td><td>str</td><td><code>true</code></td><td><code>true</code>, <code>false</code></td><td>是否使用fp16</td></tr><tr><td><code>--optim</code></td><td>str</td><td><code>adamw_torch_fused</code></td><td>-</td><td>优化器</td></tr><tr><td><code>--dataloader_pin_memory</code></td><td>str</td><td><code>false</code></td><td><code>true</code>, <code>false</code></td><td>是否固定数据加载器内存</td></tr><tr><td><code>--dataloader_num_workers</code></td><td>str</td><td><code>0</code></td><td>-</td><td>DataLoader 线程数</td></tr><tr><td><code>--dataloader_prefetch_factor</code></td><td>str</td><td><code>2</code></td><td>-</td><td>DataLoader 预取因子</td></tr><tr><td><code>--use_gradient_checkpointing</code></td><td>str</td><td><code>true</code></td><td><code>true</code>, <code>false</code>, <code>unsloth</code></td><td>梯度检查点设置</td></tr><tr><td><code>--full_finetuning</code></td><td>str</td><td><code>false</code></td><td><code>true</code>, <code>false</code></td><td>是否全量微调</td></tr></tbody></table><hr><blockquote><p>下面是一个4090微调<code>gpt-oss-20b-unsloth-bnb-4bit</code>的范例</p></blockquote><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">python3</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> run_finetune_oss.py</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --output_dir</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> /root/autodl-fs/gpt-oss-20b-unsloth-bnb-4bit</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --local_dir</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> gpt-oss-20b-bnb-4bit</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --data_path</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> ./training_data_ruozhi.jsonl</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --eval_data_path</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> ./training_data_ruozhi_eval.jsonl</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --use_qlora</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> true</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --lora_dropout</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 0.05</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --num_train_epochs</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 8</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --per_device_train_batch_size</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 4</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --per_device_eval_batch_size</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 4</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --gradient_accumulation_steps</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 8</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --learning_rate</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> 2e-5</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --lr_scheduler</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> cosine</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --logging_steps</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 5</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --eval_steps</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 40</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --save_steps</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 200</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --warmup_ratio</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 0.05</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --dataloader_num_workers</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 16</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --fp16</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> true</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --use_unsloth</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> true</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --no-gradient_checkpointing</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --dataloader_prefetch_factor</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 4</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --load_precision</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> int4</span></span></code></pre></div>',19)]))}const _=d(a,[["render",i]]);export{k as __pageData,_ as default};
