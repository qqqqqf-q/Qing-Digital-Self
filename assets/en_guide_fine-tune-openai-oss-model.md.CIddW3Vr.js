import{_ as e,c as d,o as s,ae as a}from"./chunks/framework.DUP9kEI5.js";const u=JSON.parse('{"title":"Extra: Fine-tuning OpenAI OSS Model","description":"","frontmatter":{},"headers":[],"relativePath":"en/guide/fine-tune-openai-oss-model.md","filePath":"en/guide/fine-tune-openai-oss-model.md"}'),i={name:"en/guide/fine-tune-openai-oss-model.md"};function o(n,t,r,l,h,c){return s(),d("div",null,t[0]||(t[0]=[a('<h1 id="extra-fine-tuning-openai-oss-model" tabindex="-1">Extra: Fine-tuning OpenAI OSS Model <a class="header-anchor" href="#extra-fine-tuning-openai-oss-model" aria-label="Permalink to &quot;Extra: Fine-tuning OpenAI OSS Model&quot;">​</a></h1><h2 id="this-content-has-been-tested-on-real-hardware-vgpu-32g" tabindex="-1">This content has been tested on real hardware (vgpu 32g) <a class="header-anchor" href="#this-content-has-been-tested-on-real-hardware-vgpu-32g" aria-label="Permalink to &quot;This content has been tested on real hardware (vgpu 32g)&quot;">​</a></h2><h3 id="if-you-find-a-bug-please-open-an-issue-or-contact-the-author" tabindex="-1">If you find a bug, please open an issue or contact the author <a class="header-anchor" href="#if-you-find-a-bug-please-open-an-issue-or-contact-the-author" aria-label="Permalink to &quot;If you find a bug, please open an issue or contact the author&quot;">​</a></h3><h3 id="it-would-be-even-better-if-you-could-directly-submit-a-pr-fix" tabindex="-1"><s>It would be even better if you could directly submit a PR fix</s> <a class="header-anchor" href="#it-would-be-even-better-if-you-could-directly-submit-a-pr-fix" aria-label="Permalink to &quot;~~It would be even better if you could directly submit a PR fix~~&quot;">​</a></h3><hr><h1 id="please-note" tabindex="-1">Please Note <a class="header-anchor" href="#please-note" aria-label="Permalink to &quot;Please Note&quot;">​</a></h1><blockquote><p>During testing, using <code>unsloth/gpt-oss-20b-unsloth-bnb-4bit</code> seemed to fine-tune and merge successfully. However, conversion to GGUF failed. When testing <code>unsloth/gpt-oss-20b</code>, encountered the error <code>&#39;GptOssTopKRouter&#39; object has no attribute &#39;weight&#39;</code>. This appears to be a widespread issue; I’ve found many others encountering it during fine-tuning as well. Please give the Unsloth and OpenAI teams some time — they’ll fix it. Once updated, I will immediately update this document and the code.</p></blockquote><hr><h3 id="fine-tuning-the-oss-model" tabindex="-1">Fine-tuning the OSS Model <a class="header-anchor" href="#fine-tuning-the-oss-model" aria-label="Permalink to &quot;Fine-tuning the OSS Model&quot;">​</a></h3><blockquote><p>Due to the release timing of OSS, fine-tuning methods for OSS and Qwen do not seem interchangeable. Also, it’s best to use the latest versions of <code>unsloth</code>, <code>torch</code>, <code>transformers</code>, etc.</p></blockquote><p><a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/gpt-oss-%2820B%29-Fine-tuning.ipynb#scrollTo=WQSmUBxXx2r-" target="_blank" rel="noreferrer">Here’s Unsloth’s OSS fine-tuning experience</a></p><h2 id="quick-fine-tuning-guide" tabindex="-1">Quick Fine-tuning Guide <a class="header-anchor" href="#quick-fine-tuning-guide" aria-label="Permalink to &quot;Quick Fine-tuning Guide&quot;">​</a></h2><blockquote><p>It’s recommended to use a new virtual environment Separate it from your Qwen fine-tuning environment</p></blockquote><p>Please ensure you have: <code>torch&gt;=2.8.0</code> <code>triton&gt;=3.4.0</code><strong>And! Make sure <code>unsloth</code> and <code>unsloth_zoo</code> are the latest versions</strong></p><blockquote><p>The original requirements.txt only supports unsloth up to version 2025.8.1, which cannot fine-tune OSS.</p></blockquote><h3 id="run-the-following-before-installing-dependencies" tabindex="-1">Run the following before installing dependencies <a class="header-anchor" href="#run-the-following-before-installing-dependencies" aria-label="Permalink to &quot;Run the following before installing dependencies&quot;">​</a></h3><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">pip</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> install</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;unsloth_zoo[base] @ git+https://github.com/unslothai/unsloth-zoo&quot;</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;unsloth[base] @ git+https://github.com/unslothai/unsloth&quot;</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> torchvision</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> bitsandbytes</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> git+https://github.com/huggingface/transformers</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> git+https://github.com/triton-lang/triton.git@main#subdirectory=python/triton_kernels</span></span></code></pre></div><h3 id="install-dependencies" tabindex="-1">Install dependencies <a class="header-anchor" href="#install-dependencies" aria-label="Permalink to &quot;Install dependencies&quot;">​</a></h3><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">pip</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> install</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -r</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> requirements_oss.txt</span></span></code></pre></div><h2 id="note-this-model-requires-openai-harmony-format-training-data-for-fine-tuning" tabindex="-1"><strong>Note: This model requires OpenAI Harmony format training data for fine-tuning</strong> <a class="header-anchor" href="#note-this-model-requires-openai-harmony-format-training-data-for-fine-tuning" aria-label="Permalink to &quot;**Note: This model requires OpenAI Harmony format training data for fine-tuning**&quot;">​</a></h2><p>Use <code>chatml_to_harmony.py</code> to convert ChatML format training data to Harmony format:</p><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">python3</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> chatml_to_harmony.py</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --input</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> training_data.jsonl</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --output</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> training_data_harmony.txt</span></span></code></pre></div><h3 id="download-the-model" tabindex="-1">Download the model <a class="header-anchor" href="#download-the-model" aria-label="Permalink to &quot;Download the model&quot;">​</a></h3><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">huggingface-cli</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> download</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> unsloth/gpt-oss-20b-BF16</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --local-dir</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> gpt-oss-20b</span></span></code></pre></div><blockquote><p>If you don’t have huggingface-cli, install it first:</p></blockquote><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">pip</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> install</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> huggingface-hub</span></span></code></pre></div><blockquote><p>If you need a mirror site, run:</p></blockquote><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">export</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> HF_ENDPOINT</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">https://hf-mirror.com</span></span></code></pre></div><blockquote><p>During testing, using <code>unsloth/gpt-oss-20b-unsloth-bnb-4bit</code> seemed to fine-tune and merge successfully. However, conversion to GGUF failed. When testing <code>unsloth/gpt-oss-20b</code>, encountered the error <code>&#39;GptOssTopKRouter&#39; object has no attribute &#39;weight&#39;</code>. This appears to be a widespread issue; I’ve found many others encountering it during fine-tuning as well. Please give the Unsloth and OpenAI teams some time — they’ll fix it. Once updated, I will immediately update this document and the code.</p></blockquote><h2 id="start-fine-tuning" tabindex="-1">Start Fine-tuning <a class="header-anchor" href="#start-fine-tuning" aria-label="Permalink to &quot;Start Fine-tuning&quot;">​</a></h2><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">python3</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> run_finetune_oss.py</span></span></code></pre></div><hr><table tabindex="0"><thead><tr><th>Parameter</th><th>Type</th><th>Default Value</th><th>Optional Values</th><th>Description</th></tr></thead><tbody><tr><td><code>--repo_id</code></td><td>str</td><td><code>unsloth/gpt-oss-20b-unsloth-bnb-4bit</code></td><td>-</td><td>HF repository ID</td></tr><tr><td><code>--local_dir</code></td><td>str</td><td><code>gpt-oss-20b-unsloth-bnb-4bit</code></td><td>-</td><td>Local model directory</td></tr><tr><td><code>--use_unsloth</code></td><td>str</td><td><code>false</code></td><td><code>true</code>, <code>false</code></td><td>Whether to use unsloth</td></tr><tr><td><code>--use_qlora</code></td><td>str</td><td><code>true</code></td><td><code>true</code>, <code>false</code></td><td>Whether to use QLoRA</td></tr><tr><td><code>--data_path</code></td><td>str</td><td><code>training_data.jsonl</code></td><td>-</td><td>Training data path</td></tr><tr><td><code>--eval_data_path</code></td><td>str / None</td><td>None</td><td>-</td><td>Evaluation data path</td></tr><tr><td><code>--max_samples</code></td><td>str / None</td><td>None</td><td>-</td><td>Max number of training samples</td></tr><tr><td><code>--max_eval_samples</code></td><td>str / None</td><td>None</td><td>-</td><td>Max number of evaluation samples</td></tr><tr><td><code>--model_max_length</code></td><td>str</td><td><code>2048</code></td><td>-</td><td>Max sequence length</td></tr><tr><td><code>--output_dir</code></td><td>str</td><td><code>finetune/models/qwen3-30b-a3b-qlora</code></td><td>-</td><td>Output directory</td></tr><tr><td><code>--seed</code></td><td>str</td><td><code>42</code></td><td>-</td><td>Random seed</td></tr><tr><td><code>--per_device_train_batch_size</code></td><td>str</td><td><code>1</code></td><td>-</td><td>Training batch size per device</td></tr><tr><td><code>--per_device_eval_batch_size</code></td><td>str</td><td><code>1</code></td><td>-</td><td>Evaluation batch size per device</td></tr><tr><td><code>--gradient_accumulation_steps</code></td><td>str</td><td><code>16</code></td><td>-</td><td>Gradient accumulation steps</td></tr><tr><td><code>--learning_rate</code></td><td>str</td><td><code>2e-4</code></td><td>-</td><td>Learning rate</td></tr><tr><td><code>--num_train_epochs</code></td><td>str</td><td><code>3</code></td><td>-</td><td>Number of training epochs</td></tr><tr><td><code>--max_steps</code></td><td>str</td><td><code>-1</code></td><td>-</td><td>Max steps (-1 for unlimited)</td></tr><tr><td><code>--lora_r</code></td><td>str</td><td><code>16</code></td><td>-</td><td>LoRA rank</td></tr><tr><td><code>--lora_alpha</code></td><td>str</td><td><code>32</code></td><td>-</td><td>LoRA alpha</td></tr><tr><td><code>--lora_dropout</code></td><td>str</td><td><code>0.05</code></td><td>-</td><td>LoRA dropout rate</td></tr><tr><td><code>--target_modules</code></td><td>str</td><td><code>q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj</code></td><td>-</td><td>LoRA target modules</td></tr><tr><td><code>--weight_decay</code></td><td>str</td><td><code>0.0</code></td><td>-</td><td>Weight decay</td></tr><tr><td><code>--moe_enable</code></td><td>str</td><td><code>false</code></td><td><code>true</code>, <code>false</code></td><td>Enable MoE</td></tr><tr><td><code>--moe_lora_scope</code></td><td>str</td><td><code>expert_only</code></td><td><code>expert_only</code>, <code>router_only</code>, <code>all</code></td><td>LoRA injection scope for MoE</td></tr><tr><td><code>--moe_expert_patterns</code></td><td>str</td><td><code>experts.ffn.(gate_proj|up_proj|down_proj),layers.[0-9]+.mlp.experts.[0-9]+.(w1|w2|w3)</code></td><td>-</td><td>Expert linear layer patterns (regex)</td></tr><tr><td><code>--moe_router_patterns</code></td><td>str</td><td><code>router.(gate|dense)</code></td><td>-</td><td>Router/gating layer patterns (regex)</td></tr><tr><td><code>--moe_max_experts_lora</code></td><td>str</td><td><code>-1</code></td><td>-</td><td>Max LoRA experts per layer</td></tr><tr><td><code>--moe_dry_run</code></td><td>str</td><td><code>false</code></td><td><code>true</code>, <code>false</code></td><td>Only print matched modules and exit</td></tr><tr><td><code>--load_precision</code></td><td>str</td><td><code>fp16</code></td><td><code>int8</code>, <code>int4</code>, <code>fp16</code></td><td>Model load precision</td></tr><tr><td><code>--use_flash_attention_2</code></td><td>str</td><td><code>false</code></td><td><code>true</code>, <code>false</code></td><td>Enable FlashAttention2</td></tr><tr><td><code>--logging_steps</code></td><td>str</td><td><code>1</code></td><td>-</td><td>Logging step interval</td></tr><tr><td><code>--eval_steps</code></td><td>str</td><td><code>50</code></td><td>-</td><td>Evaluation step interval</td></tr><tr><td><code>--save_steps</code></td><td>str</td><td><code>200</code></td><td>-</td><td>Model save step interval</td></tr><tr><td><code>--save_total_limit</code></td><td>str</td><td><code>2</code></td><td>-</td><td>Max saved model count</td></tr><tr><td><code>--warmup_ratio</code></td><td>str</td><td><code>0.05</code></td><td>-</td><td>Warmup ratio</td></tr><tr><td><code>--lr_scheduler_type</code></td><td>str</td><td><code>cosine</code></td><td>-</td><td>LR scheduler type</td></tr><tr><td><code>--resume_from_checkpoint</code></td><td>str / None</td><td>None</td><td>-</td><td>Resume from checkpoint</td></tr><tr><td><code>--no-gradient_checkpointing</code></td><td>flag</td><td>False</td><td>-</td><td>Disable gradient checkpointing</td></tr><tr><td><code>--no-merge_and_save</code></td><td>flag</td><td>False</td><td>-</td><td>Do not merge and save model</td></tr><tr><td><code>--fp16</code></td><td>str</td><td><code>true</code></td><td><code>true</code>, <code>false</code></td><td>Use fp16</td></tr><tr><td><code>--optim</code></td><td>str</td><td><code>adamw_torch_fused</code></td><td>-</td><td>Optimizer</td></tr><tr><td><code>--dataloader_pin_memory</code></td><td>str</td><td><code>false</code></td><td><code>true</code>, <code>false</code></td><td>Pin dataloader memory</td></tr><tr><td><code>--dataloader_num_workers</code></td><td>str</td><td><code>0</code></td><td>-</td><td>Number of dataloader workers</td></tr><tr><td><code>--dataloader_prefetch_factor</code></td><td>str</td><td><code>2</code></td><td>-</td><td>Dataloader prefetch factor</td></tr><tr><td><code>--use_gradient_checkpointing</code></td><td>str</td><td><code>true</code></td><td><code>true</code>, <code>false</code>, <code>unsloth</code></td><td>Gradient checkpointing setting</td></tr><tr><td><code>--full_finetuning</code></td><td>str</td><td><code>false</code></td><td><code>true</code>, <code>false</code></td><td>Enable full fine-tuning</td></tr><tr><td><code>--data_format</code></td><td>str</td><td><code>harmony</code></td><td><code>harmony</code>, <code>jsonl</code></td><td>Data format</td></tr></tbody></table><hr><blockquote><p>Below is an example for fine-tuning <code>gpt-oss-20b-unsloth-bnb-4bit</code>:</p></blockquote><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">python3</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> run_finetune_oss.py</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --output_dir</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> /root/autodl-fs/gpt-oss-20b-unsloth-bnb-4bit</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --local_dir</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> gpt-oss-20b-4bit</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --data_path</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> ./harmony_small.txt</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --eval_data_path</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> ./harmony_small_eval.txt</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --use_qlora</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> true</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --lora_dropout</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 0.05</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --num_train_epochs</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 8</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --per_device_train_batch_size</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 4</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --per_device_eval_batch_size</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 4</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --gradient_accumulation_steps</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 8</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --learning_rate</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> 2e-5</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --lr_scheduler</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> cosine</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --logging_steps</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 5</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --eval_steps</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 40</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --save_steps</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 200</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --warmup_ratio</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 0.05</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --dataloader_num_workers</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 16</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --fp16</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> true</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --use_unsloth</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> true</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --no-gradient_checkpointing</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --dataloader_prefetch_factor</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 4</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --load_precision</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> int4</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --data_format</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> harmony</span></span></code></pre></div>',36)]))}const k=e(i,[["render",o]]);export{u as __pageData,k as default};
