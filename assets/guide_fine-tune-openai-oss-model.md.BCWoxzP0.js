import{_ as d,c as s,o as e,ae as a}from"./chunks/framework.DUP9kEI5.js";const k=JSON.parse('{"title":"番外篇: 微调OpenAI OSS模型","description":"","frontmatter":{},"headers":[],"relativePath":"guide/fine-tune-openai-oss-model.md","filePath":"guide/fine-tune-openai-oss-model.md"}'),i={name:"guide/fine-tune-openai-oss-model.md"};function o(r,t,h,n,l,c){return e(),s("div",null,t[0]||(t[0]=[a('<h1 id="番外篇-微调openai-oss模型" tabindex="-1">番外篇: 微调OpenAI OSS模型 <a class="header-anchor" href="#番外篇-微调openai-oss模型" aria-label="Permalink to &quot;番外篇: 微调OpenAI OSS模型&quot;">​</a></h1><h2 id="此篇内容已经过实机测试-vgpu-32g" tabindex="-1">此篇内容已经过实机测试(vgpu 32g) <a class="header-anchor" href="#此篇内容已经过实机测试-vgpu-32g" aria-label="Permalink to &quot;此篇内容已经过实机测试(vgpu 32g)&quot;">​</a></h2><h3 id="如有bug请发issues或联系作者" tabindex="-1">如有Bug请发Issues或联系作者 <a class="header-anchor" href="#如有bug请发issues或联系作者" aria-label="Permalink to &quot;如有Bug请发Issues或联系作者&quot;">​</a></h3><h3 id="能直接pr修复那就更棒了" tabindex="-1"><s>能直接PR修复那就更棒了</s> <a class="header-anchor" href="#能直接pr修复那就更棒了" aria-label="Permalink to &quot;~~能直接PR修复那就更棒了~~&quot;">​</a></h3><hr><h1 id="暂时请不要尝试微调oss模型" tabindex="-1">暂时请不要尝试微调OSS模型 <a class="header-anchor" href="#暂时请不要尝试微调oss模型" aria-label="Permalink to &quot;暂时请不要尝试微调OSS模型&quot;">​</a></h1><h1 id="根据测试还存在大量的微调bug" tabindex="-1">根据测试还存在大量的微调Bug <a class="header-anchor" href="#根据测试还存在大量的微调bug" aria-label="Permalink to &quot;根据测试还存在大量的微调Bug&quot;">​</a></h1><h1 id="如果你是勇士-当我没说" tabindex="-1"><s>如果你是勇士,当我没说</s> <a class="header-anchor" href="#如果你是勇士-当我没说" aria-label="Permalink to &quot;~~如果你是勇士,当我没说~~&quot;">​</a></h1><img src="https://cdn.nodeimage.com/i/yHMIuFusDfJkDupyVyEdKNE1fUBiDy4C.png" alt="yHMIuFusDfJkDupyVyEdKNE1fUBiDy4C.png"><blockquote><p>测试了挺久,还是没解决,欢迎大家测试给此项目提PR或者给Unsloth提Issues</p></blockquote><h3 id="微调oss模型" tabindex="-1">微调OSS模型 <a class="header-anchor" href="#微调oss模型" aria-label="Permalink to &quot;微调OSS模型&quot;">​</a></h3><blockquote><p>因为OSS发布的时间,似乎微调OSS和Qwen的并不能通用<br> 并且最好使用新的<code>unsloth</code> <code>torch</code> <code>transformers</code> 等库</p></blockquote><p><a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/gpt-oss-(20B)-Fine-tuning.ipynb" target="_blank" rel="noreferrer">这是Unsloth提供的OSS微调经验</a></p><h2 id="环境配置" tabindex="-1">环境配置 <a class="header-anchor" href="#环境配置" aria-label="Permalink to &quot;环境配置&quot;">​</a></h2><blockquote><p>建议使用新的虚拟环境<br> 和Qwen的微调环境分离<br> 原requirements.txt的unsloth只支持到2025.8.1版本,并不能微调oss</p></blockquote><h3 id="先运行以下命令再安装依赖" tabindex="-1">先运行以下命令再安装依赖 <a class="header-anchor" href="#先运行以下命令再安装依赖" aria-label="Permalink to &quot;先运行以下命令再安装依赖&quot;">​</a></h3><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">pip</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> install</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;unsloth_zoo[base] @ git+https://github.com/unslothai/unsloth-zoo&quot;</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;unsloth[base] @ git+https://github.com/unslothai/unsloth&quot;</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> torchvision</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> bitsandbytes</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> git+https://github.com/huggingface/transformers</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> git+https://github.com/triton-lang/triton.git@main#subdirectory=python/triton_kernels</span></span></code></pre></div><h3 id="安装依赖" tabindex="-1">安装依赖 <a class="header-anchor" href="#安装依赖" aria-label="Permalink to &quot;安装依赖&quot;">​</a></h3><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">pip</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> install</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -r</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> requirements_oss.txt</span></span></code></pre></div><h2 id="请注意-此模型需要使用openai-harmony格式的训练数据进行微调" tabindex="-1"><strong>请注意,此模型需要使用OpenAI Harmony格式的训练数据进行微调</strong> <a class="header-anchor" href="#请注意-此模型需要使用openai-harmony格式的训练数据进行微调" aria-label="Permalink to &quot;**请注意,此模型需要使用OpenAI Harmony格式的训练数据进行微调**&quot;">​</a></h2><p>请使用chatml_to_harmony.py将chatml格式的训练数据转换为harmony格式</p><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">python3</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> chatml_to_harmony.py</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --input</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> training_data.jsonl</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --output</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> training_data_harmony.txt</span></span></code></pre></div><h3 id="下载模型" tabindex="-1">下载模型 <a class="header-anchor" href="#下载模型" aria-label="Permalink to &quot;下载模型&quot;">​</a></h3><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">huggingface-cli</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> download</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> unsloth/gpt-oss-20b-unsloth-bnb-4bit</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --local-dir</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> gpt-oss-20b</span></span></code></pre></div><blockquote><p>如果没有huggingface-cli,请先安装</p></blockquote><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">pip</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> install</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> huggingface-hub</span></span></code></pre></div><blockquote><p>如果需要镜像站请先运行</p></blockquote><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">export</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> HF_ENDPOINT</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">https://hf-mirror.com</span></span></code></pre></div><h2 id="开始微调" tabindex="-1">开始微调 <a class="header-anchor" href="#开始微调" aria-label="Permalink to &quot;开始微调&quot;">​</a></h2><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">python3</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> run_finetune_oss.py</span></span></code></pre></div><hr><table tabindex="0"><thead><tr><th>参数</th><th>类型</th><th>默认值</th><th>可选值</th><th>说明</th></tr></thead><tbody><tr><td><code>--repo_id</code></td><td>str</td><td><code>unsloth/gpt-oss-20b-unsloth-bnb-4bit</code></td><td>-</td><td>HF 仓库ID</td></tr><tr><td><code>--local_dir</code></td><td>str</td><td><code>gpt-oss-20b-unsloth-bnb-4bit</code></td><td>-</td><td>本地模型目录</td></tr><tr><td><code>--use_unsloth</code></td><td>str</td><td><code>false</code></td><td><code>true</code>, <code>false</code></td><td>是否使用unsloth</td></tr><tr><td><code>--use_qlora</code></td><td>str</td><td><code>true</code></td><td><code>true</code>, <code>false</code></td><td>是否使用QLoRA</td></tr><tr><td><code>--data_path</code></td><td>str</td><td><code>training_data.jsonl</code></td><td>-</td><td>训练数据路径</td></tr><tr><td><code>--eval_data_path</code></td><td>str / None</td><td>None</td><td>-</td><td>验证数据路径</td></tr><tr><td><code>--max_samples</code></td><td>str / None</td><td>None</td><td>-</td><td>最大训练样本数</td></tr><tr><td><code>--max_eval_samples</code></td><td>str / None</td><td>None</td><td>-</td><td>最大验证样本数</td></tr><tr><td><code>--model_max_length</code></td><td>str</td><td><code>2048</code></td><td>-</td><td>最大序列长度</td></tr><tr><td><code>--output_dir</code></td><td>str</td><td><code>finetune/models/qwen3-30b-a3b-qlora</code></td><td>-</td><td>输出目录</td></tr><tr><td><code>--seed</code></td><td>str</td><td><code>42</code></td><td>-</td><td>随机种子</td></tr><tr><td><code>--per_device_train_batch_size</code></td><td>str</td><td><code>1</code></td><td>-</td><td>每设备训练批次大小</td></tr><tr><td><code>--per_device_eval_batch_size</code></td><td>str</td><td><code>1</code></td><td>-</td><td>每设备验证批次大小</td></tr><tr><td><code>--gradient_accumulation_steps</code></td><td>str</td><td><code>16</code></td><td>-</td><td>梯度累积步数</td></tr><tr><td><code>--learning_rate</code></td><td>str</td><td><code>2e-4</code></td><td>-</td><td>学习率</td></tr><tr><td><code>--num_train_epochs</code></td><td>str</td><td><code>3</code></td><td>-</td><td>训练轮数</td></tr><tr><td><code>--max_steps</code></td><td>str</td><td><code>-1</code></td><td>-</td><td>最大步数（-1为不限）</td></tr><tr><td><code>--lora_r</code></td><td>str</td><td><code>16</code></td><td>-</td><td>LoRA 秩</td></tr><tr><td><code>--lora_alpha</code></td><td>str</td><td><code>32</code></td><td>-</td><td>LoRA alpha</td></tr><tr><td><code>--lora_dropout</code></td><td>str</td><td><code>0.05</code></td><td>-</td><td>LoRA dropout率</td></tr><tr><td><code>--target_modules</code></td><td>str</td><td><code>q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj</code></td><td>-</td><td>LoRA 目标模块</td></tr><tr><td><code>--weight_decay</code></td><td>str</td><td><code>0.0</code></td><td>-</td><td>权重衰减</td></tr><tr><td><code>--moe_enable</code></td><td>str</td><td><code>false</code></td><td><code>true</code>, <code>false</code></td><td>是否启用 MoE</td></tr><tr><td><code>--moe_lora_scope</code></td><td>str</td><td><code>expert_only</code></td><td><code>expert_only</code>, <code>router_only</code>, <code>all</code></td><td>LoRA 注入范围</td></tr><tr><td><code>--moe_expert_patterns</code></td><td>str</td><td><code>experts.ffn.(gate_proj|up_proj|down_proj),layers.[0-9]+.mlp.experts.[0-9]+.(w1|w2|w3)</code></td><td>-</td><td>专家线性层模式（正则）</td></tr><tr><td><code>--moe_router_patterns</code></td><td>str</td><td><code>router.(gate|dense)</code></td><td>-</td><td>路由/门控层模式（正则）</td></tr><tr><td><code>--moe_max_experts_lora</code></td><td>str</td><td><code>-1</code></td><td>-</td><td>每层最多注入 LoRA 的专家数</td></tr><tr><td><code>--moe_dry_run</code></td><td>str</td><td><code>false</code></td><td><code>true</code>, <code>false</code></td><td>仅打印匹配模块并退出</td></tr><tr><td><code>--load_precision</code></td><td>str</td><td><code>fp16</code></td><td><code>int8</code>, <code>int4</code>, <code>fp16</code></td><td>模型加载精度</td></tr><tr><td><code>--use_flash_attention_2</code></td><td>str</td><td><code>false</code></td><td><code>true</code>, <code>false</code></td><td>是否启用 FlashAttention2</td></tr><tr><td><code>--logging_steps</code></td><td>str</td><td><code>1</code></td><td>-</td><td>日志记录步数</td></tr><tr><td><code>--eval_steps</code></td><td>str</td><td><code>50</code></td><td>-</td><td>验证间隔步数</td></tr><tr><td><code>--save_steps</code></td><td>str</td><td><code>200</code></td><td>-</td><td>保存模型步数</td></tr><tr><td><code>--save_total_limit</code></td><td>str</td><td><code>2</code></td><td>-</td><td>最多保存数</td></tr><tr><td><code>--warmup_ratio</code></td><td>str</td><td><code>0.05</code></td><td>-</td><td>预热比例</td></tr><tr><td><code>--lr_scheduler_type</code></td><td>str</td><td><code>cosine</code></td><td>-</td><td>学习率调度器类型</td></tr><tr><td><code>--resume_from_checkpoint</code></td><td>str / None</td><td>None</td><td>-</td><td>从检查点恢复</td></tr><tr><td><code>--no-gradient_checkpointing</code></td><td>flag</td><td>False</td><td>-</td><td>不使用梯度检查点</td></tr><tr><td><code>--no-merge_and_save</code></td><td>flag</td><td>False</td><td>-</td><td>不合并并保存模型</td></tr><tr><td><code>--fp16</code></td><td>str</td><td><code>true</code></td><td><code>true</code>, <code>false</code></td><td>是否使用fp16</td></tr><tr><td><code>--optim</code></td><td>str</td><td><code>adamw_torch_fused</code></td><td>-</td><td>优化器</td></tr><tr><td><code>--dataloader_pin_memory</code></td><td>str</td><td><code>false</code></td><td><code>true</code>, <code>false</code></td><td>是否固定数据加载器内存</td></tr><tr><td><code>--dataloader_num_workers</code></td><td>str</td><td><code>0</code></td><td>-</td><td>DataLoader 线程数</td></tr><tr><td><code>--dataloader_prefetch_factor</code></td><td>str</td><td><code>2</code></td><td>-</td><td>DataLoader 预取因子</td></tr><tr><td><code>--use_gradient_checkpointing</code></td><td>str</td><td><code>true</code></td><td><code>true</code>, <code>false</code>, <code>unsloth</code></td><td>梯度检查点设置</td></tr><tr><td><code>--full_finetuning</code></td><td>str</td><td><code>false</code></td><td><code>true</code>, <code>false</code></td><td>是否全量微调</td></tr><tr><td><code>--data_format</code></td><td>str</td><td><code>harmony</code></td><td><code>harmony</code>, <code>jsonl</code></td><td>数据格式</td></tr></tbody></table><hr><blockquote><p>下面是一个微调<code>gpt-oss-20b-unsloth-bnb-4bit</code>的范例,请根据需要修改</p></blockquote><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">python3</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> run_finetune_oss.py</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --output_dir</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> /root/autodl-fs/gpt-oss-20b</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --local_dir</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> /root/autodl-tmp/gpt-oss-20b</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --data_path</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> ./harmony_small.txt</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --eval_data_path</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> ./harmony_small_eval.txt</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --use_qlora</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> true</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --lora_dropout</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 0.05</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --num_train_epochs</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 2</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --per_device_train_batch_size</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 4</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --per_device_eval_batch_size</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 4</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --gradient_accumulation_steps</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 8</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --learning_rate</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> 2e-5</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --lr_scheduler</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> cosine</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --logging_steps</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 5</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --eval_steps</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 40</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --save_steps</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 200</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --warmup_ratio</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 0.05</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --dataloader_num_workers</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 16</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --fp16</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> true</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --use_unsloth</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> true</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --dataloader_prefetch_factor</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 4</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --load_precision</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> int4</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --data_format</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> harmony</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --save_gguf</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> true</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --gguf_quantization</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> f16</span></span></code></pre></div>',35)]))}const g=d(i,[["render",o]]);export{k as __pageData,g as default};
