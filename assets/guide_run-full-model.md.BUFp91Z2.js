import{_ as a,c as s,o as e,ae as i}from"./chunks/framework.DUP9kEI5.js";const k=JSON.parse('{"title":"","description":"","frontmatter":{},"headers":[],"relativePath":"guide/run-full-model.md","filePath":"guide/run-full-model.md"}'),d={name:"guide/run-full-model.md"};function h(l,t,r,n,o,p){return e(),s("div",null,t[0]||(t[0]=[i('<h2 id="不建议-微调后直接运行全量模型-建议直接看第4-5-6-7步-等转换为guff并量化完再跑" tabindex="-1">(不建议)微调后直接运行全量模型(建议直接看第4,5,6,7步,等转换为guff并量化完再跑) <a class="header-anchor" href="#不建议-微调后直接运行全量模型-建议直接看第4-5-6-7步-等转换为guff并量化完再跑" aria-label="Permalink to &quot;(不建议)微调后直接运行全量模型(建议直接看第4,5,6,7步,等转换为guff并量化完再跑)&quot;">​</a></h2><h3 id="指定自定义模型路径" tabindex="-1">指定自定义模型路径 <a class="header-anchor" href="#指定自定义模型路径" aria-label="Permalink to &quot;指定自定义模型路径&quot;">​</a></h3><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">python</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> infer_lora_chat.py</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --base_dir</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> my-base-model</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --adapter_dir</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> my-lora-adapter</span></span></code></pre></div><h3 id="使用合并后的模型" tabindex="-1">使用合并后的模型 <a class="header-anchor" href="#使用合并后的模型" aria-label="Permalink to &quot;使用合并后的模型&quot;">​</a></h3><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">python</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> infer_lora_chat.py</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --merged</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> true</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --adapter_dir</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> my-lora-adapter</span></span></code></pre></div><h3 id="调整生成参数" tabindex="-1">调整生成参数 <a class="header-anchor" href="#调整生成参数" aria-label="Permalink to &quot;调整生成参数&quot;">​</a></h3><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">python</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> infer_lora_chat.py</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --temperature</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 0.9</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --top_p</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 0.95</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --max_new_tokens</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 1024</span></span></code></pre></div><h3 id="使用自定义系统提示词" tabindex="-1">使用自定义系统提示词 <a class="header-anchor" href="#使用自定义系统提示词" aria-label="Permalink to &quot;使用自定义系统提示词&quot;">​</a></h3><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">python</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> infer_lora_chat.py</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --system_prompt</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;你是一个乐于助人的AI助手。&quot;</span></span></code></pre></div><h3 id="命令行参数说明" tabindex="-1">命令行参数说明 <a class="header-anchor" href="#命令行参数说明" aria-label="Permalink to &quot;命令行参数说明&quot;">​</a></h3><table tabindex="0"><thead><tr><th>参数名称</th><th>类型</th><th>默认值</th><th>描述</th></tr></thead><tbody><tr><td><code>--base_dir</code></td><td>str</td><td><code>qwen3-8b-base</code></td><td>基础模型目录</td></tr><tr><td><code>--adapter_dir</code></td><td>str</td><td><code>finetune/models/qwen3-8b-qlora</code></td><td>LoRA适配器目录</td></tr><tr><td><code>--merged</code></td><td>bool</td><td><code>False</code></td><td>如果为True，则从adapter_dir/merged加载合并后的完整权重</td></tr><tr><td><code>--system_prompt</code></td><td>str</td><td>清凤数字分身人设</td><td>模型的系统提示词</td></tr><tr><td><code>--max_new_tokens</code></td><td>int</td><td><code>512</code></td><td>生成的最大新token数量</td></tr><tr><td><code>--temperature</code></td><td>float</td><td><code>0.7</code></td><td>采样温度</td></tr><tr><td><code>--top_p</code></td><td>float</td><td><code>0.9</code></td><td>Top-p采样参数</td></tr><tr><td><code>--trust_remote_code</code></td><td>bool</td><td><code>True</code></td><td>是否信任远程代码</td></tr></tbody></table>',11)]))}const u=a(d,[["render",h]]);export{k as __pageData,u as default};
