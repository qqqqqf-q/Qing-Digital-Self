import{_ as e,c as d,o as s,ae as a}from"./chunks/framework.DUP9kEI5.js";const k=JSON.parse('{"title":"Extra: Fine-tuning OpenAI OSS Model","description":"","frontmatter":{},"headers":[],"relativePath":"en/guide/fine-tune-openai-oss-model.md","filePath":"en/guide/fine-tune-openai-oss-model.md"}'),i={name:"en/guide/fine-tune-openai-oss-model.md"};function o(n,t,r,l,h,c){return s(),d("div",null,t[0]||(t[0]=[a('<h1 id="extra-fine-tuning-openai-oss-model" tabindex="-1">Extra: Fine-tuning OpenAI OSS Model <a class="header-anchor" href="#extra-fine-tuning-openai-oss-model" aria-label="Permalink to &quot;Extra: Fine-tuning OpenAI OSS Model&quot;">​</a></h1><h2 id="this-has-been-tested-on-real-hardware-vgpu-32g" tabindex="-1">This has been tested on real hardware (vgpu 32G) <a class="header-anchor" href="#this-has-been-tested-on-real-hardware-vgpu-32g" aria-label="Permalink to &quot;This has been tested on real hardware (vgpu 32G)&quot;">​</a></h2><h3 id="if-you-find-bugs-please-open-an-issue-or-contact-the-author" tabindex="-1">If you find bugs, please open an issue or contact the author <a class="header-anchor" href="#if-you-find-bugs-please-open-an-issue-or-contact-the-author" aria-label="Permalink to &quot;If you find bugs, please open an issue or contact the author&quot;">​</a></h3><h3 id="even-better-if-you-submit-a-pr-fix" tabindex="-1"><s>Even better if you submit a PR fix</s> <a class="header-anchor" href="#even-better-if-you-submit-a-pr-fix" aria-label="Permalink to &quot;~~Even better if you submit a PR fix~~&quot;">​</a></h3><hr><h1 id="do-not-attempt-to-fine-tune-oss-models-for-now" tabindex="-1">Do not attempt to fine-tune OSS models for now <a class="header-anchor" href="#do-not-attempt-to-fine-tune-oss-models-for-now" aria-label="Permalink to &quot;Do not attempt to fine-tune OSS models for now&quot;">​</a></h1><h1 id="based-on-testing-there-are-still-many-fine-tuning-bugs" tabindex="-1">Based on testing, there are still many fine-tuning bugs <a class="header-anchor" href="#based-on-testing-there-are-still-many-fine-tuning-bugs" aria-label="Permalink to &quot;Based on testing, there are still many fine-tuning bugs&quot;">​</a></h1><h1 id="if-you-re-a-brave-soul-ignore-this-warning" tabindex="-1"><s>If you’re a brave soul, ignore this warning</s> <a class="header-anchor" href="#if-you-re-a-brave-soul-ignore-this-warning" aria-label="Permalink to &quot;~~If you’re a brave soul, ignore this warning~~&quot;">​</a></h1><img src="https://cdn.nodeimage.com/i/yHMIuFusDfJkDupyVyEdKNE1fUBiDy4C.png" alt="yHMIuFusDfJkDupyVyEdKNE1fUBiDy4C.png"><blockquote><p>After long testing, still unresolved. Contributions welcome: submit PRs to this project or open issues with Unsloth.</p></blockquote><hr><h3 id="fine-tuning-oss-models" tabindex="-1">Fine-tuning OSS Models <a class="header-anchor" href="#fine-tuning-oss-models" aria-label="Permalink to &quot;Fine-tuning OSS Models&quot;">​</a></h3><blockquote><p>Because of OSS’s release timing, fine-tuning OSS is not interchangeable with Qwen. It’s best to use the latest versions of <code>unsloth</code>, <code>torch</code>, <code>transformers</code>, etc.</p></blockquote><p><a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/gpt-oss-%2820B%29-Fine-tuning.ipynb" target="_blank" rel="noreferrer">Here is Unsloth’s OSS fine-tuning notebook</a></p><hr><h2 id="environment-setup" tabindex="-1">Environment Setup <a class="header-anchor" href="#environment-setup" aria-label="Permalink to &quot;Environment Setup&quot;">​</a></h2><blockquote><p>It’s recommended to create a fresh virtual environment, separated from your Qwen fine-tuning environment. The <code>unsloth</code> in the original requirements.txt only supports up to 2025.8.1 and cannot fine-tune OSS.</p></blockquote><h3 id="run-this-command-before-installing-dependencies" tabindex="-1">Run this command before installing dependencies <a class="header-anchor" href="#run-this-command-before-installing-dependencies" aria-label="Permalink to &quot;Run this command before installing dependencies&quot;">​</a></h3><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">pip</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> install</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;unsloth_zoo[base] @ git+https://github.com/unslothai/unsloth-zoo&quot;</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;unsloth[base] @ git+https://github.com/unslothai/unsloth&quot;</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> torchvision</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> bitsandbytes</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> git+https://github.com/huggingface/transformers</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> git+https://github.com/triton-lang/triton.git@main#subdirectory=python/triton_kernels</span></span></code></pre></div><h3 id="install-dependencies" tabindex="-1">Install dependencies <a class="header-anchor" href="#install-dependencies" aria-label="Permalink to &quot;Install dependencies&quot;">​</a></h3><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">pip</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> install</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -r</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> requirements_oss.txt</span></span></code></pre></div><hr><h2 id="important-this-model-requires-training-data-in-openai-harmony-format" tabindex="-1"><strong>Important: This model requires training data in OpenAI Harmony format</strong> <a class="header-anchor" href="#important-this-model-requires-training-data-in-openai-harmony-format" aria-label="Permalink to &quot;**Important: This model requires training data in OpenAI Harmony format**&quot;">​</a></h2><p>Convert your ChatML-format training data into Harmony format using:</p><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">python3</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> chatml_to_harmony.py</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --input</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> training_data.jsonl</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --output</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> training_data_harmony.txt</span></span></code></pre></div><hr><h3 id="download-the-model" tabindex="-1">Download the model <a class="header-anchor" href="#download-the-model" aria-label="Permalink to &quot;Download the model&quot;">​</a></h3><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">huggingface-cli</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> download</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> unsloth/gpt-oss-20b-unsloth-bnb-4bit</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --local-dir</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> gpt-oss-20b</span></span></code></pre></div><blockquote><p>If you don’t have <code>huggingface-cli</code>, install it:</p></blockquote><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">pip</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> install</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> huggingface-hub</span></span></code></pre></div><blockquote><p>For mirror sites:</p></blockquote><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">export</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> HF_ENDPOINT</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">https://hf-mirror.com</span></span></code></pre></div><hr><h2 id="start-fine-tuning" tabindex="-1">Start Fine-tuning <a class="header-anchor" href="#start-fine-tuning" aria-label="Permalink to &quot;Start Fine-tuning&quot;">​</a></h2><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">python3</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> run_finetune_oss.py</span></span></code></pre></div><hr><table tabindex="0"><thead><tr><th>Parameter</th><th>Type</th><th>Default Value</th><th>Options</th><th>Description</th></tr></thead><tbody><tr><td><code>--repo_id</code></td><td>str</td><td><code>unsloth/gpt-oss-20b-unsloth-bnb-4bit</code></td><td>-</td><td>HF repo ID</td></tr><tr><td><code>--local_dir</code></td><td>str</td><td><code>gpt-oss-20b-unsloth-bnb-4bit</code></td><td>-</td><td>Local model directory</td></tr><tr><td><code>--use_unsloth</code></td><td>str</td><td><code>false</code></td><td><code>true</code>, <code>false</code></td><td>Whether to use Unsloth</td></tr><tr><td><code>--use_qlora</code></td><td>str</td><td><code>true</code></td><td><code>true</code>, <code>false</code></td><td>Whether to use QLoRA</td></tr><tr><td><code>--data_path</code></td><td>str</td><td><code>training_data.jsonl</code></td><td>-</td><td>Training data path</td></tr><tr><td><code>--eval_data_path</code></td><td>str/None</td><td>None</td><td>-</td><td>Evaluation data path</td></tr><tr><td><code>--max_samples</code></td><td>str/None</td><td>None</td><td>-</td><td>Maximum training samples</td></tr><tr><td><code>--max_eval_samples</code></td><td>str/None</td><td>None</td><td>-</td><td>Maximum evaluation samples</td></tr><tr><td><code>--model_max_length</code></td><td>str</td><td><code>2048</code></td><td>-</td><td>Max sequence length</td></tr><tr><td><code>--output_dir</code></td><td>str</td><td><code>finetune/models/qwen3-30b-a3b-qlora</code></td><td>-</td><td>Output directory</td></tr><tr><td><code>--seed</code></td><td>str</td><td><code>42</code></td><td>-</td><td>Random seed</td></tr><tr><td><code>--per_device_train_batch_size</code></td><td>str</td><td><code>1</code></td><td>-</td><td>Train batch size per device</td></tr><tr><td><code>--per_device_eval_batch_size</code></td><td>str</td><td><code>1</code></td><td>-</td><td>Eval batch size per device</td></tr><tr><td><code>--gradient_accumulation_steps</code></td><td>str</td><td><code>16</code></td><td>-</td><td>Gradient accumulation steps</td></tr><tr><td><code>--learning_rate</code></td><td>str</td><td><code>2e-4</code></td><td>-</td><td>Learning rate</td></tr><tr><td><code>--num_train_epochs</code></td><td>str</td><td><code>3</code></td><td>-</td><td>Number of training epochs</td></tr><tr><td><code>--max_steps</code></td><td>str</td><td><code>-1</code></td><td>-</td><td>Max steps (-1 = unlimited)</td></tr><tr><td><code>--lora_r</code></td><td>str</td><td><code>16</code></td><td>-</td><td>LoRA rank</td></tr><tr><td><code>--lora_alpha</code></td><td>str</td><td><code>32</code></td><td>-</td><td>LoRA alpha</td></tr><tr><td><code>--lora_dropout</code></td><td>str</td><td><code>0.05</code></td><td>-</td><td>LoRA dropout rate</td></tr><tr><td><code>--target_modules</code></td><td>str</td><td><code>q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj</code></td><td>-</td><td>LoRA target modules</td></tr><tr><td><code>--weight_decay</code></td><td>str</td><td><code>0.0</code></td><td>-</td><td>Weight decay</td></tr><tr><td><code>--moe_enable</code></td><td>str</td><td><code>false</code></td><td><code>true</code>, <code>false</code></td><td>Enable MoE</td></tr><tr><td><code>--moe_lora_scope</code></td><td>str</td><td><code>expert_only</code></td><td><code>expert_only</code>, <code>router_only</code>, <code>all</code></td><td>LoRA injection scope</td></tr><tr><td><code>--moe_expert_patterns</code></td><td>str</td><td><code>experts.ffn.(gate_proj|up_proj|down_proj),layers.[0-9]+.mlp.experts.[0-9]+.(w1|w2|w3)</code></td><td>-</td><td>Expert linear layer regex</td></tr><tr><td><code>--moe_router_patterns</code></td><td>str</td><td><code>router.(gate|dense)</code></td><td>-</td><td>Router/gate layer regex</td></tr><tr><td><code>--moe_max_experts_lora</code></td><td>str</td><td><code>-1</code></td><td>-</td><td>Max LoRA experts per layer</td></tr><tr><td><code>--moe_dry_run</code></td><td>str</td><td><code>false</code></td><td><code>true</code>, <code>false</code></td><td>Dry-run (only print matches)</td></tr><tr><td><code>--load_precision</code></td><td>str</td><td><code>fp16</code></td><td><code>int8</code>, <code>int4</code>, <code>fp16</code></td><td>Model load precision</td></tr><tr><td><code>--use_flash_attention_2</code></td><td>str</td><td><code>false</code></td><td><code>true</code>, <code>false</code></td><td>Enable FlashAttention2</td></tr><tr><td><code>--logging_steps</code></td><td>str</td><td><code>1</code></td><td>-</td><td>Logging steps</td></tr><tr><td><code>--eval_steps</code></td><td>str</td><td><code>50</code></td><td>-</td><td>Eval interval</td></tr><tr><td><code>--save_steps</code></td><td>str</td><td><code>200</code></td><td>-</td><td>Save checkpoint steps</td></tr><tr><td><code>--save_total_limit</code></td><td>str</td><td><code>2</code></td><td>-</td><td>Max checkpoints to keep</td></tr><tr><td><code>--warmup_ratio</code></td><td>str</td><td><code>0.05</code></td><td>-</td><td>Warmup ratio</td></tr><tr><td><code>--lr_scheduler_type</code></td><td>str</td><td><code>cosine</code></td><td>-</td><td>LR scheduler type</td></tr><tr><td><code>--resume_from_checkpoint</code></td><td>str/None</td><td>None</td><td>-</td><td>Resume from checkpoint</td></tr><tr><td><code>--no-gradient_checkpointing</code></td><td>flag</td><td>False</td><td>-</td><td>Disable gradient checkpointing</td></tr><tr><td><code>--no-merge_and_save</code></td><td>flag</td><td>False</td><td>-</td><td>Skip merge and save</td></tr><tr><td><code>--fp16</code></td><td>str</td><td><code>true</code></td><td><code>true</code>, <code>false</code></td><td>Use FP16</td></tr><tr><td><code>--optim</code></td><td>str</td><td><code>adamw_torch_fused</code></td><td>-</td><td>Optimizer</td></tr><tr><td><code>--dataloader_pin_memory</code></td><td>str</td><td><code>false</code></td><td><code>true</code>, <code>false</code></td><td>Pin dataloader memory</td></tr><tr><td><code>--dataloader_num_workers</code></td><td>str</td><td><code>0</code></td><td>-</td><td>Dataloader workers</td></tr><tr><td><code>--dataloader_prefetch_factor</code></td><td>str</td><td><code>2</code></td><td>-</td><td>Dataloader prefetch factor</td></tr><tr><td><code>--use_gradient_checkpointing</code></td><td>str</td><td><code>true</code></td><td><code>true</code>, <code>false</code>, <code>unsloth</code></td><td>Gradient checkpointing mode</td></tr><tr><td><code>--full_finetuning</code></td><td>str</td><td><code>false</code></td><td><code>true</code>, <code>false</code></td><td>Full model fine-tuning</td></tr><tr><td><code>--data_format</code></td><td>str</td><td><code>harmony</code></td><td><code>harmony</code>, <code>jsonl</code></td><td>Data format</td></tr></tbody></table><hr><blockquote><p>Example fine-tuning run for <code>gpt-oss-20b-unsloth-bnb-4bit</code> (adjust as needed):</p></blockquote><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">python3</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> run_finetune_oss.py</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --output_dir</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> /root/autodl-fs/gpt-oss-20b</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --local_dir</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> /root/autodl-tmp/gpt-oss-20b</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --data_path</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> ./harmony_small.txt</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --eval_data_path</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> ./harmony_small_eval.txt</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --use_qlora</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> true</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --lora_dropout</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 0.05</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --num_train_epochs</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 2</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --per_device_train_batch_size</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 4</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --per_device_eval_batch_size</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 4</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --gradient_accumulation_steps</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 8</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --learning_rate</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> 2e-5</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --lr_scheduler</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> cosine</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --logging_steps</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 5</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --eval_steps</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 40</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --save_steps</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 200</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --warmup_ratio</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 0.05</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --dataloader_num_workers</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 16</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --fp16</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> true</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --use_unsloth</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> true</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --dataloader_prefetch_factor</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 4</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --load_precision</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> int4</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --data_format</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> harmony</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --save_gguf</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> true</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --gguf_quantization</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> f16</span></span></code></pre></div>',40)]))}const u=e(i,[["render",o]]);export{k as __pageData,u as default};
