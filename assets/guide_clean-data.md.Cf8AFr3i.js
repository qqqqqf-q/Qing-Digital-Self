import{_ as s,c as a,o as e,ae as l}from"./chunks/framework.DUP9kEI5.js";const c=JSON.parse('{"title":"","description":"","frontmatter":{},"headers":[],"relativePath":"guide/clean-data.md","filePath":"guide/clean-data.md"}'),t={name:"guide/clean-data.md"};function n(p,i,h,o,d,k){return e(),a("div",null,i[0]||(i[0]=[l(`<h2 id="_2-1-清洗数据-普通版本-llm清洗版本在下一章节" tabindex="-1">2.1 .清洗数据(普通版本,llm清洗版本在下一章节) <a class="header-anchor" href="#_2-1-清洗数据-普通版本-llm清洗版本在下一章节" aria-label="Permalink to &quot;2.1 .清洗数据(普通版本,llm清洗版本在下一章节)&quot;">​</a></h2><blockquote><p>此方法比llm清洗快得多,30w条消息半分钟就好了 但是对应的质量也更低 这个部分建议在windows上优化完再上传至GPU服务器<br> 不确定在Linux上有没有兼容性问题</p></blockquote><ul><li><p>在 <code>.env</code> 文件中修改数据库路径及相关参数(请注意其中的必填段)</p></li><li><p>运行清洗脚本：</p><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">python</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> generate_training_data.py</span></span></code></pre></div></li></ul><h2 id="_2-2-清洗数据-llm清洗" tabindex="-1">2.2 .清洗数据(llm清洗) <a class="header-anchor" href="#_2-2-清洗数据-llm清洗" aria-label="Permalink to &quot;2.2 .清洗数据(llm清洗)&quot;">​</a></h2><blockquote><p>需要配置一个OpenAI兼容的API<br> 比如:LM Studio 或者 vLLM(速度更快,但搭建更麻烦,需要Linux环境)</p></blockquote><blockquote><p>这个部分同样建议在windows上优化完再上传至GPU服务器<br> 不确定在Linux上有没有兼容性问题</p></blockquote><h2 id="lm-studio搭建教程" tabindex="-1">LM Studio搭建教程 <a class="header-anchor" href="#lm-studio搭建教程" aria-label="Permalink to &quot;LM Studio搭建教程&quot;">​</a></h2><ul><li>1.前往<a href="https://lmstudio.ai/" target="_blank" rel="noreferrer">LM Studio</a>下载LM Studio</li><li>2.安装LM Studio</li><li>3.打开LM Studio,点击左侧<code>搜索</code>-&gt;<code>Model Search</code></li><li>4.搜索 <code>qwen3 8b</code>-&gt;<code>Complete Download</code></li><li>5.选择合适你的量化版本<strong>建议至少Q4,最好Q6-Q8,随你的设备情况而定,不知道的可以问AI</strong></li><li>记住你的<strong>模型名称</strong>,填写到<code>.env</code>文件的<code>Openai_model</code>中</li><li>如果不知道你的模型名称可以运行test_openai.py,会输出所有的模型名称</li><li>6.安装好后,在左侧<code>开发者/Developer</code>点击<code>Status:Stopped</code>右边的按钮</li><li>如果下面log显示端口被占用请点击<code>seetings</code>换个<code>server port</code></li><li>记住这个<code>server port</code>,将你的配置填写至<code>.env</code>文件中</li></ul><h3 id="run" tabindex="-1">run! <a class="header-anchor" href="#run" aria-label="Permalink to &quot;run!&quot;">​</a></h3><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">python</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> generate_training_data_llm.py</span></span></code></pre></div><blockquote><p>如果遇到了400报错大概率是因为message太大了被模型框架拒绝了</p></blockquote><hr><h2 id="vllm搭建" tabindex="-1">vLLM搭建 <a class="header-anchor" href="#vllm搭建" aria-label="Permalink to &quot;vLLM搭建&quot;">​</a></h2><blockquote><p>vLLM需要linux环境!<br> 如果你的显卡还算可以(&gt;6800xt,&gt;3080)<br> 可以选择使用lmstudio,多等一会就好了,还可以玩玩模型 缺点是lmstudio不能运行hf模型,且并发很烂</p></blockquote><blockquote><p>vLLM比Lm studio吃显存的多! Lm studio可以运行8b_q6到vLLM上只能运行4b_Q6</p></blockquote><blockquote><p>不过并发效率的提升是真的</p></blockquote><blockquote><p>但是!上下文很短,如果一天有超过500条消息就处理不过来了</p></blockquote><blockquote><p>3080实测4b_q6处理,最终jsonl的速率大约是<strong>300kb/minute</strong></p></blockquote><ul><li>跟着走就能搭建</li></ul><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">sudo</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> apt</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> update</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">sudo</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> apt</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> install</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> python3.10-venv</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> git</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -y</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">python3</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -m</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> venv</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> vllm_env</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">source</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> vllm_env/bin/activate</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">pip</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> install</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -U</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> pip</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">pip</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> install</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> torch</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --index-url</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> https://download.pytorch.org/whl/cu121</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">  # 如果你用CUDA</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">pip</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> install</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> vllm</span></span></code></pre></div><h3 id="和lm-studio不同的注意点" tabindex="-1">和lm studio不同的注意点 <a class="header-anchor" href="#和lm-studio不同的注意点" aria-label="Permalink to &quot;和lm studio不同的注意点&quot;">​</a></h3><ul><li>1.<code>.env</code>中的<code>Openai_model</code>需要设置路径而不只是文件夹名</li></ul><blockquote><p>是<code>/home/vllm/qwen3-4b-int8</code>而非<code>qwen3-4b-int8</code></p></blockquote><ul><li>2.需要运行的<strong>api_server</strong>是<code>vllm.entrypoints.openai.api_server</code>而不是<code>vllm.entrypoints.api_server</code>,因为第二个不兼容OpenAI API</li></ul><h3 id="运行命令范例" tabindex="-1">运行命令范例 <a class="header-anchor" href="#运行命令范例" aria-label="Permalink to &quot;运行命令范例&quot;">​</a></h3><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">python3</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -m</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> vllm.entrypoints.openai.api_server</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --model</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> /home/vllm/qwen3-4b-int8</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --gpu-memory-utilization</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 0.7</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --max-model-len</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 10240</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --max-num-seqs</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 4</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --max-num-batched-tokens</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 2048</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --dtype</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> auto</span></span></code></pre></div><blockquote><p>如果遇到了400报错大概率是因为message太大了被模型框架拒绝了</p></blockquote>`,27)]))}const u=s(t,[["render",n]]);export{c as __pageData,u as default};
