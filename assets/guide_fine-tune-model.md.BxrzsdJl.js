import{_ as d,c as e,o,ae as a}from"./chunks/framework.DUP9kEI5.js";const u=JSON.parse('{"title":"","description":"","frontmatter":{},"headers":[],"relativePath":"guide/fine-tune-model.md","filePath":"guide/fine-tune-model.md"}'),r={name:"guide/fine-tune-model.md"};function c(l,t,h,i,n,s){return o(),e("div",null,t[0]||(t[0]=[a('<h2 id="_3-微调模型" tabindex="-1">3. 微调模型 <a class="header-anchor" href="#_3-微调模型" aria-label="Permalink to &quot;3. 微调模型&quot;">​</a></h2><blockquote><p>Windows 上 Unsloth 兼容性不好，Linux 上代码有 bug，所以用 <code>no_unsloth</code> 版本。<br><s>其实是unsloth版本没写完</s></p></blockquote><blockquote><p>参数在测试时其实可以不填,都是有默认值的</p></blockquote><blockquote><p>似乎是默认8bit量化,有待修改</p></blockquote><ul><li><p>运行微调脚本：</p><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">python</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> run_finetune.py</span></span></code></pre></div></li></ul><h3 id="模型相关参数" tabindex="-1">模型相关参数 <a class="header-anchor" href="#模型相关参数" aria-label="Permalink to &quot;模型相关参数&quot;">​</a></h3><table tabindex="0"><thead><tr><th>参数名</th><th>类型</th><th>默认值</th><th>说明</th></tr></thead><tbody><tr><td><code>--repo_id</code></td><td>str</td><td><code>&quot;Qwen/Qwen3-8B-Base&quot;</code></td><td>基础模型或 MoE 模型的仓库ID</td></tr><tr><td><code>--local_dir</code></td><td>str</td><td><code>&quot;qwen3-8b-base&quot;</code></td><td>本地模型存储目录</td></tr><tr><td><code>--trust_remote_code</code></td><td>bool</td><td><code>True</code></td><td>是否信任远程代码</td></tr><tr><td><code>--use_unsloth</code></td><td>bool</td><td><code>False</code></td><td>是否使用 Unsloth 加速</td></tr><tr><td><code>--use_qlora</code></td><td>bool</td><td><code>True</code></td><td>是否使用 8bit 量化（QLoRA）</td></tr></tbody></table><hr><h3 id="数据相关参数" tabindex="-1">数据相关参数 <a class="header-anchor" href="#数据相关参数" aria-label="Permalink to &quot;数据相关参数&quot;">​</a></h3><table tabindex="0"><thead><tr><th>参数名</th><th>类型</th><th>默认值</th><th>说明</th></tr></thead><tbody><tr><td><code>--data_path</code></td><td>str</td><td><code>&quot;training_data.jsonl&quot;</code></td><td>训练数据文件路径</td></tr><tr><td><code>--eval_data_path</code></td><td>str / None</td><td><code>None</code></td><td>验证数据文件路径，None 表示不使用验证集</td></tr><tr><td><code>--max_samples</code></td><td>int / None</td><td><code>None</code></td><td>最大训练样本数，None 表示用全部</td></tr><tr><td><code>--max_eval_samples</code></td><td>int / None</td><td><code>None</code></td><td>最大验证样本数，None 表示用全部</td></tr><tr><td><code>--model_max_length</code></td><td>int</td><td><code>2048</code></td><td>最大序列长度</td></tr></tbody></table><hr><h3 id="训练相关参数" tabindex="-1">训练相关参数 <a class="header-anchor" href="#训练相关参数" aria-label="Permalink to &quot;训练相关参数&quot;">​</a></h3><table tabindex="0"><thead><tr><th>参数名</th><th>类型</th><th>默认值</th><th>说明</th></tr></thead><tbody><tr><td><code>--output_dir</code></td><td>str</td><td><code>&quot;finetune/models/qwen3-8b-qlora&quot;</code></td><td>输出目录</td></tr><tr><td><code>--seed</code></td><td>int</td><td><code>42</code></td><td>随机种子</td></tr></tbody></table><hr><h3 id="lora-参数" tabindex="-1">LoRA 参数 <a class="header-anchor" href="#lora-参数" aria-label="Permalink to &quot;LoRA 参数&quot;">​</a></h3><table tabindex="0"><thead><tr><th>参数名</th><th>类型</th><th>默认值</th><th>说明</th></tr></thead><tbody><tr><td><code>--lora_r</code></td><td>int</td><td><code>16</code></td><td>LoRA 秩</td></tr><tr><td><code>--lora_alpha</code></td><td>int</td><td><code>32</code></td><td>LoRA alpha</td></tr><tr><td><code>--lora_dropout</code></td><td>float</td><td><code>0.05</code></td><td>LoRA dropout</td></tr><tr><td><code>--target_modules</code></td><td>str</td><td><code>&quot;q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj&quot;</code></td><td>LoRA 目标模块列表（逗号分隔）</td></tr></tbody></table><hr><h3 id="moe-参数" tabindex="-1">MoE 参数 <a class="header-anchor" href="#moe-参数" aria-label="Permalink to &quot;MoE 参数&quot;">​</a></h3><table tabindex="0"><thead><tr><th>参数名</th><th>类型</th><th>默认值</th><th>说明</th><th></th></tr></thead><tbody><tr><td><code>--moe_enable</code></td><td>bool</td><td><code>False</code></td><td>是否启用 MoE 注入逻辑</td><td></td></tr><tr><td><code>--moe_lora_scope</code></td><td>str</td><td><code>&quot;expert_only&quot;</code></td><td>LoRA 注入范围：<code>expert_only</code>、<code>router_only</code>、<code>all</code></td><td></td></tr><tr><td><code>--moe_expert_patterns</code></td><td>str</td><td><code>experts.ffn.(gate_proj|up_proj|down_proj),layers.[0-9]+.mlp.experts.[0-9]+.(w1|w2|w3)</code></td><td>专家线性层匹配正则（兼容 Qwen-MoE、Mixtral）</td><td></td></tr><tr><td><code>--moe_router_patterns</code></td><td>str</td><td>`&quot;router.(gate</td><td>dense)&quot;`</td><td>路由/门控层匹配模式</td></tr><tr><td><code>--moe_max_experts_lora</code></td><td>int</td><td><code>-1</code></td><td>每层最多注入 LoRA 的专家数，<code>-1</code> 表示全部</td><td></td></tr><tr><td><code>--moe_dry_run</code></td><td>bool</td><td><code>False</code></td><td>仅打印匹配模块，不执行训练</td><td></td></tr></tbody></table><hr><h3 id="训练超参数" tabindex="-1">训练超参数 <a class="header-anchor" href="#训练超参数" aria-label="Permalink to &quot;训练超参数&quot;">​</a></h3><table tabindex="0"><thead><tr><th>参数名</th><th>类型</th><th>默认值</th><th>说明</th></tr></thead><tbody><tr><td><code>--per_device_train_batch_size</code></td><td>int</td><td><code>1</code></td><td>每卡训练 batch size</td></tr><tr><td><code>--per_device_eval_batch_size</code></td><td>int</td><td><code>1</code></td><td>每卡验证 batch size</td></tr><tr><td><code>--gradient_accumulation_steps</code></td><td>int</td><td><code>16</code></td><td>梯度累积步数</td></tr><tr><td><code>--learning_rate</code></td><td>float</td><td><code>2e-4</code></td><td>学习率</td></tr><tr><td><code>--weight_decay</code></td><td>float</td><td><code>0.0</code></td><td>权重衰减</td></tr><tr><td><code>--num_train_epochs</code></td><td>float</td><td><code>3.0</code></td><td>训练轮数</td></tr><tr><td><code>--max_steps</code></td><td>int</td><td><code>-1</code></td><td>最大步数，<code>-1</code> 表示不限制</td></tr><tr><td><code>--warmup_ratio</code></td><td>float</td><td><code>0.05</code></td><td>学习率预热比例</td></tr><tr><td><code>--lr_scheduler_type</code></td><td>str</td><td><code>&quot;cosine&quot;</code></td><td>学习率调度器类型</td></tr></tbody></table><hr><h3 id="其他参数" tabindex="-1">其他参数 <a class="header-anchor" href="#其他参数" aria-label="Permalink to &quot;其他参数&quot;">​</a></h3><table tabindex="0"><thead><tr><th>参数名</th><th>类型</th><th>默认值</th><th>说明</th></tr></thead><tbody><tr><td><code>--logging_steps</code></td><td>int</td><td><code>1</code></td><td>日志输出间隔（步）</td></tr><tr><td><code>--eval_steps</code></td><td>int</td><td><code>50</code></td><td>验证间隔步数</td></tr><tr><td><code>--save_steps</code></td><td>int</td><td><code>200</code></td><td>模型保存间隔</td></tr><tr><td><code>--save_total_limit</code></td><td>int</td><td><code>2</code></td><td>最多保存多少个检查点</td></tr><tr><td><code>--gradient_checkpointing</code></td><td>bool</td><td><code>True</code></td><td>是否使用梯度检查点</td></tr><tr><td><code>--merge_and_save</code></td><td>bool</td><td><code>True</code></td><td>是否合并 LoRA 并保存完整模型</td></tr><tr><td><code>--fp16</code></td><td>bool</td><td><code>True</code></td><td>是否使用 FP16</td></tr><tr><td><code>--optim</code></td><td>str</td><td><code>&quot;adamw_torch_fused&quot;</code></td><td>优化器</td></tr><tr><td><code>--dataloader_pin_memory</code></td><td>bool</td><td><code>False</code></td><td>DataLoader 是否使用 pin_memory</td></tr><tr><td><code>--dataloader_num_workers</code></td><td>int</td><td><code>0</code></td><td>DataLoader 工作线程数</td></tr></tbody></table><hr><h3 id="验证集未生效" tabindex="-1">验证集未生效 <a class="header-anchor" href="#验证集未生效" aria-label="Permalink to &quot;验证集未生效&quot;">​</a></h3><ul><li>检查<code>--eval_data_path</code>路径是否正确</li><li>确认验证数据文件格式与训练数据一致</li><li>查看控制台输出是否有&quot;未提供验证数据路径&quot;的提示</li></ul><h3 id="gpu显存不足" tabindex="-1">GPU显存不足 <a class="header-anchor" href="#gpu显存不足" aria-label="Permalink to &quot;GPU显存不足&quot;">​</a></h3><ul><li>减小<code>--per_device_eval_batch_size</code></li><li>减小<code>--max_eval_samples</code></li><li>增加<code>--eval_steps</code>间隔</li></ul>',30)]))}const b=d(r,[["render",c]]);export{u as __pageData,b as default};
