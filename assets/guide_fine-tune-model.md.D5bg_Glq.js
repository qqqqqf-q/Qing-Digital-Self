import{_ as d,c as s,o as e,ae as a}from"./chunks/framework.DUP9kEI5.js";const k=JSON.parse('{"title":"配置环境","description":"","frontmatter":{},"headers":[],"relativePath":"guide/fine-tune-model.md","filePath":"guide/fine-tune-model.md"}'),i={name:"guide/fine-tune-model.md"};function o(h,t,l,n,r,p){return e(),s("div",null,t[0]||(t[0]=[a(`<h2 id="微调模型" tabindex="-1">微调模型 <a class="header-anchor" href="#微调模型" aria-label="Permalink to &quot;微调模型&quot;">​</a></h2><blockquote><p>建议使用高单核的CPU进行微调<br> 不然可能存在CPU瓶颈(暂时没找到问题所在,欢迎PR修复)</p></blockquote><h2 id="在此之前-你需要配置环境" tabindex="-1">在此之前,你需要配置环境 <a class="header-anchor" href="#在此之前-你需要配置环境" aria-label="Permalink to &quot;在此之前,你需要配置环境&quot;">​</a></h2><blockquote><p>很简单,不必担心</p></blockquote><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">git</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> clone</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> https://github.com/qqqqqf-q/Qing-Digital-Self.git</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --depth</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 1</span></span></code></pre></div><p>或使用镜像(中国大陆加速)</p><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">git</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> clone</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> https://hk.gh-proxy.com/https://github.com/qqqqqf-q/Qing-Digital-Self.git</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">  --depth</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 1</span></span></code></pre></div><h1 id="配置环境" tabindex="-1">配置环境 <a class="header-anchor" href="#配置环境" aria-label="Permalink to &quot;配置环境&quot;">​</a></h1><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">python3</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> environment/setup_env.py</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --install</span></span></code></pre></div><p>默认跟着流程走就好 安装完自带检查 也可以使用</p><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">python3</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> environment/setup_env.py</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --check</span></span></code></pre></div><p>来进行检查环境 如果遇到unsloth无法安装请自行安装 先运行以下命令</p><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">wget</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -qO-</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> https://raw.githubusercontent.com/unslothai/unsloth/main/unsloth/_auto_install.py</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> |</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> python</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> -</span></span></code></pre></div><p>它会输出一个pip命令,请复制下来并在shell里运行 例如</p><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">pip</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> install</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --upgrade</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> pip</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> &amp;&amp; </span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">pip</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> install</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;unsloth[cu126-ampere-torch270] @ git+https://github.com/unslothai/unsloth.git&quot;</span></span></code></pre></div><p>如果遇到flah attn安装问题 可以尝试前往<a href="https://github.com/Dao-AILab/flash-attention/releases/" target="_blank" rel="noreferrer">此Github仓库</a> 来查看你需要的离线安装包(这个不需要编译,会快非常多) 命令类似:</p><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">wget</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> https://github.com/Dao-AILab/flash-attention/releases/download/v2.8.3/flash_attn-2.8.3+cu12torch2.4cxx11abiTRUE-cp312-cp312-linux_x86_64.whl&#39;</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">pip install flash_attn-2.8.3+cu12torch2.4cxx11abiTRUE-cp312-cp312-linux_x86_64.whl</span></span></code></pre></div><h1 id="以下才是真正的微调" tabindex="-1">以下才是真正的微调 <a class="header-anchor" href="#以下才是真正的微调" aria-label="Permalink to &quot;以下才是真正的微调&quot;">​</a></h1><blockquote><p>参数在测试时其实可以不填,都是有默认值的</p></blockquote><blockquote><p>似乎是默认8bit量化,有待修改</p></blockquote><ul><li><p>运行微调脚本：</p><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">python</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> run_finetune.py</span></span></code></pre></div></li></ul><h3 id="模型相关参数-列表有四列-请滚动查看" tabindex="-1">模型相关参数(列表有四列,请滚动查看) <a class="header-anchor" href="#模型相关参数-列表有四列-请滚动查看" aria-label="Permalink to &quot;模型相关参数(列表有四列,请滚动查看)&quot;">​</a></h3><table tabindex="0"><thead><tr><th>参数名</th><th>类型</th><th>默认值</th><th>说明</th></tr></thead><tbody><tr><td><code>--repo_id</code></td><td>str</td><td><code>&#39;Qwen/Qwen3-30B-A3B-Instruct-2507&#39;</code></td><td>HF 仓库ID</td></tr><tr><td><code>--local_dir</code></td><td>str</td><td><code>&#39;qwen3-30b-a3b-instruct&#39;</code></td><td>本地模型目录</td></tr><tr><td><code>--use_unsloth</code></td><td>str</td><td><code>&#39;false&#39;</code></td><td>是否使用 unsloth</td></tr><tr><td><code>--use_qlora</code></td><td>str</td><td><code>&#39;true&#39;</code></td><td>是否使用 qlora</td></tr><tr><td><code>--data_path</code></td><td>str</td><td><code>&#39;training_data.jsonl&#39;</code></td><td>训练数据路径</td></tr><tr><td><code>--eval_data_path</code></td><td>str</td><td><code>None</code></td><td>验证数据文件路径</td></tr><tr><td><code>--max_samples</code></td><td>str</td><td><code>None</code></td><td>最大训练样本数</td></tr><tr><td><code>--max_eval_samples</code></td><td>str</td><td><code>None</code></td><td>最大验证样本数</td></tr><tr><td><code>--model_max_length</code></td><td>str</td><td><code>&#39;2048&#39;</code></td><td>最大序列长度</td></tr><tr><td><code>--output_dir</code></td><td>str</td><td><code>&#39;finetune/models/qwen3-30b-a3b-qlora&#39;</code></td><td>输出目录</td></tr><tr><td><code>--seed</code></td><td>str</td><td><code>&#39;42&#39;</code></td><td>随机种子</td></tr><tr><td><code>--per_device_train_batch_size</code></td><td>str</td><td><code>&#39;1&#39;</code></td><td>每设备训练批次大小</td></tr><tr><td><code>--per_device_eval_batch_size</code></td><td>str</td><td><code>&#39;1&#39;</code></td><td>每设备验证批次大小</td></tr><tr><td><code>--gradient_accumulation_steps</code></td><td>str</td><td><code>&#39;16&#39;</code></td><td>梯度累积步数</td></tr><tr><td><code>--learning_rate</code></td><td>str</td><td><code>&#39;2e-4&#39;</code></td><td>学习率</td></tr><tr><td><code>--num_train_epochs</code></td><td>str</td><td><code>&#39;3&#39;</code></td><td>训练轮数</td></tr><tr><td><code>--max_steps</code></td><td>str</td><td><code>&#39;-1&#39;</code></td><td>最大步数，-1表示不限制</td></tr><tr><td><code>--lora_r</code></td><td>str</td><td><code>&#39;16&#39;</code></td><td>LoRA 秩</td></tr><tr><td><code>--lora_alpha</code></td><td>str</td><td><code>&#39;32&#39;</code></td><td>LoRA alpha 值</td></tr><tr><td><code>--lora_dropout</code></td><td>str</td><td><code>&#39;0.05&#39;</code></td><td>LoRA dropout 率</td></tr><tr><td><code>--target_modules</code></td><td>str</td><td><code>&#39;太长了请进文件查看&#39;</code></td><td>LoRA 目标模块</td></tr><tr><td><code>--weight_decay</code></td><td>str</td><td><code>&#39;0.0&#39;</code></td><td>权重衰减</td></tr><tr><td><code>--moe_enable</code></td><td>str</td><td><code>&#39;false&#39;</code></td><td>是否启用 MoE 注入逻辑</td></tr><tr><td><code>--moe_lora_scope</code></td><td>str</td><td><code>&#39;expert_only&#39;</code></td><td>LoRA 注入范围</td></tr><tr><td><code>--moe_expert_patterns</code></td><td>str</td><td><code>&#39;太长了写不下,去文件里看&#39;</code></td><td>专家线性层模式</td></tr><tr><td><code>--moe_router_patterns</code></td><td>str</td><td><code>&#39;markdown会转译,也去文件里看&#39;</code></td><td>路由/门控线性层模式</td></tr><tr><td><code>--moe_max_experts_lora</code></td><td>str</td><td><code>&#39;-1&#39;</code></td><td>每层注入 LoRA 的专家数上限</td></tr><tr><td><code>--moe_dry_run</code></td><td>str</td><td><code>&#39;false&#39;</code></td><td>是否为 Dry-Run</td></tr><tr><td><code>--load_precision</code></td><td>str</td><td><code>&#39;fp16&#39;</code></td><td>模型加载精度：<code>int8</code> / <code>int4</code> / <code>fp16</code></td></tr><tr><td><code>--logging_steps</code></td><td>str</td><td><code>&#39;1&#39;</code></td><td>日志记录步数</td></tr><tr><td><code>--eval_steps</code></td><td>str</td><td><code>&#39;50&#39;</code></td><td>验证间隔步数</td></tr><tr><td><code>--save_steps</code></td><td>str</td><td><code>&#39;200&#39;</code></td><td>保存模型步数</td></tr><tr><td><code>--save_total_limit</code></td><td>str</td><td><code>&#39;2&#39;</code></td><td>最多保存模型数量</td></tr><tr><td><code>--warmup_ratio</code></td><td>str</td><td><code>&#39;0.05&#39;</code></td><td>学习率预热比例</td></tr><tr><td><code>--lr_scheduler_type</code></td><td>str</td><td><code>&#39;cosine&#39;</code></td><td>学习率调度器类型</td></tr><tr><td><code>--resume_from_checkpoint</code></td><td>str</td><td><code>None</code></td><td>恢复训练的检查点路径</td></tr><tr><td><code>--no-gradient_checkpointing</code></td><td>flag</td><td><code>False</code></td><td>不使用梯度检查点（使用时加此参数）</td></tr><tr><td><code>--no-merge_and_save</code></td><td>flag</td><td><code>False</code></td><td>不合并并保存模型（使用时加此参数）</td></tr><tr><td><code>--fp16</code></td><td>str</td><td><code>&#39;true&#39;</code></td><td>是否使用 fp16</td></tr><tr><td><code>--optim</code></td><td>str</td><td><code>&#39;adamw_torch_fused&#39;</code></td><td>优化器名称</td></tr><tr><td><code>--dataloader_pin_memory</code></td><td>str</td><td><code>&#39;false&#39;</code></td><td>是否固定 DataLoader 内存</td></tr><tr><td><code>--dataloader_num_workers</code></td><td>str</td><td><code>&#39;0&#39;</code></td><td>DataLoader 工作线程数</td></tr><tr><td><code>--dataloader_prefetch_factor</code></td><td>str</td><td><code>&#39;2&#39;</code></td><td>DataLoader 预取因子</td></tr><tr><td><code>--use_flash_attention_2</code></td><td>str</td><td><code>&#39;false&#39;</code></td><td>是否使用 FlashAttention2(对unsloth无效) （使用时加此参数）</td></tr></tbody></table><hr><blockquote><p>参数还是太复杂了,建议询问AI<br> 下面是一个4090微调<code>qwen2.5-7b-instruct</code>的范例</p></blockquote><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">python3</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> run_finetune.py</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --output_dir</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> /root/autodl-fs/qwen2.5-7b-qing-v1</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --local_dir</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> ./model/Qwen2.5-7B-Instruct</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --data_path</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> ./dataset/sft.jsonl</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --use_qlora</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> true</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --lora_dropout</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 0.1</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --num_train_epochs</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 8</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --per_device_train_batch_size</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 4</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --per_device_eval_batch_size</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 4</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --gradient_accumulation_steps</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 8</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --learning_rate</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> 2e-5</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --lr_scheduler</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> cosine</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --logging_steps</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 5</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --eval_steps</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 40</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --save_steps</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 200</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --warmup_ratio</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 0.05</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --dataloader_num_workers</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 16</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --fp16</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> true</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --use_unsloth</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> true</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --no-gradient_checkpointing</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">  --load_precision</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> int8</span></span></code></pre></div><h3 id="验证集未生效" tabindex="-1">验证集未生效 <a class="header-anchor" href="#验证集未生效" aria-label="Permalink to &quot;验证集未生效&quot;">​</a></h3><ul><li>检查<code>--eval_data_path</code>路径是否正确</li><li>确认验证数据文件格式与训练数据一致</li><li>查看控制台输出是否有&quot;未提供验证数据路径&quot;的提示</li></ul><h3 id="gpu显存不足" tabindex="-1">GPU显存不足 <a class="header-anchor" href="#gpu显存不足" aria-label="Permalink to &quot;GPU显存不足&quot;">​</a></h3><ul><li>减小<code>--per_device_eval_batch_size</code></li><li>减小<code>--max_eval_samples</code></li><li>增加<code>--eval_steps</code>间隔</li></ul><h3 id="dev注" tabindex="-1">Dev注 <a class="header-anchor" href="#dev注" aria-label="Permalink to &quot;Dev注&quot;">​</a></h3><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">python3</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> cli.py</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> train</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> start</span></span></code></pre></div><p>此参数似乎还是不太能使用的样子,有很多Bug,有待修改</p>`,33)]))}const g=d(i,[["render",o]]);export{k as __pageData,g as default};
