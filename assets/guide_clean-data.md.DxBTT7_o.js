import{_ as i,c as a,o as t,ae as l}from"./chunks/framework.DUP9kEI5.js";const F=JSON.parse('{"title":"以下为不使用云端llm服务清洗使用","description":"","frontmatter":{},"headers":[],"relativePath":"guide/clean-data.md","filePath":"guide/clean-data.md"}'),n={name:"guide/clean-data.md"};function e(h,s,p,k,d,o){return t(),a("div",null,s[0]||(s[0]=[l(`<h2 id="将数据转化为csv" tabindex="-1">将数据转化为csv <a class="header-anchor" href="#将数据转化为csv" aria-label="Permalink to &quot;将数据转化为csv&quot;">​</a></h2><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">python</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> cli.py</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> data</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> extract</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 或自定义parser字段</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">python</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> cli.py</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> data</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> extract</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --qq-db-path</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> ./data/qq.db</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --qq-number-ai</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> 1234567890--output</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> ./dataset/csv</span></span></code></pre></div><table tabindex="0"><thead><tr><th>参数</th><th>说明</th><th>默认值/备注</th></tr></thead><tbody><tr><td><code>-h, --help</code></td><td>显示帮助信息并退出</td><td>-</td></tr><tr><td><code>--source-type {qq,tg,telegram}</code></td><td>指定数据源类型</td><td>不指定则自动检测</td></tr><tr><td><code>--data-dir DATA_DIR</code></td><td>数据目录路径</td><td><code>./dataset/original/</code></td></tr><tr><td><code>--output OUTPUT</code></td><td>输出目录路径</td><td><code>./dataset/csv/</code></td></tr><tr><td><code>--qq-db-path QQ_DB_PATH</code></td><td>QQ 数据库文件路径</td><td>-</td></tr><tr><td><code>--qq-number-ai QQ_NUMBER_AI</code></td><td>AI 的 QQ 号码（用于区分发送者）</td><td>-</td></tr><tr><td><code>--telegram-chat-id TELEGRAM_CHAT_ID</code></td><td>AI 的 Telegram 聊天名称（用于区分发送者）</td><td>-</td></tr><tr><td><code>--tg-data-dir TG_DATA_DIR</code></td><td>Telegram 数据目录</td><td>如不指定则使用 <code>--data-dir</code></td></tr></tbody></table><h2 id="清洗数据-普通版本-llm清洗版本在下一章节" tabindex="-1">清洗数据(普通版本,llm清洗版本在下一章节) <a class="header-anchor" href="#清洗数据-普通版本-llm清洗版本在下一章节" aria-label="Permalink to &quot;清洗数据(普通版本,llm清洗版本在下一章节)&quot;">​</a></h2><blockquote><p>此方法比llm清洗快得多,30w条消息几秒就好了 但是对应的质量也更低 这个部分建议在windows上优化完再上传至GPU服务器</p></blockquote><ul><li>在 <code>setting.jsonc</code> 文件中修改数据库路径及相关参数(请注意其中的必填段)</li><li><code>data_agrs</code>中的一些字段以及下面的<code>system prompt</code></li><li>运行清洗脚本：</li></ul><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">python</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> cli.py</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> data</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> clean</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> raw</span></span></code></pre></div><hr><h2 id="清洗数据-llm清洗" tabindex="-1">清洗数据(llm清洗) <a class="header-anchor" href="#清洗数据-llm清洗" aria-label="Permalink to &quot;清洗数据(llm清洗)&quot;">​</a></h2><blockquote><p>Develop版本正在改进此功能</p></blockquote><blockquote><p>需要配置一个OpenAI兼容的API<br> 比如:LM Studio 或者 vLLM(速度更快,但搭建更麻烦,需要Linux环境)</p></blockquote><blockquote><p>这个部分同样建议在windows上优化完再上传至GPU服务器<br> 不确定在Linux上有没有兼容性问题</p></blockquote><ul><li>前往<code>setting.jsonc</code>文件中修改<code>clean_set_args</code>组的<code>openai_api</code>字段</li><li>设置<code>api_base</code> <code>api_key</code> <code>model_name</code>等字段</li></ul><h3 id="run" tabindex="-1">run! <a class="header-anchor" href="#run" aria-label="Permalink to &quot;run!&quot;">​</a></h3><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">python</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> cli.py</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> data</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> clean</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> llm</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 打分策略</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">python</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> cli.py</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> data</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> clean</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> llm</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --parser</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> scoring</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 设置分数阈值</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">python</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> cli.py</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> data</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> clean</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> llm</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --parser</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> scoring</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --accept-score</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 4</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 使用句段策略(没写完)</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">python</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> cli.py</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> data</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> clean</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> llm</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --parser</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> segment</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 断点继续</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">python</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> cli.py</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> data</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> clean</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> llm</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --resume</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 其他参数</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">--input</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> -</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> 输入CSV目录路径（默认从配置读取）</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">--output</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> -</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> 输出文件路径（默认从配置读取）</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">--batch-size</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> -</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> 批处理大小（默认从配置读取）</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">--workers</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> -</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> 工作进程数（默认从配置读取）</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">--resume</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> -</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> 从上次中断处继续</span></span></code></pre></div><blockquote><p>可以调大<code>betch_size</code>来减少api调用次数以减少tpm/rpm限制<br> 不过会增大token</p></blockquote><blockquote><p>如果遇到了400报错大概率是因为message太大了被模型框架拒绝了</p></blockquote><h1 id="以下为不使用云端llm服务清洗使用" tabindex="-1">以下为不使用云端llm服务清洗使用 <a class="header-anchor" href="#以下为不使用云端llm服务清洗使用" aria-label="Permalink to &quot;以下为不使用云端llm服务清洗使用&quot;">​</a></h1><h2 id="lm-studio搭建教程" tabindex="-1">LM Studio搭建教程 <a class="header-anchor" href="#lm-studio搭建教程" aria-label="Permalink to &quot;LM Studio搭建教程&quot;">​</a></h2><ul><li>1.前往<a href="https://lmstudio.ai/" target="_blank" rel="noreferrer">LM Studio</a>下载LM Studio</li><li>2.安装LM Studio</li><li>3.打开LM Studio,点击左侧<code>搜索</code>-&gt;<code>Model Search</code></li><li>4.搜索 <code>qwen2.5-7b-instruct</code>-&gt;<code>Complete Download</code></li><li>5.选择合适你的量化版本<strong>建议至少Q4,最好Q6-Q8,随你的设备情况而定,不知道的可以问AI</strong></li><li>记住你的<strong>模型名称</strong>,填写到<code>setting.jsonc</code>文件的<code>model_name</code>中</li><li>如果不知道你的模型名称可以运行test_openai.py,会输出所有的模型名称</li><li>6.安装好后,在左侧<code>开发者/Developer</code>点击<code>Status:Stopped</code>右边的按钮</li><li>如果下面log显示端口被占用请点击<code>seetings</code>换个<code>server port</code></li><li>记住这个<code>server port</code>,将你的配置填写至<code>setting.jsonc</code>文件中</li></ul><hr><h2 id="vllm搭建" tabindex="-1">vLLM搭建 <a class="header-anchor" href="#vllm搭建" aria-label="Permalink to &quot;vLLM搭建&quot;">​</a></h2><blockquote><p>vLLM需要linux环境!<br> 如果你的显卡还算可以(&gt;6800xt,&gt;3080)<br> 可以选择使用lmstudio,多等一会就好了,还可以玩玩模型 缺点是lmstudio不能运行hf模型,且并发很烂</p></blockquote><blockquote><p>vLLM比Lm studio吃显存的多! Lm studio可以运行8b_q6到vLLM上只能运行4b_Q6</p></blockquote><blockquote><p>不过并发效率的提升是真的</p></blockquote><blockquote><p>但是!上下文较短<br> 不过现在应该遇不到那么长的上下文了</p></blockquote><blockquote><p>3080实测4b_q6处理,最终jsonl的速率大约是<strong>300kb/minute</strong></p></blockquote><ul><li>跟着走就能搭建</li></ul><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">sudo</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> apt</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> update</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">sudo</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> apt</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> install</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> python3.10-venv</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> git</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -y</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">python3</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -m</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> venv</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> vllm_env</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">source</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> vllm_env/bin/activate</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">pip</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> install</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -U</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> pip</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">pip</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> install</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> torch</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --index-url</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> https://download.pytorch.org/whl/cu121</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">  # 如果你用CUDA</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">pip</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> install</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> vllm</span></span></code></pre></div><h3 id="和lm-studio不同的注意点" tabindex="-1">和lm studio不同的注意点 <a class="header-anchor" href="#和lm-studio不同的注意点" aria-label="Permalink to &quot;和lm studio不同的注意点&quot;">​</a></h3><ul><li>1.<code>setting.jsonc</code>中的<code>model_name</code>需要设置路径而不只是文件夹名</li></ul><blockquote><p>是<code>/home/vllm/qwen3-4b-int8</code>而非<code>qwen3-4b-int8</code></p></blockquote><ul><li>2.需要运行的<strong>api_server</strong>是<code>vllm.entrypoints.openai.api_server</code>而不是<code>vllm.entrypoints.api_server</code>,因为第二个不兼容OpenAI API</li></ul><h3 id="运行命令范例" tabindex="-1">运行命令范例 <a class="header-anchor" href="#运行命令范例" aria-label="Permalink to &quot;运行命令范例&quot;">​</a></h3><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">python3</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -m</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> vllm.entrypoints.openai.api_server</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --model</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> /home/vllm/qwen3-4b-int8</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --gpu-memory-utilization</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 0.7</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --max-model-len</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 10240</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --max-num-seqs</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 4</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --max-num-batched-tokens</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 2048</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --dtype</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> auto</span></span></code></pre></div><blockquote><p>如果遇到了400报错大概率是因为message太大了被模型框架拒绝了</p></blockquote>`,36)]))}const c=i(n,[["render",e]]);export{F as __pageData,c as default};
