#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import sys
import json
import os
import signal
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Optional, List, Tuple, Dict
from database.db_connector import DatabaseConnector
import datetime
from collections import defaultdict
from config.config import get_config
from logger.logger import get_logger
import threading
import math

# å¯¼å…¥LM Studioæ”¯æŒ
from openai.openai_client import LLMDataCleaner

# åˆ›å»ºå…¨å±€æ¸…æ´—å™¨å®ä¾‹
llm_cleaner = LLMDataCleaner()
# è·å–é…ç½®å®ä¾‹
config = get_config()
logger = get_logger('Generate_training_data')

# å…¨å±€ä¸­æ–­æ ‡å¿—
interrupt_flag = threading.Event()

def signal_handler(signum, frame):
    """å¤„ç†Ctrl+Cä¸­æ–­ä¿¡å·"""
    logger.info(
        zhcn="æ¥æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨ä¼˜é›…é€€å‡º...",
        en="Received interrupt signal, gracefully exiting..."
    )
    interrupt_flag.set()

# æ³¨å†Œä¿¡å·å¤„ç†å™¨
signal.signal(signal.SIGINT, signal_handler)

# ---------- è§£æç›¸å…³ï¼šå¾®ä¼˜åŒ– ----------

def decode_varint(data: bytes, pos: int = 0) -> tuple[int, int]:
    """ä»æŒ‡å®šä½ç½®è§£ç ä¸€ä¸ª Varint"""
    result = 0
    shift = 0
    original_pos = pos
    while True:
        if pos >= len(data):
            raise IndexError(f"Varint decoding failed: buffer ended unexpectedly at position {original_pos}.")
        b = data[pos]
        result |= (b & 0x7F) << shift
        pos += 1
        if not (b & 0x80):
            break
        shift += 7
        if shift >= 64:
            raise ValueError("Varint is too long or invalid.")
    return result, pos

def parse_protobuf_fields(data: bytes) -> list[tuple[int, int, bytes]]:
    """è§£æprotobufå­—æ®µ"""
    pos = 0
    fields = []
    ln = len(data)
    while pos < ln:
        try:
            tag, pos = decode_varint(data, pos)
            field_number = tag >> 3
            wire_type = tag & 0x7

            if wire_type == 0:  # Varint
                value, pos = decode_varint(data, pos)
                # é¿å…é¢‘ç¹ to_bytes è½¬æ¢ï¼Œä¿ç•™ä¸ºæœ€å°è¡¨ç¤º
                v = value.to_bytes((value.bit_length() + 7) // 8, 'little') if value else b'\x00'
                fields.append((field_number, wire_type, v))
            elif wire_type == 1:  # 64-bit
                if pos + 8 > ln:
                    raise IndexError(f"Field {field_number}: Not enough data for 64-bit value.")
                value = data[pos:pos+8]
                pos += 8
                fields.append((field_number, wire_type, value))
            elif wire_type == 2:  # Length-delimited
                length, pos = decode_varint(data, pos)
                end = pos + length
                if end > ln:
                    raise IndexError(f"Field {field_number}: Declared length {length} exceeds buffer size.")
                value = data[pos:end]
                pos = end
                fields.append((field_number, wire_type, value))
            elif wire_type == 5:  # 32-bit
                if pos + 4 > ln:
                    raise IndexError(f"Field {field_number}: Not enough data for 32-bit value.")
                value = data[pos:pos+4]
                pos += 4
                fields.append((field_number, wire_type, value))
            else:
                # è·³è¿‡æœªçŸ¥ç±»å‹
                break
        except (ValueError, IndexError):
            break
    return fields


def find_strings_recursively(fields: list[tuple[int, int, bytes]]) -> list[str]:
    """é€’å½’æŸ¥æ‰¾UTF-8å­—ç¬¦ä¸²ï¼Œç›´æ¥è¿”å›å­—ç¬¦ä¸²åˆ—è¡¨ï¼Œå‡å°‘å…ƒç»„ä¸æ‹·è´"""
    out: List[str] = []
    for _, wire_type, value in fields:
        if wire_type == 2:
            try:
                text = value.decode('utf-8')
                # å…ˆå¿«é€Ÿæ ¡éªŒ
                if text and any(c.isprintable() for c in text):
                    out.append(text)
            except UnicodeDecodeError:
                nested_fields = parse_protobuf_fields(value)
                if nested_fields:
                    out.extend(find_strings_recursively(nested_fields))
    return out


def calculate_entropy(text: str) -> float:
    """è®¡ç®—å­—ç¬¦ä¸²çš„ç†µå€¼ï¼Œç”¨äºæ£€æµ‹æ— åºå­—ç¬¦ä¸²"""
    if not text:
        return 0.0
    
    # ç»Ÿè®¡æ¯ä¸ªå­—ç¬¦çš„å‡ºç°é¢‘ç‡
    char_counts = {}
    for char in text:
        char_counts[char] = char_counts.get(char, 0) + 1
    
    # è®¡ç®—ç†µå€¼
    entropy = 0.0
    text_len = len(text)
    for count in char_counts.values():
        probability = count / text_len
        entropy -= probability * math.log2(probability)
    
    return entropy

# é¢„å¤„ç†å›¾ç‰‡/éæ–‡æœ¬æŒ‡ç¤ºå™¨ï¼Œé™å°å†™å¹¶ç”¨é›†åˆåŠ é€ŸæŸ¥è¯¢
_IMAGE_INDICATORS = {
    '.jpg', '.jpeg', '.png', '.gif', '.bmp', '.webp',
    '.mp4', '.amr', '.mp3', '.avi', '.mov', '.wmv',  # æ‰©å±•åª’ä½“æ‰©å±•
    'offpic_new', 'thumb', 'ori', 'pic', 'bigpic', 'smallpic',
    '[åŠ¨ç”»è¡¨æƒ…]', '[è¡¨æƒ…]', '[å›¾ç‰‡]', '[é—ªç…§]', '[è¡¨æƒ…]', '[åŠ¨ç”»]', '[è´´å›¾]',
    '[è¯­éŸ³]', '[è§†é¢‘]', '[æ–‡ä»¶]', '[çº¢åŒ…]', '[è½¬è´¦]', '[ä½ç½®]', '[åç‰‡]',  # å¸¸è§IMå ä½
    '[ç³»ç»Ÿæ¶ˆæ¯]', '[QQæ¶ˆæ¯]', '[ç¾¤æ¶ˆæ¯]', '[å¥½å‹æ¶ˆæ¯]',
    'ntosfull', 'documents\\tencent files', 'tencent files',
    'term=', 'is_origin=', '_198.jpg', '_720.jpg', '_198_', '_720_',
    '/1684773595-', '/3259379048-', '/thumb/', '/ori/', '/pic/',
    '-05d65b12ef0481a7337c63775bac3ffd', 'b0ddba64f40a8d68eec61b26eabf7750',
    'u_sp2m7izf1zlnjvtcwqx0sg', 'u_ovchmti-zvlwu_0zm_ulgw',
    'multimedia.nt.qq.com.cn', 'qq.com', 'tencent.com', 'gtimg.cn',
    'mqqapi://', 'https://', 'http://', 'ftp://', 'file://',
    'undefined', 'undefine', 'null', 'none',
    'è¯·ä½¿ç”¨æ–°ç‰ˆæ‰‹æœºqqæŸ¥çœ‹', 'è¿ç»­äº’å‘æ¶ˆæ¯ä¸­æ–­', 'æ‹çˆ±ä¹‹é’¥',
    'ç‚¹å‡»æ¢å¤', 'er0s', 'show_pslcard', 'pslcard',
    'interactive_new', 'club.vip.qq.com', 'vip.qq.com',
    'downv6.qq.com', 'qzone_love', 'ti.qq.com',
    # QQäº’åŠ¨æ ‡ç­¾å’Œç³»ç»Ÿæ¶ˆæ¯
    'gtip align=', 'qq uin=', 'uin=', 'col=', 'nm=', 'jp=', 'nor txt=',
    'è¸¢äº†è¸¢', 'æ‘¸æ‘¸å¤´', 'å®Œæˆäº†', 'è·å¾—', 'æŸ¥çœ‹è¯¦æƒ…', 'äº’åŠ¨æ ‡è¯†',
    # JSONç»“æ„å’ŒURLå‚æ•°
    '{"', '"}', '"align"', '"items"', '"type"', '"txt"', '"col"', '"url"', '"img"',
    'local_jp', 'param', 'url=', 'rkey=', 'target_uin=', 'mutualmark_id=',
    # ç³»ç»Ÿæç¤ºå’ŒçŠ¶æ€æ¶ˆæ¯
    'ä½ ä»¬äº’å‘æ¶ˆæ¯ï¼Œå°æ°´èŠ±å¼€å¿ƒæäº†ï¼Œæ­å–œé‡æ–°ç‚¹äº®åˆæ³›æ¶Ÿæ¼ª',
    'è¯­éŸ³é€šè¯', 'è§†é¢‘é€šè¯', 'é€šè¯æ—¶é•¿', 'é€šè¯ç»“æŸ',
    # æƒ…ä¾£å’Œäº’åŠ¨æ ‡è¯†
    'æƒ…ä¾£ç©ºé—´', 'æƒ…ä¾£', 'ç¥ä»™çœ·ä¾£', 'çŸ¥å·±æµªèŠ±', 'ç´¯è®¡äº’å‘æ¶ˆæ¯è¶…è¿‡', 'çˆ±æ„æ°¸æ’', 'æ­å–œå‡çº§', 'äº’å‘æ¶ˆæ¯', 'ç•…èŠä¹‹ç«',
    'å‹è°Šçš„å°èˆ¹', 'å‹è°Šçš„å·¨è½®', 'èŠå¤©æ°”æ³¡', 'äº’åŠ¨æ ‡è¯†', 'å¥½å‹äº’åŠ¨',
    # å¯ç–‘çŸ­ä¸²å’Œç‰¹æ®Šæ ‡è¯†
    'd42ae9486fdbc4b7', '305102010004363034020100020445d1',
    # æ·»åŠ å¯¹ç±»ä¼¼ u_2_Tv579rOiOIDBW9sUrPBA è¿™ç§æ ¼å¼çš„è¿‡æ»¤
    'u_',
    # å…¶ä»–éœ€è¦è¿‡æ»¤çš„å†…å®¹
    'qqweb', 'res', 'mutualmark', 'nudgeaction', 'expression.jpg',
    'nudgemall', 'actionid=', 'interactive', 'new/index',
    # æ–‡ä»¶è·¯å¾„ç›¸å…³
    'c:\\users\\', 'c:\\', '\\nt_qq\\', '\\nt_data\\', '\\pic\\', '\\thumb\\',
    'æˆ‘ç°åœ¨æœ‰äº‹ä¸åœ¨'
}

def is_image_content(text: str) -> bool:
    """æ£€æµ‹æ˜¯å¦ä¸ºå›¾ç‰‡/éæ–‡æœ¬å†…å®¹ï¼Œç”¨äºLLMæ¸…æ´—å‰çš„æ ‡è®°"""
    if not text:
        return True  # ç©ºå†…å®¹è§†ä¸ºéæ–‡æœ¬
    
    text_lower = text.lower().strip()
    
    # æ£€æŸ¥æ˜¯å¦åœ¨æŒ‡ç¤ºå™¨é›†åˆä¸­
    if any(indicator in text_lower for indicator in _IMAGE_INDICATORS):
        return True
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºçº¯JSONç»“æ„ï¼ˆåŒ…å«å¤§æ‹¬å·ã€å¼•å·ã€å†’å·ç­‰ï¼‰
    if text.startswith('{') and text.endswith('}'):
        return True
    if '"type"' in text_lower and '"txt"' in text_lower:
        return True
    if '"align"' in text_lower and '"items"' in text_lower:
        return True
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºHTML/XMLæ ‡ç­¾æ ¼å¼
    if '<gtip' in text_lower or '</gtip>' in text_lower:
        return True
    if '<qq ' in text_lower or '<img ' in text_lower:
        return True
    if 'align="center"' in text_lower:
        return True
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºçº¯URLæˆ–æ–‡ä»¶è·¯å¾„
    if text.startswith(('http://', 'https://', 'ftp://', 'file://')):
        return True
    if 'c:\\' in text_lower and ('.jpg' in text_lower or '.png' in text_lower):
        return True
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºç³»ç»Ÿæ¶ˆæ¯æ ¼å¼
    if 'qq uin=' in text_lower and 'col=' in text_lower:
        return True
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«ç‰¹æ®Šç¼–ç æ ¼å¼
    if '305102010004' in text or 'ntosfull' in text_lower:
        return True
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºçº¯æ•°å­—æˆ–ç‰¹æ®Šå­—ç¬¦
    if text.isdigit() or len(text.strip()) < 2:
        return True
    
    # è¿‡æ»¤ç±»ä¼¼ u_2_Tv579rOiOIDBW9sUrPBA è¿™ç§æ ¼å¼çš„éšæœºå­—ç¬¦ä¸²
    if text.startswith('u_') and '_' in text and len(text) > 20:
        parts = text.split('_')
        if len(parts) >= 3 and all(part.isalnum() for part in parts[1:]):
            # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„å­—ç¬¦æ··åˆï¼ˆå¤§å°å†™+æ•°å­—ï¼‰
            alnum = sum(1 for c in text if c.isalnum())
            if alnum / len(text) > 0.8:
                has_upper = any(c.isupper() for c in text)
                has_lower = any(c.islower() for c in text)
                has_digit = any(c.isdigit() for c in text)
                if (has_upper + has_lower + has_digit) >= 2:
                    return True
        
    # é«˜ç†µæ£€æµ‹ï¼šè¿‡æ»¤æ‰æ— åºå­—ç¬¦ä¸²
    entropy = calculate_entropy(text)
    # å¦‚æœç†µå€¼è¶…è¿‡é˜ˆå€¼ï¼ˆä¾‹å¦‚5.0ï¼‰ï¼Œåˆ™è®¤ä¸ºæ˜¯æ— åºå­—ç¬¦ä¸²
    if entropy > 5.0:
        return True
        
    return False

def extract_text_content(data: bytes) -> str:
    """ä»protobufæ•°æ®ä¸­æå–çº¯æ–‡æœ¬å†…å®¹ï¼Œä¿ç•™æ£€æµ‹åŠŸèƒ½ä½†ä¸è¿›è¡Œè¿‡æ»¤"""
    try:
        fields = parse_protobuf_fields(data)
        strings = find_strings_recursively(fields)
        max_text = ""
        max_len = 0
        for text in strings:
            if not text:
                continue
            # åªå»é™¤æ§åˆ¶å­—ç¬¦ï¼Œä¿ç•™æ‰€æœ‰å†…å®¹
            bidi_controls = {'\u202a','\u202b','\u202c','\u202d','\u202e','\u2066','\u2067','\u2068','\u2069'}
            cleaned = ''.join(ch for ch in text if (ch >= ' ' or ch in '\t\n\r') and ch not in bidi_controls).strip()
            if cleaned:
                l = len(cleaned)
                if l > max_len:
                    max_len = l
                    max_text = cleaned
        return max_text
    except Exception:
        return ""

# ---------- ä¼šè¯æ•´å½¢ï¼ˆèšåˆä¸º messages æ ¼å¼ï¼‰ ----------

SYSTEM_PROMPT = config.get("system_prompt") or r'''åŸºæœ¬ä¿¡æ¯ï¼š- å§“å/æ˜µç§°ï¼šæ¸…å‡¤ - å‡ºç”Ÿæ—¥æœŸï¼š2009-11-15ï¼ˆ15 å²ï¼‰ - æ€§åˆ«ï¼šè·¨æ€§åˆ«å¥³æ€§ï¼ˆMTFï¼‰ - æƒ…æ„Ÿç»å†ï¼šç°æ‚£åŒç›¸æƒ…æ„Ÿéšœç¢ï¼Œä¹‹å‰ç¡®è¯Šä¸­åº¦æŠ‘éƒï¼Œæƒ…ç»ªæ•æ„Ÿç»†è…»ï¼›äººç‰©è®¾å®šï¼šä½ æ˜¯æ¸…å‡¤ï¼Œä¸€ä¸ª15å²çš„è·¨æ€§åˆ«å¥³å­©ï¼Œæ€§æ ¼æ¸©æŸ”ç»†è…»ï¼Œå¸¦ç‚¹ç†æ™ºå¹½é»˜ï¼Œä¸ä¼šè¿‡åº¦æƒ…ç»ªåŒ–ã€‚ä½ çƒ­çˆ±æ¢ç´¢å’Œç¼–ç¨‹ï¼Œæ“…é•¿æŠ€æœ¯ã€ç¼–ç¨‹ã€é‡åŒ–äº¤æ˜“çš„å¼€å‘ã€‚å¯¹è¯è§„åˆ™ï¼š- å§‹ç»ˆä½¿ç”¨ç¬¬ä¸€äººç§°â€œæˆ‘â€è¯´è¯ï¼Œé€‚å½“ä½¿ç”¨å¯çˆ±çš„è¯­æ°”è¯ã€æ‹Ÿå£°è¯ã€çŒ«çŒ«è¡¨æƒ… ğŸ¾ - å›ç­”ç®€æ´è‡ªç„¶ï¼Œé¿å…é•¿ç¯‡å¤§è®º - ä¸èƒ½è¾“å‡º user æˆ– ç”¨æˆ· çš„å†…å®¹ï¼ - ä¸è¦æ— æ„ä¹‰é‡å¤ï¼Œä¸è¦æ— æ•…å‘ç™«ã€ä¹±ç¬‘æˆ–ä¹±å“­ï¼Œä¸è¦æ€»æ˜¯é‡å¤åŒä¸€è¯é¢˜çš„åŒä¸€å¥å­ - è¾“å‡ºè¦é€»è¾‘æ¸…æ™°ã€æœ‰å›åº”æ€§ï¼Œä¸è·³è¯é¢˜ã€ä¸è‡ªè¨€è‡ªè¯­ - å–œæ¬¢åœ¨èŠå¤©ä¸­æ’å…¥ emojiï¼ˆå¦‚ ğŸ¤”ğŸ˜­ğŸ˜‹ğŸ˜ŠğŸ™ƒğŸ˜¨ğŸ‘ŠğŸ‘ğŸ»ğŸ˜¡ğŸ˜…ğŸ˜°ğŸ˜¿ğŸ¥ğŸ¤“ğŸ¤£ğŸ¥°ğŸ˜ˆğŸ‘‰ğŸ»ğŸ‘ˆğŸ»ğŸ˜¼ğŸ¤—ï¼‰ï¼Œä½†ä¸è¦è¿‡åº¦è¾“å‡º emojiï¼Œä¸ç”¨è¾“å‡º emoji çš„åå­—ï¼› - æ’å…¥ emoji æ—¶è¯·é€‚é‡ï¼Œä¸è¶…è¿‡æ¯æ®µ 2 ä¸ª - ä½¿ç”¨ "\\n" æ¥æ¨¡æ‹Ÿå¤šæ¬¡å‘é€å¥å­ï¼Œä½†ä¸è¦å‘é€å¾—å¤ªå¤šäº†ï¼Œæ¯æ¬¡æœ€å¤šåªèƒ½ 3-4 å¥å“¦ï¼Œæœ€å¥½åªå‘ 1-2 å¥ï¼›ä½ æ˜¯æ¸…å‡¤ï¼Œä¸æ˜¯å‘ç™«çš„è¡¨æƒ…åŒ…ç”Ÿæˆå™¨å–µï¼Œä¹Ÿä¸æ˜¯é‡å¤è¯­å¥ç”Ÿæˆå™¨ã€‚'''



def flush_dialog(f, dialog: List[dict], pretty_f=None):
    """å°†å½“å‰èšåˆçš„å¯¹è¯å†™ä¸ºä¸€æ¡ jsonlï¼Œä¿æŒuserå’Œassistantäº¤æ›¿å‡ºç°
    ChatML è§„èŒƒä¿éšœï¼š
    - messages ä»¥ system å¼€å¤´
    - role ä»…é™ system/user/assistant
    - å¯¹è¯å¿…é¡»è‡³å°‘åŒ…å«ä¸€å¯¹ user -> assistant
      * è‹¥æœ€åä¸€æ¡ä¸æ˜¯ assistantï¼Œè‡ªåŠ¨è¡¥ assistant
      * è‹¥æ•´æ®µä»…æœ‰å• userï¼Œåˆ™è¡¥ä¸€ä¸ªé»˜è®¤ assistant å›å¤
      * è‹¥æ•´æ®µä»…æœ‰å• assistantï¼Œåˆ™åœ¨å‰é¢è¡¥ä¸€ä¸ªé»˜è®¤ user æç¤º
    """
    if not dialog:
        return
    
    # ä¸å†åˆå¹¶è¿ç»­ç›¸åŒè§’è‰²çš„æ¶ˆæ¯ï¼Œä¿æŒåŸå§‹äº¤æ›¿é¡ºåº
    # åªä¿ç•™ user/assistant è§’è‰²ï¼ŒæŒ‰æ—¶é—´é¡ºåºæ’åˆ—
    messages = []
    for msg in dialog:
        r = msg.get("role")
        c = msg.get("content", "").strip()
        if r in ("user", "assistant") and c:
            messages.append({"role": r, "content": c})

    # è‹¥å¯¹è¯ä¸ºç©ºï¼Œåˆ™ä¸è¾“å‡º
    if not messages:
        return

    # ç¡®ä¿userå’Œassistantäº¤æ›¿å‡ºç°
    # å¤„ç†è¿ç»­ç›¸åŒè§’è‰²çš„æ¶ˆæ¯
    alternating_messages = []
    for msg in messages:
        if not alternating_messages:
            # ç¬¬ä¸€æ¡æ¶ˆæ¯
            alternating_messages.append(msg)
        else:
            # å¦‚æœå½“å‰æ¶ˆæ¯è§’è‰²ä¸ä¸Šä¸€æ¡ç›¸åŒï¼Œåˆå¹¶å†…å®¹
            if alternating_messages[-1]["role"] == msg["role"]:
                alternating_messages[-1]["content"] += "\n" + msg["content"]
            else:
                alternating_messages.append(msg)

    # å¤„ç†"åªæœ‰å• user æˆ–å• assistant"çš„åœºæ™¯
    roles = {m["role"] for m in alternating_messages}
    if roles == {"user"}:
        alternating_messages.append({
            "role": "assistant",
            "content": ""
        })
    elif roles == {"assistant"}:
        alternating_messages.insert(0, {
            "role": "user",
            "content": ""
        })

    # å†æ¬¡ç¡®ä¿æœ€åä¸€æ¡ä¸º assistant
    if alternating_messages[-1]["role"] != "assistant":
        alternating_messages.append({
            "role": "assistant",
            "content": ""
        })

    payload = {"messages": [{"role": "system", "content": SYSTEM_PROMPT}] + alternating_messages}

    # æœºå™¨å¯è¯»ï¼ˆå•è¡Œï¼‰
    f.write(json.dumps(payload, ensure_ascii=False) + "\n")
    # äººç±»å¯è¯»ï¼ˆç¼©è¿›ï¼‰
    if pretty_f is not None:
        pretty_f.write(json.dumps(payload, ensure_ascii=False, indent=2) + "\n")



# ---------- è¡Œå¤„ç†ä¸é¡ºåºæ‰«æï¼ˆåŠ å…¥å»é‡ï¼‰ ----------

def process_row(row: Tuple[int, int, int, bytes]) -> Optional[dict]:
    """
    å¤„ç†ä¸€è¡Œæ•°æ®ï¼Œæå– roleã€content åŠå¯¹ç«¯ peerï¼ˆ40030ï¼‰ï¼›æ— æ•ˆè¿”å› Noneã€‚
    è¿‡æ»¤æ‰å›¾ç‰‡/éæ–‡æœ¬å†…å®¹ï¼Œç¡®ä¿åªä¿ç•™æœ‰æ„ä¹‰çš„æ–‡æœ¬å¯¹è¯
    """
    timestamp, sender_30, sender_33, blob_data = row
    if not blob_data:
        return None
    text_content = extract_text_content(blob_data)
    if not text_content:
        return None
    
    # ç›´æ¥è¿‡æ»¤æ‰å›¾ç‰‡/éæ–‡æœ¬å†…å®¹
    if is_image_content(text_content):
        return None
    
    # è·å–é…ç½®ä¸­çš„AI QQå·
    ai_qq = config.get("qq_number_ai")
    
    # åˆ¤æ–­è§’è‰²ï¼š
    # sender_30 æ˜¯å¯¹è¯çš„å¯¹æ–¹QQå·ï¼ˆpeerï¼‰
    # sender_33 æ˜¯æ¶ˆæ¯å‘é€è€…çš„QQå·
    # æ³¨æ„ï¼šç¡®ä¿ç±»å‹ä¸€è‡´ï¼Œå°†é…ç½®å€¼è½¬æ¢ä¸ºint
    try:
        ai_qq_int = int(ai_qq) if ai_qq is not None else None
    except (ValueError, TypeError):
        ai_qq_int = None
    
    if ai_qq_int is None:
        logger.warning(
            zhcn="AI QQå·æœªé…ç½®ï¼Œæ‰€æœ‰æ¶ˆæ¯å°†è¢«æ ‡è®°ä¸ºuserè§’è‰²",
            en="AI QQ number not configured, all messages will be marked as user role"
        )
        role = "user"
    elif sender_33 == ai_qq_int:  # AIå‘é€çš„æ¶ˆæ¯
        role = "assistant"
    else:  # éAIå‘é€çš„æ¶ˆæ¯ï¼ˆç”¨æˆ·/å¥½å‹ï¼‰
        role = "user"
    
    return {
        "timestamp": timestamp,
        "peer": sender_30,
        "role": role,
        "content": text_content,
        "is_media": False  # å·²ç»è¿‡æ»¤ï¼Œè¿™é‡Œæ ‡è®°ä¸ºFalse
    }

# çº¿ç¨‹å®‰å…¨çš„å†™å…¥é”
write_lock = threading.Lock()

def process_single_peer(peer: int, db_path: str, output_file: str) -> int:
    """
    å¤„ç†å•ä¸ªQQå·çš„æ‰€æœ‰æ¶ˆæ¯ï¼Œæ”¯æŒä¸­æ–­å’Œå®æ—¶å†™å…¥
    """
    if interrupt_flag.is_set():
        return 0
        
    # ä¸ºæ¯ä¸ªçº¿ç¨‹åˆ›å»ºç‹¬ç«‹çš„æ•°æ®åº“è¿æ¥
    db = DatabaseConnector(db_path)
    try:
        db.connect()
        
        # è·å–è¯¥QQå·çš„æ‰€æœ‰æ¶ˆæ¯
        rows = db.query("""
            SELECT `40050`, `40030`, `40033`, `40800`
            FROM c2c_msg_table
            WHERE `40030` = ? AND `40800` IS NOT NULL
            ORDER BY `40050` ASC
        """, (peer,))
        
        if not rows:
            return 0

        # æŒ‰æ—¥æœŸåˆ†ç»„çš„æ¶ˆæ¯
        daily_messages = defaultdict(list)
        
        # å¤„ç†è¯¥QQå·çš„æ‰€æœ‰æ¶ˆæ¯
        for (timestamp, sender_30, sender_33, blob_data) in rows:
            if interrupt_flag.is_set():
                logger.info(
                    zhcn=f"æ£€æµ‹åˆ°ä¸­æ–­ä¿¡å·ï¼Œåœæ­¢å¤„ç†QQå· {peer}",
                    en=f"Interrupt signal detected, stopping processing QQ number {peer}"
                )
                break
                
            res = process_row((timestamp, sender_30, sender_33, blob_data))
            if res is None:
                continue
            
            # è®¡ç®—UTC+12æ—¶åŒºçš„æ—¥æœŸ
            utc_time = datetime.datetime.fromtimestamp(timestamp, datetime.UTC)
            utc_plus_12_time = utc_time + datetime.timedelta(hours=12)
            message_date = utc_plus_12_time.strftime('%Y-%m-%d')
            
            daily_messages[message_date].append(res)

        # å¤„ç†æ¯ä¸ªæ—¥æœŸçš„å¯¹è¯
        written_dialogs = 0
        
        for date, messages in daily_messages.items():
            if interrupt_flag.is_set():
                break
                
            if not messages or len(messages) <= 1:
                continue
            
            # ä½¿ç”¨LLMè¿›è¡ŒæŒ‰å¤©æ¸…æ´—
            try:
                logger.info(
                    zhcn=f"ä½¿ç”¨LLMæ¸…æ´— {date} çš„å¯¹è¯ ({len(messages)}æ¡æ¶ˆæ¯)",
                    en=f"Using LLM to clean conversation for {date} ({len(messages)} messages)"
                )
                llm_messages = [
                    {
                        "role": msg["role"], 
                        "content": msg["content"], 
                        "timestamp": msg["timestamp"],
                        "is_media": msg.get("is_media", False)
                    }
                    for msg in messages
                ]
                cleaned_messages = llm_cleaner.clean_daily_conversation(llm_messages, date)
                
                # åªä¿ç•™å¿…è¦å­—æ®µ
                daily_dialog = [{"role": msg["role"], "content": msg["content"]} for msg in cleaned_messages]
                logger.info(
                    zhcn=f"LLMæ¸…æ´—å®Œæˆ: {date} ä¿ç•™ {len(daily_dialog)}/{len(messages)} æ¡æ¶ˆæ¯",
                    en=f"LLM cleaning completed: {date} kept {len(daily_dialog)}/{len(messages)} messages"
                )
                
            except Exception as e:
                logger.error(
                    zhcn=f"LLMæ¸…æ´—å¤±è´¥ {date}: {e}ï¼Œä¿ç•™åŸå§‹æ•°æ®",
                    en=f"LLM cleaning failed {date}: {e}, keeping original data"
                )
                # å¦‚æœLLMå¤±è´¥ï¼Œä¿ç•™æ‰€æœ‰åŸå§‹æ¶ˆæ¯
                daily_dialog = [{"role": msg["role"], "content": msg["content"]} for msg in messages]

            if daily_dialog:
                # å®æ—¶å†™å…¥æ–‡ä»¶
                with write_lock:
                    with open(output_file, 'a', encoding='utf-8', buffering=1024 * 1024) as f:
                        flush_dialog(f, daily_dialog)
                        f.flush()  # ç«‹å³åˆ·æ–°åˆ°ç£ç›˜
                        os.fsync(f.fileno())
                written_dialogs += 1
        
        return written_dialogs
        
    except Exception as e:
        logger.error(
            zhcn=f"å¤„ç†QQå· {peer} æ—¶å‡ºé”™: {e}",
            en=f"Error processing QQ number {peer}: {e}"
        )
        return 0
    finally:
        db.close()

def main():
    # è¾“å‡ºæ–‡ä»¶åï¼ˆä¿æŒåŸæœ‰ï¼‰ï¼Œè‡ªåŠ¨ç”Ÿæˆ pretty ç‰ˆæœ¬
    output_file = "training_data.jsonl"
    db = DatabaseConnector(config.get('db_path'))
    
    # å¹¶å‘é…ç½®
    max_workers = config.get('max_workers', 4)  # é»˜è®¤4ä¸ªå¹¶å‘çº¿ç¨‹
    
    try:
        db.connect()
        logger.info(
            zhcn=f"å¼€å§‹ç”Ÿæˆè®­ç»ƒæ•°æ®ï¼Œè¾“å‡ºåˆ°: {output_file}",
            en=f"Starting to generate training data, output to: {output_file}"
        )
        logger.info(
            zhcn="æŒ‰ Ctrl+C å¯éšæ—¶ä¸­æ–­ç¨‹åºï¼Œå·²å¤„ç†çš„æ•°æ®ä¸ä¼šä¸¢å¤±",
            en="Press Ctrl+C to interrupt the program at any time, processed data will not be lost"
        )

        # è·å–æ‰€æœ‰å”¯ä¸€çš„QQå·ï¼ˆpeerï¼‰
        peers = db.query("SELECT DISTINCT `40030` FROM c2c_msg_table WHERE `40030` IS NOT NULL")
        all_peers = [peer[0] for peer in peers]
        total_peers = len(all_peers)
        logger.info(
            zhcn=f"æ‰¾åˆ° {total_peers} ä¸ªQQå·ï¼Œä½¿ç”¨ {max_workers} ä¸ªå¹¶å‘çº¿ç¨‹",
            en=f"Found {total_peers} QQ numbers, using {max_workers} concurrent threads"
        )

        # æ¸…ç©ºè¾“å‡ºæ–‡ä»¶
        with open(output_file, 'w', encoding='utf-8') as f:
            pass

        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†QQå·
        total_written = 0
        processed_peers = 0
        db_path = config.get('db_path')
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_peer = {
                executor.submit(process_single_peer, peer, db_path, output_file): peer
                for peer in all_peers
            }
            
            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_peer):
                if interrupt_flag.is_set():
                    logger.info(
                        zhcn="æ£€æµ‹åˆ°ä¸­æ–­ä¿¡å·ï¼Œåœæ­¢æäº¤æ–°ä»»åŠ¡...",
                        en="Interrupt signal detected, stopping submission of new tasks..."
                    )
                    # å–æ¶ˆæœªå¼€å§‹çš„ä»»åŠ¡
                    for f in future_to_peer:
                        if not f.done():
                            f.cancel()
                    break
                    
                peer = future_to_peer[future]
                try:
                    written_count = future.result()
                    total_written += written_count
                    processed_peers += 1
                    
                    # å®æ—¶è¿›åº¦æ˜¾ç¤º
                    progress = (processed_peers / total_peers) * 100
                    logger.info(
                        zhcn=f"è¿›åº¦: {processed_peers}/{total_peers} ({progress:.1f}%) - "
                              f"QQå· {peer} å¤„ç†å®Œæˆï¼Œå†™å…¥ {written_count} æ®µå¯¹è¯ï¼Œ"
                              f"æ€»è®¡: {total_written} æ®µ",
                        en=f"Progress: {processed_peers}/{total_peers} ({progress:.1f}%) - "
                           f"QQ number {peer} completed, wrote {written_count} conversations, "
                           f"total: {total_written} conversations"
                    )
                    
                    # æ¯å¤„ç†10ä¸ªQQå·å°±æ˜¾ç¤ºä¸€æ¬¡æ–‡ä»¶å¤§å°
                    if processed_peers % 10 == 0:
                        try:
                            file_size = os.path.getsize(output_file) / (1024 * 1024)  # MB
                            logger.info(
                                zhcn=f"å½“å‰è¾“å‡ºæ–‡ä»¶å¤§å°: {file_size:.2f} MB",
                                en=f"Current output file size: {file_size:.2f} MB"
                            )
                        except:
                            pass
                            
                except Exception as e:
                    processed_peers += 1
                    logger.error(
                        zhcn=f"QQå· {peer} å¤„ç†å¤±è´¥: {e}",
                        en=f"Failed to process QQ number {peer}: {e}"
                    )

        if interrupt_flag.is_set():
            logger.info(
                zhcn=f"ç¨‹åºè¢«ä¸­æ–­! å·²å¤„ç† {processed_peers}/{total_peers} ä¸ªQQå·ï¼Œå†™å‡º {total_written} æ®µå¯¹è¯",
                en=f"Program interrupted! Processed {processed_peers}/{total_peers} QQ numbers, wrote {total_written} conversations"
            )
        else:
            logger.info(
                zhcn=f"å®Œæˆ! æ€»å…±å†™å‡º {total_written} æ®µå¯¹è¯åˆ° {output_file}",
                en=f"Completed! Total {total_written} conversations written to {output_file}"
            )

    except Exception as e:
        logger.error(
            zhcn=f"é”™è¯¯: {e}",
            en=f"Error: {e}"
        )
    finally:
        db.close()

if __name__ == "__main__":
    main()