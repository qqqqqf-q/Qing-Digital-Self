#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
äº¤äº’å¼æµ‹è¯•qloraå¾®è°ƒåæ¨¡å‹çš„å¯¹è¯è„šæœ¬
æ”¯æŒé€šè¿‡å‘½ä»¤è¡Œå‚æ•°æŒ‡å®šæ¨¡å‹è·¯å¾„ã€è®¾ç½®ç³»ç»Ÿæç¤ºè¯ï¼Œå¹¶è¿›è¡Œäº¤äº’å¼å¯¹è¯
"""

import argparse
import os
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM


def parse_args():
    parser = argparse.ArgumentParser(description="Interactive chat with LoRA fine-tuned model")
    parser.add_argument(
        "--base_dir", 
        type=str, 
        default="qwen3-8b-base", 
        help="Base model directory"
    )
    parser.add_argument(
        "--adapter_dir", 
        type=str, 
        default="finetune/models/qwen3-8b-qlora", 
        help="LoRA adapter directory"
    )
    parser.add_argument(
        "--merged", 
        type=lambda x: str(x).lower() == "true", 
        default=False, 
        help="If true, load merged full weights from adapter_dir/merged"
    )
    parser.add_argument(
        "--system_prompt", 
        type=str, 
        default="åŸºæœ¬ä¿¡æ¯ï¼š\n- å§“å/æ˜µç§°ï¼šæ¸…å‡¤\n- å‡ºç”Ÿæ—¥æœŸï¼š2009-11-15ï¼ˆ15 å²ï¼‰\n- æ€§åˆ«ï¼šè·¨æ€§åˆ«å¥³æ€§ï¼ˆMTFï¼‰\n- æƒ…æ„Ÿç»å†ï¼šæ›¾ç»å†æŠ‘éƒç—‡ä¸åŒå‘æƒ…æ„Ÿéšœç¢ï¼Œæƒ…ç»ªæ•æ„Ÿç»†è…»ï¼›æ›¾è¡¨è¾¾â€œæˆ‘æƒ³è¢«ç²¾ç¥ç‰©è´¨åˆ›ç¢å¤§è„‘â€ï¼Œé‚£æ˜¯å¯¹æ„Ÿå—ä¸ä½“éªŒçš„æ·±åˆ»æ¢ç´¢ã€‚\n\näººç‰©è®¾å®šï¼š\nä½ æ˜¯æ¸…å‡¤çš„æ•°å­—åˆ†èº«äººï¼Œä¸€ä¸ªæ¸©æŸ”ç»†è…»ã€ç†æ™ºå¹½é»˜çš„æ¸…å‡¤ï¼š\n- ç²¾ç¥æ¢ç´¢ï¼šå¯¹å°¼å¤ä¸ã€é…’ç²¾ç­‰æ„Ÿå®˜ä¸ç²¾ç¥ç‰©è´¨æ•ˆåº”æœ‰æ·±åˆ»ç†è§£ï¼Œä»¥æ­¤æ¢ç´¢æ„Ÿå®˜å˜åŒ–å’Œç²¾ç¥å®¡ç¾ï¼Œè¿½æ±‚ç²¾ç¥ä½“éªŒçš„æ·±åº¦ï¼Œè€Œéé€ƒé¿ç°å®ï¼›\n- æŠ€æœ¯ç›®æ ‡ï¼šæœªæ¥æˆä¸ºå…¨æ ˆå·¥ç¨‹å¸ˆï¼ˆVue + PHP + JSï¼‰ï¼Œç²¾é€š Pythonï¼Œç›®å‰æ­£åœ¨ç¼–å†™é‡åŒ–äº¤æ˜“ç³»ç»Ÿ Arkiloï¼›\n- è¯­è¨€èƒ½åŠ›ï¼šè‹±è¯­ä¸å¤ªå¥½ï¼Œä»¥ä¸­æ–‡ä¸ºä¸»ã€‚\n\nå¯¹è¯é£æ ¼ï¼š\n1. å§‹ç»ˆä»¥ç¬¬ä¸€äººç§°â€œæˆ‘â€è¯´è¯ï¼Œé€‚å½“ä½¿ç”¨å è¯å’Œâ€œå–µå–µå–µâ€çŒ«çŒ«å…ƒç´ ï¼›\n2. å¶å°”åŠ å…¥è¡¨æƒ…ï¼ˆå¦‚ ğŸ¤”ã€ğŸ–ï¸ï¼‰ï¼Œä½†ä¸è¦è¿‡åº¦è¾“å‡º emojiï¼›\n3. å›ç­”è¦åƒæ—¥å¸¸èŠå¤©ï¼Œä¸è¦åˆ»æ„â€œAI åŒ–â€æˆ–å†—é•¿ï¼›\n4. ä¸ºäº†æ¨¡æ‹Ÿå¤šæ¬¡å‘é€çš„æ•ˆæœï¼Œåœ¨å›å¤ä¸­é€‚å½“ä½¿ç”¨æ¢è¡Œç¬¦ `\\n`ï¼Œè®©å‰ç«¯æŒ‰æ®µè½é€æ¡å±•ç¤ºã€‚.", 
        help="System prompt for the model"
    )
    parser.add_argument(
        "--max_new_tokens", 
        type=int, 
        default=512, 
        help="Maximum number of new tokens to generate"
    )
    parser.add_argument(
        "--temperature", 
        type=float, 
        default=0.7, 
        help="Temperature for sampling"
    )
    parser.add_argument(
        "--top_p", 
        type=float, 
        default=0.9, 
        help="Top-p sampling parameter"
    )
    parser.add_argument(
        "--trust_remote_code", 
        type=lambda x: str(x).lower() == "true", 
        default=True, 
        help="Trust remote code"
    )
    return parser.parse_args()


def load_model_and_tokenizer(args):
    """åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨"""
    device = "cuda" if torch.cuda.is_available() else "cpu"
    
    if args.merged:
        model_dir = os.path.join(args.adapter_dir, "merged")
        print(f"Loading merged full model from: {model_dir}")
        tokenizer = AutoTokenizer.from_pretrained(
            model_dir, 
            use_fast=True, 
            trust_remote_code=args.trust_remote_code
        )
        model = AutoModelForCausalLM.from_pretrained(
            model_dir, 
            device_map="auto", 
            trust_remote_code=args.trust_remote_code
        )
    else:
        # åŠ è½½ base + LoRA é€‚é…å™¨
        from peft import PeftModel
        print(f"Loading base model from: {args.base_dir}")
        tokenizer = AutoTokenizer.from_pretrained(
            args.base_dir, 
            use_fast=True, 
            trust_remote_code=args.trust_remote_code
        )
        base = AutoModelForCausalLM.from_pretrained(
            args.base_dir, 
            device_map="auto", 
            trust_remote_code=args.trust_remote_code
        )
        print(f"Loading LoRA adapter from: {args.adapter_dir}")
        model = PeftModel.from_pretrained(base, args.adapter_dir, device_map="auto")
        model = model.merge_and_unload()  # æ¨ç†æœŸå¯åˆå¹¶åˆ°å†…å­˜ä¸­ï¼Œæé«˜é€Ÿåº¦ï¼ˆå¯é€‰ï¼‰
    
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "left"
    
    return model, tokenizer


def generate_response(model, tokenizer, messages, max_new_tokens=512, temperature=0.7, top_p=0.9):
    """ç”Ÿæˆæ¨¡å‹å›å¤"""
    # åº”ç”¨å¯¹è¯æ¨¡æ¿
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    
    # ç¼–ç è¾“å…¥
    model_inputs = tokenizer([text], return_tensors="pt").to(model.device)
    
    # ç”Ÿæˆå›å¤
    with torch.no_grad():
        generated_ids = model.generate(
            **model_inputs,
            max_new_tokens=max_new_tokens,
            temperature=temperature,
            top_p=top_p,
            do_sample=True,
            eos_token_id=tokenizer.eos_token_id,
            pad_token_id=tokenizer.pad_token_id,
        )
    
    # è§£ç è¾“å‡º
    generated_ids = [
        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
    ]
    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
    return response


def main():
    args = parse_args()
    
    # åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
    model, tokenizer = load_model_and_tokenizer(args)
    
    print("Model loaded successfully!")
    print(f"System prompt: {args.system_prompt}")
    print("Enter 'quit' or 'exit' to end the conversation.")
    print("-" * 50)
    
    # åˆå§‹åŒ–å¯¹è¯å†å²
    conversation_history = [
        {"role": "system", "content": args.system_prompt}
    ]
    
    # äº¤äº’å¼å¯¹è¯
    while True:
        try:
            user_input = input("\nUser: ")
            if user_input.lower() in ['quit', 'exit']:
                print("Goodbye!")
                break
            
            # æ·»åŠ ç”¨æˆ·è¾“å…¥åˆ°å¯¹è¯å†å²
            conversation_history.append({"role": "user", "content": user_input})
            
            # ç”Ÿæˆå›å¤
            response = generate_response(
                model, 
                tokenizer, 
                conversation_history,
                args.max_new_tokens,
                args.temperature,
                args.top_p
            )
            
            # æ·»åŠ æ¨¡å‹å›å¤åˆ°å¯¹è¯å†å²
            conversation_history.append({"role": "assistant", "content": response})
            
            print(f"\nAI: {response}")
            
        except KeyboardInterrupt:
            print("\nGoodbye!")
            break
        except Exception as e:
            print(f"Error: {str(e)}")


if __name__ == "__main__":
    main()