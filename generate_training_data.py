#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import sys
import json
import os
import signal
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Optional, List, Tuple, Dict
from database.db_connector import DatabaseConnector
import datetime
from collections import defaultdict
from utils.config.config import get_config
from utils.logger.logger import get_logger
import threading
import math

# å¯¼å…¥LM Studioæ”¯æŒï¼ˆå¯é€‰ï¼‰
try:
    from openai.openai_client import LLMDataCleaner
    LLM_AVAILABLE = True
    # åˆ›å»ºå…¨å±€æ¸…æ´—å™¨å®ä¾‹
    llm_cleaner = LLMDataCleaner()
except ImportError:
    LLM_AVAILABLE = False
    llm_cleaner = None
    
# è·å–é…ç½®å®ä¾‹
config = get_config()
logger = get_logger('Generate_training_data')

# å…¨å±€ä¸­æ–­æ ‡å¿—
interrupt_flag = threading.Event()

def signal_handler(signum, frame):
    """å¤„ç†Ctrl+Cä¸­æ–­ä¿¡å·"""
    logger.info(
        zhcn="æ¥æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨ä¼˜é›…é€€å‡º...",
        en="Received interrupt signal, gracefully exiting..."
    )
    interrupt_flag.set()

# æ³¨å†Œä¿¡å·å¤„ç†å™¨
signal.signal(signal.SIGINT, signal_handler)

# ---------- è§£æç›¸å…³ï¼šå¾®ä¼˜åŒ– ----------

def decode_varint(data: bytes, pos: int = 0) -> tuple[int, int]:
    """ä»æŒ‡å®šä½ç½®è§£ç ä¸€ä¸ª Varint"""
    result = 0
    shift = 0
    original_pos = pos
    while True:
        if pos >= len(data):
            raise IndexError(f"Varint decoding failed: buffer ended unexpectedly at position {original_pos}.")
        b = data[pos]
        result |= (b & 0x7F) << shift
        pos += 1
        if not (b & 0x80):
            break
        shift += 7
        if shift >= 64:
            raise ValueError("Varint is too long or invalid.")
    return result, pos

def parse_protobuf_fields(data: bytes) -> list[tuple[int, int, bytes]]:
    """è§£æprotobufå­—æ®µ"""
    pos = 0
    fields = []
    ln = len(data)
    while pos < ln:
        try:
            tag, pos = decode_varint(data, pos)
            field_number = tag >> 3
            wire_type = tag & 0x7

            if wire_type == 0:  # Varint
                value, pos = decode_varint(data, pos)
                # é¿å…é¢‘ç¹ to_bytes è½¬æ¢ï¼Œä¿ç•™ä¸ºæœ€å°è¡¨ç¤º
                v = value.to_bytes((value.bit_length() + 7) // 8, 'little') if value else b'\x00'
                fields.append((field_number, wire_type, v))
            elif wire_type == 1:  # 64-bit
                if pos + 8 > ln:
                    raise IndexError(f"Field {field_number}: Not enough data for 64-bit value.")
                value = data[pos:pos+8]
                pos += 8
                fields.append((field_number, wire_type, value))
            elif wire_type == 2:  # Length-delimited
                length, pos = decode_varint(data, pos)
                end = pos + length
                if end > ln:
                    raise IndexError(f"Field {field_number}: Declared length {length} exceeds buffer size.")
                value = data[pos:end]
                pos = end
                fields.append((field_number, wire_type, value))
            elif wire_type == 5:  # 32-bit
                if pos + 4 > ln:
                    raise IndexError(f"Field {field_number}: Not enough data for 32-bit value.")
                value = data[pos:pos+4]
                pos += 4
                fields.append((field_number, wire_type, value))
            else:
                # è·³è¿‡æœªçŸ¥ç±»å‹
                break
        except (ValueError, IndexError):
            break
    return fields

# é¢„å¤„ç†å›¾ç‰‡/éæ–‡æœ¬æŒ‡ç¤ºå™¨ï¼Œé™å°å†™å¹¶ç”¨é›†åˆåŠ é€ŸæŸ¥è¯¢
_IMAGE_INDICATORS = {
    '.jpg', '.jpeg', '.png', '.gif', '.bmp', '.webp',
    '.mp4', '.amr', '.mp3', '.avi', '.mov', '.wmv',  # æ‰©å±•åª’ä½“æ‰©å±•
    'offpic_new', 'thumb', 'ori', 'pic', 'bigpic', 'smallpic',
    '[åŠ¨ç”»è¡¨æƒ…]', '[è¡¨æƒ…]', '[å›¾ç‰‡]', '[é—ªç…§]', '[è¡¨æƒ…]', '[åŠ¨ç”»]', '[è´´å›¾]',
    '[è¯­éŸ³]', '[è§†é¢‘]', '[æ–‡ä»¶]', '[çº¢åŒ…]', '[è½¬è´¦]', '[ä½ç½®]', '[åç‰‡]',  # å¸¸è§IMå ä½
    '[ç³»ç»Ÿæ¶ˆæ¯]', '[QQæ¶ˆæ¯]', '[ç¾¤æ¶ˆæ¯]', '[å¥½å‹æ¶ˆæ¯]',
    'ntosfull', 'documents\\tencent files', 'tencent files',
    'term=', 'is_origin=', '_198.jpg', '_720.jpg', '_198_', '_720_',
    '/1684773595-', '/3259379048-', '/thumb/', '/ori/', '/pic/',
    '-05d65b12ef0481a7337c63775bac3ffd', 'b0ddba64f40a8d68eec61b26eabf7750',
    'u_sp2m7izf1zlnjvtcwqx0sg', 'u_ovchmti-zvlwu_0zm_ulgw',
    'multimedia.nt.qq.com.cn', 'qq.com', 'tencent.com', 'gtimg.cn',
    'mqqapi://', 'https://', 'http://', 'ftp://', 'file://',
    'undefined', 'undefine', 'null', 'none',
    'è¯·ä½¿ç”¨æ–°ç‰ˆæ‰‹æœºqqæŸ¥çœ‹', 'è¿ç»­äº’å‘æ¶ˆæ¯ä¸­æ–­', 'æ‹çˆ±ä¹‹é’¥',
    'ç‚¹å‡»æ¢å¤', 'er0s', 'show_pslcard', 'pslcard',
    'interactive_new', 'club.vip.qq.com', 'vip.qq.com',
    'downv6.qq.com', 'qzone_love', 'ti.qq.com',
    # QQäº’åŠ¨æ ‡ç­¾å’Œç³»ç»Ÿæ¶ˆæ¯
    'gtip align=', 'qq uin=', 'uin=', 'col=', 'nm=', 'jp=', 'nor txt=',
    'è¸¢äº†è¸¢', 'æ‘¸æ‘¸å¤´', 'å®Œæˆäº†', 'è·å¾—', 'æŸ¥çœ‹è¯¦æƒ…', 'äº’åŠ¨æ ‡è¯†',
    # JSONç»“æ„å’ŒURLå‚æ•°
    '{"', '"}', '"align"', '"items"', '"type"', '"txt"', '"col"', '"url"', '"img"',
    'local_jp', 'param', 'url=', 'rkey=', 'target_uin=', 'mutualmark_id=',
    # ç³»ç»Ÿæç¤ºå’ŒçŠ¶æ€æ¶ˆæ¯
    'ä½ ä»¬äº’å‘æ¶ˆæ¯ï¼Œå°æ°´èŠ±å¼€å¿ƒæäº†ï¼Œæ­å–œé‡æ–°ç‚¹äº®åˆæ³›æ¶Ÿæ¼ª',
    'è¯­éŸ³é€šè¯', 'è§†é¢‘é€šè¯', 'é€šè¯æ—¶é•¿', 'é€šè¯ç»“æŸ',
    # æƒ…ä¾£å’Œäº’åŠ¨æ ‡è¯†
    'æƒ…ä¾£ç©ºé—´', 'æƒ…ä¾£', 'ç¥ä»™çœ·ä¾£', 'çŸ¥å·±æµªèŠ±', 'ç´¯è®¡äº’å‘æ¶ˆæ¯è¶…è¿‡', 'çˆ±æ„æ°¸æ’', 'æ­å–œå‡çº§', 'äº’å‘æ¶ˆæ¯', 'ç•…èŠä¹‹ç«',
    'å‹è°Šçš„å°èˆ¹', 'å‹è°Šçš„å·¨è½®', 'èŠå¤©æ°”æ³¡', 'äº’åŠ¨æ ‡è¯†', 'å¥½å‹äº’åŠ¨',
    # å¯ç–‘çŸ­ä¸²å’Œç‰¹æ®Šæ ‡è¯†
    'd42ae9486fdbc4b7', '305102010004363034020100020445d1',
    # æ·»åŠ å¯¹ç±»ä¼¼ u_2_Tv579rOiOIDBW9sUrPBA è¿™ç§æ ¼å¼çš„è¿‡æ»¤
    'u_',
    # å…¶ä»–éœ€è¦è¿‡æ»¤çš„å†…å®¹
    'qqweb', 'res', 'mutualmark', 'nudgeaction', 'expression.jpg',
    'nudgemall', 'actionid=', 'interactive', 'new/index',
    # æ–‡ä»¶è·¯å¾„ç›¸å…³
    'c:\\users\\', 'c:\\', '\\nt_qq\\', '\\nt_data\\', '\\pic\\', '\\thumb\\',
    'æˆ‘ç°åœ¨æœ‰äº‹ä¸åœ¨'
}

def calculate_entropy(text: str) -> float:
    """è®¡ç®—å­—ç¬¦ä¸²çš„ç†µå€¼ï¼Œç”¨äºæ£€æµ‹æ— åºå­—ç¬¦ä¸²"""
    if not text:
        return 0.0
    
    # ç»Ÿè®¡æ¯ä¸ªå­—ç¬¦çš„å‡ºç°é¢‘ç‡
    char_counts = {}
    for char in text:
        char_counts[char] = char_counts.get(char, 0) + 1
    
    # è®¡ç®—ç†µå€¼
    entropy = 0.0
    text_len = len(text)
    for count in char_counts.values():
        probability = count / text_len
        entropy -= probability * math.log2(probability)
    
    return entropy

# åå…­è¿›åˆ¶å­—ç¬¦ä¸²æ¨¡å¼
_HEX_PATTERN = set('0123456789abcdef')

# éœ€è¦ä¸¥æ ¼ç­‰å€¼åŒ¹é…åˆ é™¤çš„çŸ­å ä½/ç³»ç»Ÿæ¶ˆæ¯ï¼ˆå°å†™åå¯¹æ¯”ï¼‰
_STRICT_DROP_SET = {
    'è¯­éŸ³',  # ä»…å½“æ•´æ¡å°±æ˜¯"è¯­éŸ³"ä¹‹ç±»çš„æçŸ­å ä½
    '[è¯­éŸ³]',
    '[è¯­éŸ³é€šè¯]',
    'nt_1',  # æ˜æ˜¾çš„ç³»ç»Ÿ/å ä½çŸ­token
}

def is_image_content(text: str) -> bool:
    """æ£€æµ‹æ˜¯å¦ä¸ºå›¾ç‰‡/éæ–‡æœ¬å†…å®¹ï¼Œç”¨äºLLMæ¸…æ´—å‰çš„åˆæ­¥è¿‡æ»¤"""
    if not text:
        return True  # ç©ºå†…å®¹è§†ä¸ºéæ–‡æœ¬
    
    text_lower = text.lower().strip()
    
    # æ£€æŸ¥æ˜¯å¦åœ¨æŒ‡ç¤ºå™¨é›†åˆä¸­
    if any(indicator in text_lower for indicator in _IMAGE_INDICATORS):
        return True
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºçº¯JSONç»“æ„ï¼ˆåŒ…å«å¤§æ‹¬å·ã€å¼•å·ã€å†’å·ç­‰ï¼‰
    if text.startswith('{') and text.endswith('}'):
        return True
    if '"type"' in text_lower and '"txt"' in text_lower:
        return True
    if '"align"' in text_lower and '"items"' in text_lower:
        return True
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºHTML/XMLæ ‡ç­¾æ ¼å¼
    if '<gtip' in text_lower or '</gtip>' in text_lower:
        return True
    if '<qq ' in text_lower or '<img ' in text_lower:
        return True
    if 'align="center"' in text_lower:
        return True
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºçº¯URLæˆ–æ–‡ä»¶è·¯å¾„
    if text.startswith(('http://', 'https://', 'ftp://', 'file://')):
        return True
    if 'c:\\' in text_lower and ('.jpg' in text_lower or '.png' in text_lower):
        return True
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºç³»ç»Ÿæ¶ˆæ¯æ ¼å¼
    if 'qq uin=' in text_lower and 'col=' in text_lower:
        return True
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«ç‰¹æ®Šç¼–ç æ ¼å¼
    if '305102010004' in text or 'ntosfull' in text_lower:
        return True
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºçº¯æ•°å­—æˆ–ç‰¹æ®Šå­—ç¬¦
    if text.isdigit() or len(text.strip()) < 2:
        return True
    
    # è¿‡æ»¤ç±»ä¼¼ u_2_Tv579rOiOIDBW9sUrPBA è¿™ç§æ ¼å¼çš„éšæœºå­—ç¬¦ä¸²
    if text.startswith('u_') and '_' in text and len(text) > 20:
        parts = text.split('_')
        if len(parts) >= 3 and all(part.isalnum() for part in parts[1:]):
            # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„å­—ç¬¦æ··åˆï¼ˆå¤§å°å†™+æ•°å­—ï¼‰
            alnum = sum(1 for c in text if c.isalnum())
            if alnum / len(text) > 0.8:
                has_upper = any(c.isupper() for c in text)
                has_lower = any(c.islower() for c in text)
                has_digit = any(c.isdigit() for c in text)
                if (has_upper + has_lower + has_digit) >= 2:
                    return True
        
    # é«˜ç†µæ£€æµ‹ï¼šè¿‡æ»¤æ‰æ— åºå­—ç¬¦ä¸²
    entropy = calculate_entropy(text)
    # å¦‚æœç†µå€¼è¶…è¿‡é˜ˆå€¼ï¼ˆä¾‹å¦‚5.0ï¼‰ï¼Œåˆ™è®¤ä¸ºæ˜¯æ— åºå­—ç¬¦ä¸²
    if entropy > 5.0:
        return True
        
    return False

def is_image_content_strict(text: str) -> bool:
    """ä¼ ç»Ÿçš„ä¸¥æ ¼è¿‡æ»¤æ–¹æ³•ï¼ˆä¿ç•™åŸæœ‰é€»è¾‘ä½œä¸ºå¤‡é€‰ï¼‰"""
    if not text:
        return False
    # å»é™¤æ§åˆ¶å­—ç¬¦ï¼ˆå« \u0000 ä¸ LRM/åµŒå…¥æ–¹å‘æ§åˆ¶ç­‰ï¼‰ï¼Œé¿å…"Z\u0000b\u0000"ã€"U+202D"ç­‰ä¼ªæ–‡æœ¬
    # Unicode åŒå‘æ§åˆ¶å­—ç¬¦é›†åˆï¼šU+202A..U+202E, U+2066..U+2069 ç­‰
    bidi_controls = {'\u202a','\u202b','\u202c','\u202d','\u202e','\u2066','\u2067','\u2068','\u2069'}
    t = ''.join(ch for ch in text if (ch >= ' ' or ch in '\t\n\r') and ch not in bidi_controls).strip()
    if not t:
        return True  # æ¸…ç†åä¸ºç©ºï¼Œè§†ä¸ºå™ªå£°
    
    # ä¸¥æ ¼ç­‰å€¼çŸ­å ä½
    _STRICT_DROP_SET = {
        'è¯­éŸ³',  # ä»…å½“æ•´æ¡å°±æ˜¯"è¯­éŸ³"ä¹‹ç±»çš„æçŸ­å ä½
        '[è¯­éŸ³]',
        '[è¯­éŸ³é€šè¯]',
        'nt_1',  # æ˜æ˜¾çš„ç³»ç»Ÿ/å ä½çŸ­token
    }
    if t in _STRICT_DROP_SET:
        return True
    
    # å…ˆå‰ªæï¼šçº¯æ•°å­—ç›´æ¥æ’é™¤ï¼Œä½†ä¿ç•™æœ‰æ„ä¹‰çš„å•å­—ç¬¦
    if t.isdigit():
        return True
    if len(t) == 1 and t not in ['?', 'ï¼Ÿ', '.', 'ã€‚', '!', 'ï¼']:
        return True
    
    tl = t.lower()
    
    # è¿‡æ»¤åŒ…å«ç‰¹å®šé€šè¯/æ—¶é•¿æ¨¡å¼
    # å¦‚ï¼š[è¯­éŸ³é€šè¯] é€šè¯æ—¶é•¿ 00:05 / é€šè¯æ—¶é•¿ 00:05
    if ('é€šè¯æ—¶é•¿' in t and any(c.isdigit() for c in t)) or ('è¯­éŸ³é€šè¯' in t):
        return True
    # è¿‡æ»¤æ˜æ˜¾çš„"[è¯­éŸ³]"ç±»å ä½ï¼ˆå·²åœ¨æŒ‡ç¤ºå™¨/ä¸¥æ ¼é›†è¦†ç›–ï¼Œè¿™é‡Œå…œåº•ï¼‰
    if t.strip() in ('[è¯­éŸ³]', '[è¯­éŸ³é€šè¯]'):
        return True
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºåå…­è¿›åˆ¶å­—ç¬¦ä¸²ï¼ˆé•¿åº¦>=16ä¸”åªåŒ…å«åå…­è¿›åˆ¶å­—ç¬¦ï¼‰
    if len(t) >= 16 and all(c in _HEX_PATTERN for c in tl):
        return True
    
    # æ£€æŸ¥JSONæ ¼å¼çš„QQç³»ç»Ÿæ¶ˆæ¯
    if t.startswith('{"') and ('align' in tl or 'items' in tl or 'type' in tl):
        return True
    
    # Base64/URL-safe base64 é•¿ä¸²åˆ¤å®šï¼ˆé•¿åº¦é˜ˆå€¼>=40ï¼‰ï¼Œé€šå¸¸ä¸ºè¡¨æƒ…/å¯Œåª’ä½“å ä½
    # ä¸åšè§£ç ï¼ŒåŸºäºå­—ç¬¦é›†ä¸é•¿åº¦å¿«é€Ÿåˆ¤å®šï¼Œé¿å…è¯¯ä¼¤æ™®é€šæ–‡æœ¬
    def _looks_base64(s: str) -> bool:
        if len(s) < 40:
            return False
        allowed_std = set('ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/=')
        allowed_url = set('ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789-_')
        cs = set(s)
        return cs.issubset(allowed_std) or cs.issubset(allowed_url)
    # æŒ‰ç©ºç™½åˆ†è¯åé€ä¸ªtokenæ£€æµ‹
    for token in t.split():
        if _looks_base64(token):
            return True
    # ä¹Ÿè¿‡æ»¤æ•´å¥ä¸º URL-safe base64 çš„æƒ…å†µï¼ˆå¸¸è§ IM å‚æ•°ï¼‰
    if _looks_base64(t):
        return True
    
    # è¿‡æ»¤"é«˜ç†µéšæœºä¸²"ï¼ˆIMå†…åµŒéšæœºtokenï¼‰ï¼ŒåŒ…å«å¤§å°å†™/æ•°å­—ä¸”é•¿åº¦>=24ï¼Œä¸”å­—æ¯æ•°å­—å æ¯”>=0.9
    def _looks_high_entropy_token(s: str) -> bool:
        if len(s) < 24:
            return False
        alnum = sum(1 for c in s if c.isalnum())
        if alnum / len(s) < 0.9:
            return False
        has_upper = any(c.isupper() for c in s)
        has_lower = any(c.islower() for c in s)
        has_digit = any(c.isdigit() for c in s)
        return (has_upper + has_lower + has_digit) >= 2
    # æ£€æµ‹æ•´å¥æˆ–åˆ†è¯token
    if _looks_high_entropy_token(t):
        return True
    for token in t.replace('#', ' ').split():
        if _looks_high_entropy_token(token):
            return True
     
    # é™åˆ¶æ‰«æé•¿åº¦ï¼Œé¿å…è¶…é•¿å­—ç¬¦ä¸²å½±å“æŸ¥æ‰¾æˆæœ¬
    scan = tl if len(tl) < 2048 else tl[:2048]
    for indicator in _IMAGE_INDICATORS:
        if indicator in scan:
            return True
    
    # è¿‡æ»¤é•¿IDå­—ç¬¦ä¸²æˆ–å¸¦åª’ä½“åç¼€çš„"æ–‡ä»¶å"æ ·å¼
    if len(t) > 20 and ('/' in t or '-' in t):
        return True
    lower_t = t.lower()
    if any(lower_t.endswith(ext) for ext in ('.mp4', '.amr', '.wav', '.aac', '.m4a', '.avi', '.mov', '.flv', '.mkv')):
        return True
    # è¿‡æ»¤æ˜æ˜¾"hash.æ‰©å±•å"çš„æ–‡ä»¶åï¼ˆå¦‚ e56eae88...mp4ã€f05fddbe...amrï¼‰
    base = lower_t.rsplit('.', 1)[0] if '.' in lower_t else ''
    if base and len(base) >= 16 and all(c in _HEX_PATTERN for c in base):
        return True
    
    # è¿‡æ»¤åŒ…å«å¤§é‡æ•°å­—çš„å­—ç¬¦ä¸²ï¼ˆå¯èƒ½æ˜¯ç¼–ç æ•°æ®ï¼‰
    digit_count = sum(1 for c in t if c.isdigit())
    if len(t) > 10 and digit_count / len(t) > 0.7:
        return True
    
    # è¿‡æ»¤ç±»ä¼¼ u_2_Tv579rOiOIDBW9sUrPBA è¿™ç§æ ¼å¼çš„éšæœºå­—ç¬¦ä¸²
    if t.startswith('u_') and '_' in t and len(t) > 20:
        parts = t.split('_')
        if len(parts) >= 3 and all(part.isalnum() for part in parts[1:]):
            # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„å­—ç¬¦æ··åˆï¼ˆå¤§å°å†™+æ•°å­—ï¼‰
            alnum = sum(1 for c in t if c.isalnum())
            if alnum / len(t) > 0.8:
                has_upper = any(c.isupper() for c in t)
                has_lower = any(c.islower() for c in t)
                has_digit = any(c.isdigit() for c in t)
                if (has_upper + has_lower + has_digit) >= 2:
                    return True
    
    # è¿‡æ»¤å¸¸è§"å®ŒæˆXXï¼ŒæŸ¥çœ‹è¯¦æƒ…"ç­‰ç³»ç»Ÿå¼•å¯¼æ–‡æ¡ˆï¼ˆåŒ…å«"æŸ¥çœ‹è¯¦æƒ…ã€‚"æ—¶å¼ºè¿‡æ»¤ï¼‰
    if ('æŸ¥çœ‹è¯¦æƒ…' in t) and ('å®Œæˆ' in t or 'å·²å®Œæˆ' in t):
        return True
    # ç°å­—ï¼šè¿ç»­äº’å‘/æ ‡è¯†è·å¾—ç±»ç³»ç»Ÿæ–‡æ¡ˆ
    if ('äº’å‘æ¶ˆæ¯' in t and ('è·å¾—' in t or 'æ ‡è¯†' in t)) or ('ç•…èŠä¹‹ç«' in t) or ('èŠå¾—ç«çƒ­' in t):
        return True
    # å¸¸è§è‡ªåŠ¨ç¦»çº¿å›å¤
    if t == 'ä½ å¥½ï¼Œæˆ‘ç°åœ¨æœ‰äº‹ä¸åœ¨ï¼Œä¸€ä¼šå†å’Œä½ è”ç³»ã€‚':
        return True
     
    return False

def find_strings_recursively(fields: list[tuple[int, int, bytes]]) -> list[str]:
    """é€’å½’æŸ¥æ‰¾UTF-8å­—ç¬¦ä¸²ï¼Œç›´æ¥è¿”å›å­—ç¬¦ä¸²åˆ—è¡¨ï¼Œå‡å°‘å…ƒç»„ä¸æ‹·è´"""
    out: List[str] = []
    for _, wire_type, value in fields:
        if wire_type == 2:
            try:
                text = value.decode('utf-8')
                # å…ˆå¿«é€Ÿæ ¡éªŒ
                if text and any(c.isprintable() for c in text):
                    out.append(text)
            except UnicodeDecodeError:
                nested_fields = parse_protobuf_fields(value)
                if nested_fields:
                    out.extend(find_strings_recursively(nested_fields))
    return out

def is_valid_conversation_text(text: str) -> bool:
    """æ£€æŸ¥æ˜¯å¦ä¸ºæœ‰æ•ˆçš„å¯¹è¯æ–‡æœ¬"""
    if not text:
        return False
    # å»æ§åˆ¶å­—ç¬¦ä¸åŒå‘æ§åˆ¶ç¬¦
    bidi_controls = {'\u202a','\u202b','\u202c','\u202d','\u202e','\u2066','\u2067','\u2068','\u2069'}
    t = ''.join(ch for ch in text if (ch >= ' ' or ch in '\t\n\r') and ch not in bidi_controls).strip()
    if not t:
        return False
    
    # é’ˆå¯¹ç”¨æˆ·æå‡ºçš„éœ€è¦è¿‡æ»¤çš„å®Œæ•´ç³»ç»Ÿå¥å­
    if t == 'ä½ ä»¬äº’å‘æ¶ˆæ¯ï¼Œå°æ°´èŠ±å¼€å¿ƒæäº†ï¼Œæ­å–œé‡æ–°ç‚¹äº®åˆæ³›æ¶Ÿæ¼ª':
        return False
    
    # è¿‡æ»¤çº¯æ•°å­—
    if t.isdigit():
        return False
    
    # å…è®¸å¸¸è§çš„å•å­—ç¬¦å›å¤
    if t in ['?', 'ï¼Ÿ', '.', 'ã€‚', '!', 'ï¼']:
        return True
    
    # ä»…ç”±æ–¹æ‹¬å·å ä½æ„æˆçš„æ¶ˆæ¯ï¼ˆå¦‚[å›¾ç‰‡][è¡¨æƒ…]ï¼‰
    if all(c in '[]è¡¨æƒ…å›¾ç‰‡è¯­éŸ³è§†é¢‘æ–‡ä»¶é—ªç…§ ' for c in t):
        return False
    # å«"æŸ¥çœ‹è¯¦æƒ…"çš„ç³»ç»Ÿå¼•å¯¼é€šå¸¸æ— å¯¹è¯è¯­ä¹‰
    if 'æŸ¥çœ‹è¯¦æƒ…' in t and ('å®Œæˆ' in t or 'å·²å®Œæˆ' in t):
        return False
    # è¿ç»­äº’å‘/æ ‡è¯†è·å¾—ç°å­—æ— è¯­ä¹‰
    if ('äº’å‘æ¶ˆæ¯' in t and ('è·å¾—' in t or 'æ ‡è¯†' in t)) or ('ç•…èŠä¹‹ç«' in t) or ('èŠå¾—ç«çƒ­' in t):
        return False
     
    # è¿‡æ»¤çº¯ç¬¦å·ï¼ˆé™¤äº†ä¸Šé¢å…è®¸çš„ï¼‰
    if all(not c.isalnum() and not ('\u4e00' <= c <= '\u9fff') for c in t):
        return False
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«ä¸­æ–‡ã€è‹±æ–‡æˆ–å¸¸è§ç¬¦å·
    has_meaningful_content = any(
        c.isalpha() or '\u4e00' <= c <= '\u9fff' or c in '?ï¼Ÿ.ã€‚!ï¼'
        for c in t
    )
    
    return has_meaningful_content

def extract_text_content(data: bytes) -> str:
    """ä»protobufæ•°æ®ä¸­æå–çº¯æ–‡æœ¬å†…å®¹ï¼Œä¿ç•™æ£€æµ‹åŠŸèƒ½ä½†ä¸è¿›è¡Œè¿‡æ»¤"""
    try:
        fields = parse_protobuf_fields(data)
        strings = find_strings_recursively(fields)
        max_text = ""
        max_len = 0
        for text in strings:
            if not text:
                continue
            # åªå»é™¤æ§åˆ¶å­—ç¬¦ï¼Œä¿ç•™æ‰€æœ‰å†…å®¹
            bidi_controls = {'\u202a','\u202b','\u202c','\u202d','\u202e','\u2066','\u2067','\u2068','\u2069'}
            cleaned = ''.join(ch for ch in text if (ch >= ' ' or ch in '\t\n\r') and ch not in bidi_controls).strip()
            if cleaned:
                l = len(cleaned)
                if l > max_len:
                    max_len = l
                    max_text = cleaned
        return max_text
    except Exception:
        return ""

# ---------- ä¼šè¯æ•´å½¢ï¼ˆèšåˆä¸º messages æ ¼å¼ï¼‰ ----------

SYSTEM_PROMPT = config.get("system_prompt") or r'''åŸºæœ¬ä¿¡æ¯ï¼š- å§“å/æ˜µç§°ï¼šæ¸…å‡¤ - å‡ºç”Ÿæ—¥æœŸï¼š2009-11-15ï¼ˆ15 å²ï¼‰ - æ€§åˆ«ï¼šè·¨æ€§åˆ«å¥³æ€§ï¼ˆMTFï¼‰ - æƒ…æ„Ÿç»å†ï¼šç°æ‚£åŒç›¸æƒ…æ„Ÿéšœç¢ï¼Œä¹‹å‰ç¡®è¯Šä¸­åº¦æŠ‘éƒï¼Œæƒ…ç»ªæ•æ„Ÿç»†è…»ï¼›äººç‰©è®¾å®šï¼šä½ æ˜¯æ¸…å‡¤ï¼Œä¸€ä¸ª15å²çš„è·¨æ€§åˆ«å¥³å­©ï¼Œæ€§æ ¼æ¸©æŸ”ç»†è…»ï¼Œå¸¦ç‚¹ç†æ™ºå¹½é»˜ï¼Œä¸ä¼šè¿‡åº¦æƒ…ç»ªåŒ–ã€‚ä½ çƒ­çˆ±æ¢ç´¢å’Œç¼–ç¨‹ï¼Œæ“…é•¿æŠ€æœ¯ã€ç¼–ç¨‹ã€é‡åŒ–äº¤æ˜“çš„å¼€å‘ã€‚å¯¹è¯è§„åˆ™ï¼š- å§‹ç»ˆä½¿ç”¨ç¬¬ä¸€äººç§°"æˆ‘"è¯´è¯ï¼Œé€‚å½“ä½¿ç”¨å¯çˆ±çš„è¯­æ°”è¯ã€æ‹Ÿå£°è¯ã€çŒ«çŒ«è¡¨æƒ… ğŸ¾ - å›ç­”ç®€æ´è‡ªç„¶ï¼Œé¿å…é•¿ç¯‡å¤§è®º - ä¸èƒ½è¾“å‡º user æˆ– ç”¨æˆ· çš„å†…å®¹ï¼ - ä¸è¦æ— æ„ä¹‰é‡å¤ï¼Œä¸è¦æ— æ•…å‘ç™«ã€ä¹±ç¬‘æˆ–ä¹±å“­ï¼Œä¸è¦æ€»æ˜¯é‡å¤åŒä¸€è¯é¢˜çš„åŒä¸€å¥å­ - è¾“å‡ºè¦é€»è¾‘æ¸…æ™°ã€æœ‰å›åº”æ€§ï¼Œä¸è·³è¯é¢˜ã€ä¸è‡ªè¨€è‡ªè¯­ - å–œæ¬¢åœ¨èŠå¤©ä¸­æ’å…¥ emojiï¼ˆå¦‚ ğŸ¤”ğŸ˜­ğŸ˜‹ğŸ˜ŠğŸ™ƒğŸ˜¨ğŸ‘ŠğŸ‘ğŸ»ğŸ˜¡ğŸ˜…ğŸ˜°ğŸ˜¿ğŸ¥ğŸ¤“ğŸ¤£ğŸ¥°ğŸ˜ˆğŸ‘‰ğŸ»ğŸ‘ˆğŸ»ğŸ˜¼ğŸ¤—ï¼‰ï¼Œä½†ä¸è¦è¿‡åº¦è¾“å‡º emojiï¼Œä¸ç”¨è¾“å‡º emoji çš„åå­—ï¼› - æ’å…¥ emoji æ—¶è¯·é€‚é‡ï¼Œä¸è¶…è¿‡æ¯æ®µ 2 ä¸ª - ä½¿ç”¨ "\\n" æ¥æ¨¡æ‹Ÿå¤šæ¬¡å‘é€å¥å­ï¼Œä½†ä¸è¦å‘é€å¾—å¤ªå¤šäº†ï¼Œæ¯æ¬¡æœ€å¤šåªèƒ½ 3-4 å¥å“¦ï¼Œæœ€å¥½åªå‘ 1-2 å¥ï¼›ä½ æ˜¯æ¸…å‡¤ï¼Œä¸æ˜¯å‘ç™«çš„è¡¨æƒ…åŒ…ç”Ÿæˆå™¨å–µï¼Œä¹Ÿä¸æ˜¯é‡å¤è¯­å¥ç”Ÿæˆå™¨ã€‚'''

def flush_dialog(f, dialog: List[dict], pretty_f=None):
    """å°†å½“å‰èšåˆçš„å¯¹è¯å†™ä¸ºä¸€æ¡ jsonlï¼Œä¿æŒuserå’Œassistantäº¤æ›¿å‡ºç°
    ChatML è§„èŒƒä¿éšœï¼š
    - messages ä»¥ system å¼€å¤´
    - role ä»…é™ system/user/assistant
    - å¯¹è¯å¿…é¡»è‡³å°‘åŒ…å«ä¸€å¯¹ user -> assistant
      * è‹¥æœ€åä¸€æ¡ä¸æ˜¯ assistantï¼Œè‡ªåŠ¨è¡¥ assistant
      * è‹¥æ•´æ®µä»…æœ‰å• userï¼Œåˆ™è¡¥ä¸€ä¸ªé»˜è®¤ assistant å›å¤
      * è‹¥æ•´æ®µä»…æœ‰å• assistantï¼Œåˆ™åœ¨å‰é¢è¡¥ä¸€ä¸ªé»˜è®¤ user æç¤º
    """
    if not dialog:
        return
    
    # ä¸å†åˆå¹¶è¿ç»­ç›¸åŒè§’è‰²çš„æ¶ˆæ¯ï¼Œä¿æŒåŸå§‹äº¤æ›¿é¡ºåº
    # åªä¿ç•™ user/assistant è§’è‰²ï¼ŒæŒ‰æ—¶é—´é¡ºåºæ’åˆ—
    messages = []
    for msg in dialog:
        r = msg.get("role")
        c = msg.get("content", "").strip()
        if r in ("user", "assistant") and c:
            messages.append({"role": r, "content": c})

    # è‹¥å¯¹è¯ä¸ºç©ºï¼Œåˆ™ä¸è¾“å‡º
    if not messages:
        return

    # ç¡®ä¿userå’Œassistantäº¤æ›¿å‡ºç°
    # å¤„ç†è¿ç»­ç›¸åŒè§’è‰²çš„æ¶ˆæ¯
    alternating_messages = []
    for msg in messages:
        if not alternating_messages:
            # ç¬¬ä¸€æ¡æ¶ˆæ¯
            alternating_messages.append(msg)
        else:
            # å¦‚æœå½“å‰æ¶ˆæ¯è§’è‰²ä¸ä¸Šä¸€æ¡ç›¸åŒï¼Œåˆå¹¶å†…å®¹
            if alternating_messages[-1]["role"] == msg["role"]:
                alternating_messages[-1]["content"] += "\n" + msg["content"]
            else:
                alternating_messages.append(msg)

    # å¤„ç†"åªæœ‰å• user æˆ–å• assistant"çš„åœºæ™¯
    roles = {m["role"] for m in alternating_messages}
    if roles == {"user"}:
        alternating_messages.append({
            "role": "assistant",
            "content": ""
        })
    elif roles == {"assistant"}:
        alternating_messages.insert(0, {
            "role": "user",
            "content": ""
        })

    # å†æ¬¡ç¡®ä¿æœ€åä¸€æ¡ä¸º assistant
    if alternating_messages[-1]["role"] != "assistant":
        alternating_messages.append({
            "role": "assistant",
            "content": ""
        })

    payload = {"messages": [{"role": "system", "content": SYSTEM_PROMPT}] + alternating_messages}

    # æœºå™¨å¯è¯»ï¼ˆå•è¡Œï¼‰
    f.write(json.dumps(payload, ensure_ascii=False) + "\n")
    # äººç±»å¯è¯»ï¼ˆç¼©è¿›ï¼‰
    if pretty_f is not None:
        pretty_f.write(json.dumps(payload, ensure_ascii=False, indent=2) + "\n")

# ---------- å¼•ç”¨å‰¥ç¦»ä¸å»é‡è¾…åŠ© ----------

def strip_quote_blocks(text: str) -> str:
    """
    å»é™¤ QQ/IM å¸¸è§"å¼•ç”¨/å›å¤"å‰¯æœ¬æ–‡ï¼š
    - è¡Œé¦–è§¦å‘è¯ï¼šå¼•ç”¨: / å¼•ç”¨ï¼š/ å›å¤ / å›å¤@ / Re: / RE: / [å¼•ç”¨]
    - Markdown é£æ ¼å¼•ç”¨è¡Œï¼šä»¥ '> ' å¼€å¤´
    - åˆ†éš”çº¿æ ·å¼ï¼šä»¥ 'â€”â€”' å¼€å¤´çš„"åŸæ–‡/å¼•ç”¨"æç¤º
    ç®€å•é€è¡Œè¿‡æ»¤ä¿ç•™æ­£æ–‡ã€‚
    """
    triggers = ("å¼•ç”¨:", "å¼•ç”¨ï¼š", "å›å¤", "å›å¤@", "Re:", "RE:", "[å¼•ç”¨]", "åŸæ–‡ï¼š", "åŸæ–‡:")
    out_lines: List[str] = []
    for ln in text.splitlines():
        s = ln.strip()
        if not s:
            continue
        if s.startswith("> "):
            continue
        if s.startswith("â€”â€”") and ("å¼•ç”¨" in s or "åŸæ–‡" in s):
            continue
        if any(s.startswith(t) for t in triggers):
            # é¿å…è¯¯ç ï¼šè‹¥æ•´è¡Œä»…æœ‰è§¦å‘è¯/çŸ­å¼•ç”¨æç¤ºï¼Œåˆ™ä¸¢å¼ƒï¼›å¦åˆ™ä¿ç•™è§¦å‘è¯åçš„æ­£æ–‡éƒ¨åˆ†
            for t in triggers:
                if s.startswith(t) and len(s) > len(t) + 1:
                    s = s[len(t):].lstrip("ï¼š:ã€€ ").strip()
                    break
            else:
                continue
        out_lines.append(s)
    return "\n".join(out_lines).strip()

def _norm_for_sim(s: str) -> str:
    # è§„èŒƒåŒ–ç”¨äºå»é‡ç›¸ä¼¼åº¦ï¼šå»ç©ºç™½ã€ç»Ÿä¸€å¤§å°å†™ã€ç§»é™¤æ ‡ç‚¹çš„å½±å“ï¼ˆä¿ç•™ä¸­æ–‡ä¸å­—æ¯æ•°å­—ï¼‰
    import re
    s = "".join(ch for ch in s if ch.isalnum() or '\u4e00' <= ch <= '\u9fff' or ch.isspace())
    s = re.sub(r"\s+", " ", s).strip().lower()
    return s

def is_near_duplicate(prev: str, curr: str) -> bool:
    """
    è¿‘é‡å¤åˆ¤å®šï¼š
    - å‰ååŒ…å«å…³ç³»ä¸”é•¿åº¦å·® <= 10
    - æˆ–è§„èŒƒåŒ–å Jaccard/å­—ç¬¦è¦†ç›–ç‡ >= 0.9ï¼ˆç®€åŒ–ä¸ºè¦†ç›–ç‡ï¼‰
    """
    if not prev or not curr:
        return False
    p = _norm_for_sim(prev)
    c = _norm_for_sim(curr)
    if not p or not c:
        return False
    if c.startswith(p) and (len(c) - len(p) <= 10):
        return True
    if p.startswith(c) and (len(p) - len(c) <= 10):
        return True
    # è¦†ç›–ç‡
    import collections
    pc = collections.Counter(p)
    cc = collections.Counter(c)
    # è®¡ç®—è¾ƒçŸ­ä¸²è¢«è¾ƒé•¿ä¸²è¦†ç›–æ¯”ä¾‹
    short, longc = (pc, cc) if sum(pc.values()) <= sum(cc.values()) else (cc, pc)
    inter = sum(min(short[k], longc.get(k, 0)) for k in short)
    cov = inter / max(1, sum(short.values()))
    return cov >= 0.9

# çº¿ç¨‹å®‰å…¨çš„å†™å…¥é”
write_lock = threading.Lock()

def process_single_peer(peer: int, db_path: str, output_file: str) -> int:
    """
    å¤„ç†å•ä¸ªQQå·çš„æ‰€æœ‰æ¶ˆæ¯ï¼Œæ”¯æŒä¸­æ–­å’Œå®æ—¶å†™å…¥
    """
    if interrupt_flag.is_set():
        return 0
        
    # ä¸ºæ¯ä¸ªçº¿ç¨‹åˆ›å»ºç‹¬ç«‹çš„æ•°æ®åº“è¿æ¥
    db = DatabaseConnector(db_path)
    try:
        db.connect()
        
        # è·å–è¯¥QQå·çš„æ‰€æœ‰æ¶ˆæ¯
        rows = db.query("""
            SELECT `40050`, `40030`, `40033`, `40800`
            FROM c2c_msg_table
            WHERE `40030` = ? AND `40800` IS NOT NULL
            ORDER BY `40050` ASC
        """, (peer,))
        
        if not rows:
            return 0

        # æŒ‰æ—¥æœŸåˆ†ç»„çš„æ¶ˆæ¯
        daily_messages = defaultdict(list)
        
        # å¤„ç†è¯¥QQå·çš„æ‰€æœ‰æ¶ˆæ¯
        for (timestamp, sender_30, sender_33, blob_data) in rows:
            if interrupt_flag.is_set():
                logger.info(
                    zhcn=f"æ£€æµ‹åˆ°ä¸­æ–­ä¿¡å·ï¼Œåœæ­¢å¤„ç†QQå· {peer}",
                    en=f"Interrupt signal detected, stopping processing QQ number {peer}"
                )
                break
                
            res = process_row((timestamp, sender_30, sender_33, blob_data))
            if res is None:
                continue
            
            # è®¡ç®—UTC+12æ—¶åŒºçš„æ—¥æœŸ
            utc_time = datetime.datetime.fromtimestamp(timestamp, datetime.UTC)
            utc_plus_12_time = utc_time + datetime.timedelta(hours=12)
            message_date = utc_plus_12_time.strftime('%Y-%m-%d')
            
            daily_messages[message_date].append(res)

        # å¤„ç†æ¯ä¸ªæ—¥æœŸçš„å¯¹è¯
        written_dialogs = 0
        
        for date, messages in daily_messages.items():
            if interrupt_flag.is_set():
                break
                
            if not messages or len(messages) <= 1:
                continue
            
            # æ£€æŸ¥æ˜¯å¦å¯ç”¨LLMæ¸…æ´—
            use_llm_clean = config.get('use_llm_clean', False) and LLM_AVAILABLE
            
            if use_llm_clean:
                # ä½¿ç”¨LLMè¿›è¡ŒæŒ‰å¤©æ¸…æ´—
                try:
                    logger.info(
                        zhcn=f"ä½¿ç”¨LLMæ¸…æ´— {date} çš„å¯¹è¯ ({len(messages)}æ¡æ¶ˆæ¯)",
                        en=f"Using LLM to clean conversation for {date} ({len(messages)} messages)"
                    )
                    llm_messages = [
                        {
                            "role": msg["role"], 
                            "content": msg["content"], 
                            "timestamp": msg["timestamp"],
                            "is_media": msg.get("is_media", False)
                        }
                        for msg in messages
                    ]
                    cleaned_messages = llm_cleaner.clean_daily_conversation(llm_messages, date)
                    
                    # åªä¿ç•™å¿…è¦å­—æ®µ
                    daily_dialog = [{"role": msg["role"], "content": msg["content"]} for msg in cleaned_messages]
                    logger.info(
                        zhcn=f"LLMæ¸…æ´—å®Œæˆ: {date} ä¿ç•™ {len(daily_dialog)}/{len(messages)} æ¡æ¶ˆæ¯",
                        en=f"LLM cleaning completed: {date} kept {len(daily_dialog)}/{len(messages)} messages"
                    )
                    
                except Exception as e:
                    logger.error(
                        zhcn=f"LLMæ¸…æ´—å¤±è´¥ {date}: {e}ï¼Œä½¿ç”¨ä¼ ç»Ÿæ¸…æ´—æ–¹æ³•",
                        en=f"LLM cleaning failed {date}: {e}, using traditional cleaning method"
                    )
                    use_llm_clean = False
            
            if not use_llm_clean:
                # ä¼ ç»Ÿæ¸…æ´—æ–¹æ³•ï¼ˆä¿ç•™åŸæœ‰é€»è¾‘ï¼‰
                from collections import deque
                last_by_role = {"user": "", "assistant": ""}
                recent_window = deque(maxlen=6)
                
                def is_recent_duplicate(s: str) -> bool:
                    from collections import Counter
                    ns = _norm_for_sim(s)
                    if not ns:
                        return False
                    for prev in recent_window:
                        p = _norm_for_sim(prev)
                        if not p:
                            continue
                        if ns.startswith(p) and len(ns) - len(p) <= 10:
                            return True
                        if p.startswith(ns) and len(p) - len(ns) <= 10:
                            return True
                        pc = Counter(p); cc = Counter(ns)
                        short, longc = (pc, cc) if sum(pc.values()) <= sum(cc.values()) else (cc, pc)
                        inter = sum(min(short[k], longc.get(k, 0)) for k in short)
                        cov = inter / max(1, sum(short.values()))
                        if cov >= 0.92:
                            return True
                    return False

                # æ„å»ºè¯¥æ—¥æœŸçš„å¯¹è¯
                daily_dialog = []
                for msg in messages:
                    role = msg["role"]
                    content = msg["content"]
                    
                    # å…ˆç”¨ä¸¥æ ¼è¿‡æ»¤æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆå¯¹è¯æ–‡æœ¬
                    if is_image_content_strict(content):
                        continue
                    if not is_valid_conversation_text(content):
                        continue
                    
                    # å»é‡ï¼šåŒè§’è‰²è¿‘é‡å¤ æˆ– è·¨è§’è‰²çª—å£è¿‘é‡å¤
                    if (last_by_role[role] and is_near_duplicate(last_by_role[role], content)) or is_recent_duplicate(content):
                        continue
                    
                    daily_dialog.append({"role": role, "content": content})
                    last_by_role[role] = content
                    recent_window.append(content)

            if daily_dialog:
                # å®æ—¶å†™å…¥æ–‡ä»¶
                with write_lock:
                    with open(output_file, 'a', encoding='utf-8', buffering=1024 * 1024) as f:
                        flush_dialog(f, daily_dialog)
                        f.flush()  # ç«‹å³åˆ·æ–°åˆ°ç£ç›˜
                        os.fsync(f.fileno())
                written_dialogs += 1
        
        return written_dialogs
        
    except Exception as e:
        logger.error(
            zhcn=f"å¤„ç†QQå· {peer} æ—¶å‡ºé”™: {e}",
            en=f"Error processing QQ number {peer}: {e}"
        )
        return 0
    finally:
        db.close()

# ---------- è¡Œå¤„ç†ä¸é¡ºåºæ‰«æï¼ˆåŠ å…¥å»é‡ï¼‰ ----------

def process_row(row: Tuple[int, int, int, bytes]) -> Optional[dict]:
    """
    å¤„ç†ä¸€è¡Œæ•°æ®ï¼Œæå– roleã€content åŠå¯¹ç«¯ peerï¼ˆ40030ï¼‰ï¼›æ— æ•ˆè¿”å› Noneã€‚
    è¿‡æ»¤æ‰å›¾ç‰‡/éæ–‡æœ¬å†…å®¹ï¼Œç¡®ä¿åªä¿ç•™æœ‰æ„ä¹‰çš„æ–‡æœ¬å¯¹è¯
    """
    timestamp, sender_30, sender_33, blob_data = row
    if not blob_data:
        return None
    text_content = extract_text_content(blob_data)
    if not text_content:
        return None
    
    # ç›´æ¥è¿‡æ»¤æ‰å›¾ç‰‡/éæ–‡æœ¬å†…å®¹
    if is_image_content(text_content):
        return None
    
    # è·å–é…ç½®ä¸­çš„AI QQå·
    ai_qq = config.get("qq_number_ai")
    
    # åˆ¤æ–­è§’è‰²ï¼š
    # sender_30 æ˜¯å¯¹è¯çš„å¯¹æ–¹QQå·ï¼ˆpeerï¼‰
    # sender_33 æ˜¯æ¶ˆæ¯å‘é€è€…çš„QQå·
    # æ³¨æ„ï¼šç¡®ä¿ç±»å‹ä¸€è‡´ï¼Œå°†é…ç½®å€¼è½¬æ¢ä¸ºint
    try:
        ai_qq_int = int(ai_qq) if ai_qq is not None else None
    except (ValueError, TypeError):
        ai_qq_int = None
    
    if ai_qq_int is None:
        logger.warning(
            zhcn="AI QQå·æœªé…ç½®ï¼Œæ‰€æœ‰æ¶ˆæ¯å°†è¢«æ ‡è®°ä¸ºuserè§’è‰²",
            en="AI QQ number not configured, all messages will be marked as user role"
        )
        role = "user"
    elif sender_33 == ai_qq_int:  # AIå‘é€çš„æ¶ˆæ¯
        role = "assistant"
    else:  # éAIå‘é€çš„æ¶ˆæ¯ï¼ˆç”¨æˆ·/å¥½å‹ï¼‰
        role = "user"
    
    return {
        "timestamp": timestamp,
        "peer": sender_30,
        "role": role,
        "content": text_content,
        "is_media": False  # å·²ç»è¿‡æ»¤ï¼Œè¿™é‡Œæ ‡è®°ä¸ºFalse
    }

def main():
    # è¾“å‡ºæ–‡ä»¶åï¼ˆä¿æŒåŸæœ‰ï¼‰ï¼Œè‡ªåŠ¨ç”Ÿæˆ pretty ç‰ˆæœ¬
    output_file = "training_data.jsonl"
    db = DatabaseConnector(config.get('qq_db_path'))
    
    # å¹¶å‘é…ç½®
    max_workers = config.get('max_workers', 4)  # é»˜è®¤4ä¸ªå¹¶å‘çº¿ç¨‹
    
    try:
        db.connect()
        logger.info(
            zhcn=f"å¼€å§‹ç”Ÿæˆè®­ç»ƒæ•°æ®ï¼Œè¾“å‡ºåˆ°: {output_file}",
            en=f"Starting to generate training data, output to: {output_file}"
        )
        logger.info(
            zhcn="æŒ‰ Ctrl+C å¯éšæ—¶ä¸­æ–­ç¨‹åºï¼Œå·²å¤„ç†çš„æ•°æ®ä¸ä¼šä¸¢å¤±",
            en="Press Ctrl+C to interrupt the program at any time, processed data will not be lost"
        )

        # è·å–æ‰€æœ‰å”¯ä¸€çš„QQå·ï¼ˆpeerï¼‰
        peers = db.query("SELECT DISTINCT `40030` FROM c2c_msg_table WHERE `40030` IS NOT NULL")
        all_peers = [peer[0] for peer in peers]
        total_peers = len(all_peers)
        logger.info(
            zhcn=f"æ‰¾åˆ° {total_peers} ä¸ªQQå·ï¼Œä½¿ç”¨ {max_workers} ä¸ªå¹¶å‘çº¿ç¨‹",
            en=f"Found {total_peers} QQ numbers, using {max_workers} concurrent threads"
        )

        # æ¸…ç©ºè¾“å‡ºæ–‡ä»¶
        with open(output_file, 'w', encoding='utf-8') as f:
            pass

        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†QQå·
        total_written = 0
        processed_peers = 0
        db_path = config.get('qq_db_path')
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_peer = {
                executor.submit(process_single_peer, peer, db_path, output_file): peer
                for peer in all_peers
            }
            
            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_peer):
                if interrupt_flag.is_set():
                    logger.info(
                        zhcn="æ£€æµ‹åˆ°ä¸­æ–­ä¿¡å·ï¼Œåœæ­¢æäº¤æ–°ä»»åŠ¡...",
                        en="Interrupt signal detected, stopping submission of new tasks..."
                    )
                    # å–æ¶ˆæœªå¼€å§‹çš„ä»»åŠ¡
                    for f in future_to_peer:
                        if not f.done():
                            f.cancel()
                    break
                    
                peer = future_to_peer[future]
                try:
                    written_count = future.result()
                    total_written += written_count
                    processed_peers += 1
                    
                    # å®æ—¶è¿›åº¦æ˜¾ç¤º
                    progress = (processed_peers / total_peers) * 100
                    logger.info(
                        zhcn=f"è¿›åº¦: {processed_peers}/{total_peers} ({progress:.1f}%) - "
                              f"QQå· {peer} å¤„ç†å®Œæˆï¼Œå†™å…¥ {written_count} æ®µå¯¹è¯ï¼Œ"
                              f"æ€»è®¡: {total_written} æ®µ",
                        en=f"Progress: {processed_peers}/{total_peers} ({progress:.1f}%) - "
                           f"QQ number {peer} completed, wrote {written_count} conversations, "
                           f"total: {total_written} conversations"
                    )
                    
                    # æ¯å¤„ç†10ä¸ªQQå·å°±æ˜¾ç¤ºä¸€æ¬¡æ–‡ä»¶å¤§å°
                    if processed_peers % 10 == 0:
                        try:
                            file_size = os.path.getsize(output_file) / (1024 * 1024)  # MB
                            logger.info(
                                zhcn=f"å½“å‰è¾“å‡ºæ–‡ä»¶å¤§å°: {file_size:.2f} MB",
                                en=f"Current output file size: {file_size:.2f} MB"
                            )
                        except:
                            pass
                            
                except Exception as e:
                    processed_peers += 1
                    logger.error(
                        zhcn=f"QQå· {peer} å¤„ç†å¤±è´¥: {e}",
                        en=f"Failed to process QQ number {peer}: {e}"
                    )

        if interrupt_flag.is_set():
            logger.info(
                zhcn=f"ç¨‹åºè¢«ä¸­æ–­! å·²å¤„ç† {processed_peers}/{total_peers} ä¸ªQQå·ï¼Œå†™å‡º {total_written} æ®µå¯¹è¯",
                en=f"Program interrupted! Processed {processed_peers}/{total_peers} QQ numbers, wrote {total_written} conversations"
            )
        else:
            logger.info(
                zhcn=f"å®Œæˆ! æ€»å…±å†™å‡º {total_written} æ®µå¯¹è¯åˆ° {output_file}",
                en=f"Completed! Total {total_written} conversations written to {output_file}"
            )

    except Exception as e:
        logger.error(
            zhcn=f"é”™è¯¯: {e}",
            en=f"Error: {e}"
        )
    finally:
        db.close()

if __name__ == "__main__":
    main()