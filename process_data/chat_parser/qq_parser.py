#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import sqlite3
import csv
import os
import pandas as pd
import re
from typing import Optional, List, Dict, Tuple, Any
from datetime import datetime
from pathlib import Path
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„ä»¥å¯¼å…¥é¡¹ç›®æ¨¡å—
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))

from config.config import get_config
from logger.logger import get_logger
from database.db_connector import DatabaseConnector

config = get_config()
logger = get_logger('QQParser')

class QQParser:
    """
    QQ èŠå¤©æ•°æ®è§£æå™¨
    å°† QQ SQLite æ•°æ®åº“æ ¼å¼è½¬æ¢ä¸ºç»Ÿä¸€çš„ CSV æ ¼å¼
    """
    
    def __init__(self, db_path: str, output_dir: str = "./dataset/csv/"):
        """
        åˆå§‹åŒ– QQ è§£æå™¨
        
        Args:
            db_path: QQ æ•°æ®åº“æ–‡ä»¶è·¯å¾„
            output_dir: CSV è¾“å‡ºç›®å½•
        """
        self.db_path = db_path
        self.output_dir = Path(output_dir)
        self.db_connector = DatabaseConnector(db_path)
        # å¢å¼ºè¿‡æ»¤å™¨çš„å·²çŸ¥ä¹±ç æ¨¡å¼åº“
        self.known_garbage_patterns = [
            'tAB>b)Z)L',
            'c/PØ®ğ…ŸXo',  
            'Ú“~!ufX1L',
            'Rd;kj\ncd',
            'HR=    Î¯0\\5O',
            'E|gQf',
        ]
        
        # æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼
        self.regex_patterns = [
            r'^[A-Z]{1,3}[><!|=~]{1,3}',  # å¤§å†™å­—æ¯+ç‰¹æ®Šç¬¦å·å¼€å¤´
            r'[)><!;=]{2,}',              # è¿ç»­ç‰¹æ®Šç¬¦å·
            r'[A-Za-z]{1,3}[^\w\s\u4e00-\u9fff]{2,}',  # å­—æ¯åè·Ÿå¤šä¸ªç‰¹æ®Šç¬¦å·
            r'^[A-Z][a-z]?[^\w\s]+',      # å•ä¸ªå¤§å†™å­—æ¯å¼€å¤´çš„ä¹±ç 
            r'[\\][0-9]',                 # åæ–œæ +æ•°å­—
        ]
        
        # Unicodeå­—ç¬¦èŒƒå›´æ£€æŸ¥
        self.problematic_unicode_ranges = [
            (0x0600, 0x06FF),  # é˜¿æ‹‰ä¼¯æ–‡
            (0x0400, 0x04FF),  # è¥¿é‡Œå°”æ–‡  
            (0x0370, 0x03FF),  # å¸Œè…Šæ–‡
            (0xE000, 0xF8FF),  # ç§ç”¨åŒº
        ]
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # æ¶ˆæ¯ç±»å‹æ˜ å°„
        self.message_types = {
            0: "text",
            1: "image", 
            2: "voice",
            3: "video",
            4: "file",
            5: "location",
            6: "link",
            7: "system",
            8: "sticker",
            9: "audio",
            10: "other"
        }
    
    def decode_varint(self, data: bytes, pos: int = 0) -> Tuple[int, int]:
        """ä»æŒ‡å®šä½ç½®è§£ç ä¸€ä¸ª Varint"""
        result = 0
        shift = 0
        original_pos = pos
        while True:
            if pos >= len(data):
                raise IndexError(f"Varintè§£ç å¤±è´¥: åœ¨ä½ç½®{original_pos}æ„å¤–ç»“æŸ")
            b = data[pos]
            result |= (b & 0x7F) << shift
            pos += 1
            if not (b & 0x80):
                break
            shift += 7
            if shift >= 64:
                raise ValueError("Varintè¿‡é•¿æˆ–æ— æ•ˆ")
        return result, pos
    
    def parse_protobuf_fields(self, data: bytes) -> List[Tuple[int, int, bytes]]:
        """è§£æprotobufå­—æ®µ"""
        pos = 0
        fields = []
        while pos < len(data):
            try:
                tag, pos = self.decode_varint(data, pos)
                field_num = tag >> 3
                wire_type = tag & 0x07
                
                if wire_type == 0:  # varint
                    value, pos = self.decode_varint(data, pos)
                    fields.append((field_num, wire_type, value.to_bytes((value.bit_length() + 7) // 8, 'little')))
                elif wire_type == 1:  # 64-bit
                    if pos + 8 > len(data):
                        break
                    value_bytes = data[pos:pos+8]
                    pos += 8
                    fields.append((field_num, wire_type, value_bytes))
                elif wire_type == 2:  # length-delimited
                    length, pos = self.decode_varint(data, pos)
                    if pos + length > len(data):
                        break
                    value_bytes = data[pos:pos+length]
                    pos += length
                    fields.append((field_num, wire_type, value_bytes))
                elif wire_type == 5:  # 32-bit
                    if pos + 4 > len(data):
                        break
                    value_bytes = data[pos:pos+4]
                    pos += 4
                    fields.append((field_num, wire_type, value_bytes))
                else:
                    break
            except (IndexError, ValueError):
                break
        return fields
    
    def find_strings_recursively(self, fields: List[Tuple[int, int, bytes]]) -> List[str]:
        """é€’å½’æŸ¥æ‰¾å­—ç¬¦ä¸²"""
        strings = []
        for field_num, wire_type, data in fields:
            if wire_type == 2:  # length-delimited
                try:
                    text = data.decode('utf-8', errors='ignore')
                    if text and self.is_valid_text(text):
                        strings.append(text)
                except UnicodeDecodeError:
                    pass
                
                # é€’å½’è§£æåµŒå¥—å­—æ®µ
                try:
                    nested_fields = self.parse_protobuf_fields(data)
                    strings.extend(self.find_strings_recursively(nested_fields))
                except:
                    pass
        return strings
    
    def contains_sensitive_info(self, text: str) -> bool:
        """æ£€æµ‹æ˜¯å¦åŒ…å«æ•æ„Ÿä¿¡æ¯éœ€è¦è¿‡æ»¤"""
        if not text:
            return False
        
        sensitive_patterns = [
            # ç”¨æˆ·åå’Œè·¯å¾„
            r'Users\\\w+',  # Windowsç”¨æˆ·å
            r'C:\\Users',     # ç³»ç»Ÿè·¯å¾„
            # QQç›¸å…³è·¯å¾„
            r'Tencent Files',
            r'NTOS.*::', 
            # ç”µè¯å·ç æ¨¡å¼
            r'\b1[3-9]\d{9}\b',
            # èº«ä»½è¯å·
            r'\b\d{17}[\dX]\b',
            # IPåœ°å€
            r'\b(?:\d{1,3}\.){3}\d{1,3}\b'
        ]
        
        import re
        return any(re.search(pattern, text) for pattern in sensitive_patterns)
    
    def contains_garbled_chars(self, text: str) -> bool:
        """æ£€æµ‹æ˜¯å¦åŒ…å«ä¹±ç å­—ç¬¦"""
        if not text:
            return False
        
        # å¸¸è§çš„ä¹±ç å­—ç¬¦æ¨¡å¼
        garbled_patterns = [
            # éå¸¸ç”¨Unicodeå­—ç¬¦
            lambda c: 0x0080 <= ord(c) <= 0x00FF and c not in 'Â Â¡Â¢Â£Â¤Â¥Â¦Â§Â¨Â©ÂªÂ«Â¬Â­Â®Â¯',
            # æ§åˆ¶å­—ç¬¦
            lambda c: 0x0000 <= ord(c) <= 0x001F and c not in '\t\n\r',
            # é«˜UnicodeåŒºé—´çš„ç‰¹æ®Šå­—ç¬¦
            lambda c: ord(c) > 0xFFFF,
            # ç§ç”¨åŒºå­—ç¬¦
            lambda c: 0xE000 <= ord(c) <= 0xF8FF,
        ]
        
        garbled_count = sum(1 for c in text for pattern in garbled_patterns if pattern(c))
        
        # å¦‚æœä¹±ç å­—ç¬¦æ¯”ä¾‹å¤ªé«˜ï¼Œè®¤ä¸ºæ˜¯ä¹±ç æ–‡æœ¬
        if len(text) > 0 and garbled_count / len(text) > 0.2:
            return True
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å¤ªå¤šéå¯æ‰“å°å­—ç¬¦
        non_printable = sum(1 for c in text if not c.isprintable())
        if len(text) > 0 and non_printable / len(text) > 0.3:
            return True
        
        return False
    
    def is_meaningful_text(self, text: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºæœ‰æ„ä¹‰çš„æ–‡æœ¬å†…å®¹"""
        if not text or len(text.strip()) < 1:
            return False
        
        text = text.strip()
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«ä¹±ç å­—ç¬¦
        if self.contains_garbled_chars(text):
            return False
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«æœ‰æ„ä¹‰çš„å­—ç¬¦ï¼ˆä¸­æ–‡ã€è‹±æ–‡å­—æ¯ã€æ•°å­—ï¼‰
        meaningful_chars = sum(1 for c in text if c.isalpha() or c.isdigit() or 'ä¸€' <= c <= 'é¿¿')
        if meaningful_chars == 0:
            return False
        
        # æ£€æŸ¥å¯è¯»æ€§ï¼šæœ‰æ„ä¹‰å­—ç¬¦å æ¯”ä¸èƒ½å¤ªä½
        total_chars = len(text)
        if total_chars > 0 and meaningful_chars / total_chars < 0.5:  # æé«˜é˜ˆå€¼
            return False
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºå¸¸è§çš„æœ‰æ„ä¹‰è¯è¯­
        # å¦‚æœåŒ…å«å¸¸ç”¨è¯æ±‡ï¼Œè®¤ä¸ºæ˜¯æœ‰æ„ä¹‰çš„
        common_words = [
            # ä¸­æ–‡å¸¸ç”¨è¯
            'ä½ ', 'æˆ‘', 'ä»–', 'å¥¹', 'æ˜¯', 'ä¸', 'æœ‰', 'çš„', 'äº†', 'åœ¨', 'éƒ½', 'å¯ä»¥', 'ä¹Ÿ',
            'æ€ä¹ˆ', 'ä»€ä¹ˆ', 'ä¸ºä»€ä¹ˆ', 'å¯¹', 'å¥½', 'è¡Œ', 'çš„ç¡®', 'çœŸå‡', 'è¿™', 'é‚£',
            'å¤ª', 'ç‚¹', 'äº†', 'çŒœ', 'éƒ½', 'å¿ƒåŠ¨', 'å †æ–™', 'çŒ¿',
            # è‹±æ–‡å¸¸ç”¨è¯
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with',
            'how', 'what', 'why', 'when', 'where', 'who', 'yes', 'no', 'ok', 'good', 'bad',
            # å“ç‰Œåç§°
            'Neo', 'iPhone', 'Android', 'Windows', 'Mac'
        ]
        
        text_lower = text.lower()
        contains_common_word = any(word.lower() in text_lower for word in common_words)
        
        # å¦‚æœåŒ…å«å¸¸ç”¨è¯æ±‡ï¼Œåˆ™è®¤ä¸ºæ˜¯æœ‰æ„ä¹‰çš„
        if contains_common_word:
            return True
        
        # å¦åˆ™éœ€è¦æ›´ä¸¥æ ¼çš„éªŒè¯
        # è‡³å°‘åŒ…å«3ä¸ªæœ‰æ„ä¹‰å­—ç¬¦ï¼Œä¸”ä¸æ˜¯çº¯æ•°å­—æˆ–ç¬¦å·
        if meaningful_chars >= 3 and not text.replace(' ', '').isdigit():
            # è¿›ä¸€æ­¥æ£€æŸ¥ï¼šä¸èƒ½å…¨éƒ¨æ˜¯ç‰¹æ®Šå­—ç¬¦
            special_chars = sum(1 for c in text if not c.isalnum() and not c.isspace())
            return special_chars / len(text) < 0.5
        
        return False
    
    def is_valid_text(self, text: str) -> bool:
        """éªŒè¯æ–‡æœ¬æ˜¯å¦æœ‰æ•ˆ"""
        if not text or len(text) < 1:
            return False
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºç©ºå­—ç¬¦ä¸²æˆ–undefined
        text_clean = text.strip().lower()
        if not text_clean or text_clean in ['undefined', 'null', 'none', '']:
            return False
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºåª’ä½“å†…å®¹
        if self.is_media_content(text):
            return False
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«æ•æ„Ÿä¿¡æ¯
        if self.contains_sensitive_info(text):
            return False
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºæœ‰æ„ä¹‰çš„æ–‡æœ¬
        if not self.is_meaningful_text(text):
            return False
            
        return True
    
    def clean_text_content(self, text: str) -> str:
        """æ¸…ç†æ–‡æœ¬å†…å®¹ï¼Œå»é™¤ä¹±ç å’Œä¸å¯è§å­—ç¬¦"""
        if not text:
            return ""
        
        # å»é™¤æ§åˆ¶å­—ç¬¦å’ŒåŒå‘æ–‡æœ¬æ§åˆ¶ç¬¦
        bidi_controls = {'\u202a','\u202b','\u202c','\u202d','\u202e','\u2066','\u2067','\u2068','\u2069'}
        control_chars = set(chr(i) for i in range(32)) - {'\t', '\n', '\r'}
        unwanted_chars = bidi_controls | control_chars
        
        # æ¸…ç†å­—ç¬¦
        cleaned = ''.join(ch for ch in text if ch not in unwanted_chars)
        
        # å»é™¤å¼€å¤´å’Œç»“å°¾çš„éå­—æ¯æ•°å­—å­—ç¬¦ï¼ˆä½†ä¿ç•™ä¸­æ–‡ç­‰ï¼‰
        cleaned = cleaned.strip()
        
        # å¦‚æœå¼€å¤´æˆ–ç»“å°¾æœ‰æ˜æ˜¾çš„ä¹±ç æ¨¡å¼ï¼Œå°è¯•ç§»é™¤
        while cleaned and not (cleaned[0].isalnum() or ord(cleaned[0]) > 127):
            cleaned = cleaned[1:]
        
        while cleaned and not (cleaned[-1].isalnum() or ord(cleaned[-1]) > 127 or cleaned[-1] in '.!?ã€‚ï¼ï¼Ÿ,ï¼Œ;:ï¼›ï¼š'):
            cleaned = cleaned[:-1]
        
        return cleaned.strip()
    
    def calculate_text_quality(self, text: str) -> float:
        """è®¡ç®—æ–‡æœ¬è´¨é‡è¯„åˆ†"""
        if not text:
            return 0.0
        
        score = 0.0
        total_chars = len(text)
        
        # ä¸­æ–‡å­—ç¬¦åŠ åˆ†
        chinese_chars = sum(1 for c in text if '\u4e00' <= c <= '\u9fff')
        score += (chinese_chars / total_chars) * 2.0
        
        # è‹±æ–‡å­—æ¯åŠ åˆ†
        ascii_letters = sum(1 for c in text if c.isalpha() and ord(c) < 128)
        score += (ascii_letters / total_chars) * 1.5
        
        # æ•°å­—é€‚é‡åŠ åˆ†
        digits = sum(1 for c in text if c.isdigit())
        score += min((digits / total_chars) * 0.5, 0.3)
        
        # æ ‡ç‚¹ç¬¦å·é€‚é‡åŠ åˆ†
        punctuation = sum(1 for c in text if c in '.,!?;:ã€‚ï¼Œï¼ï¼Ÿï¼›ï¼š')
        score += min((punctuation / total_chars) * 0.3, 0.2)
        
        # é•¿åº¦å¥–åŠ±ï¼ˆä½†ä¸è¦å¤ªé•¿ï¼‰
        if 5 <= total_chars <= 200:
            score += 0.5
        elif total_chars > 200:
            score += 0.2
        
        return score
    
    def extract_text_content(self, data: bytes) -> str:
        """ä»protobufæ•°æ®ä¸­æå–çº¯æ–‡æœ¬å†…å®¹"""
        try:
            fields = self.parse_protobuf_fields(data)
            strings = self.find_strings_recursively(fields)
            
            # æŸ¥æ‰¾æœ€åˆé€‚çš„æ–‡æœ¬å†…å®¹
            candidates = []
            for text in strings:
                if not text:
                    continue
                
                cleaned = self.clean_text_content(text)
                if cleaned and len(cleaned) >= 1:
                    # è®¡ç®—æ–‡æœ¬è´¨é‡è¯„åˆ†
                    score = self.calculate_text_quality(cleaned)
                    candidates.append((cleaned, score, len(cleaned)))
            
            if not candidates:
                return ""
            
            # æŒ‰è´¨é‡è¯„åˆ†å’Œé•¿åº¦æ’åºï¼Œé€‰æ‹©æœ€ä½³å€™é€‰
            candidates.sort(key=lambda x: (x[1], x[2]), reverse=True)
            return candidates[0][0]
            
        except Exception:
            return ""
    
    def is_encoded_data(self, text: str) -> bool:
        """æ£€æµ‹æ˜¯å¦ä¸ºç¼–ç æ•°æ®ï¼ˆQQçš„å›¾ç‰‡/æ–‡ä»¶ç¼–ç ï¼‰"""
        if not text or len(text) < 8:
            return False
        
        text = text.strip()
        
        # QQå¸¸è§çš„ç¼–ç æ•°æ®æ¨¡å¼
        qq_encoded_patterns = [
            # åè¿›åˆ¶æ•°å­—å¼€å¤´çš„ç¼–ç ï¼ˆé€šå¸¸æ˜¯å›¾ç‰‡IDï¼‰
            lambda s: len(s) > 20 and s.startswith(('10P', '20P', '30P')) and any(c.isalnum() for c in s[3:]),
            # base64ç±»ä¼¼çš„ç¼–ç 
            lambda s: len(s) > 20 and all(c in 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/=_-' for c in s),
            # çº¯æ•°å­—ID
            lambda s: len(s) > 15 and s.isdigit(),
            # åå…­è¿›åˆ¶ç¼–ç 
            lambda s: len(s) > 16 and all(c in '0123456789abcdefABCDEF' for c in s),
            # æ··åˆå­—æ¯æ•°å­—æ— æ„ä¹‰å­—ç¬¦ä¸²
            lambda s: len(s) > 12 and sum(1 for c in s if c.isalnum()) / len(s) > 0.8 and not any(word in s.lower() for word in ['neo', 'phone', 'user', 'file', 'path'])
        ]
        
        return any(pattern(text) for pattern in qq_encoded_patterns)
    
    def contains_file_path(self, text: str) -> bool:
        """æ£€æµ‹æ˜¯å¦åŒ…å«æ–‡ä»¶è·¯å¾„"""
        if not text:
            return False
        
        path_indicators = [
            'C:\\', 'D:\\', 'E:\\', 'F:\\',  # Windowsè·¯å¾„
            '/home/', '/usr/', '/var/', '/tmp/',  # Linuxè·¯å¾„ 
            'Documents', 'Desktop', 'Downloads',  # å¸¸è§æ–‡ä»¶å¤¹
            'Tencent Files', 'QQ',  # QQç›¸å…³è·¯å¾„
            'NTOS', 'Full::'  # QQç³»ç»Ÿæ ‡è¯†ç¬¦
        ]
        
        return any(indicator in text for indicator in path_indicators)
    
    def is_qq_download_link(self, text: str) -> bool:
        """æ£€æµ‹æ˜¯å¦ä¸ºQQä¸‹è½½é“¾æ¥æˆ–æ–‡ä»¶ä¼ è¾“é“¾æ¥"""
        if not text:
            return False
        
        # QQæ–‡ä»¶ä¸‹è½½é“¾æ¥æ¨¡å¼
        qq_download_patterns = [
            # æ ‡å‡†ä¸‹è½½é“¾æ¥
            'download?appid=',
            'fileid=',
            '&spec=',
            # å…¶ä»–QQæ–‡ä»¶ç›¸å…³æ¨¡å¼
            'qq.com/file/',
            'qzone.qq.com/',
            'weiyun.com/',
            # æ–‡ä»¶ä¼ è¾“ç›¸å…³
            'appid=1406',  # QQå¸¸ç”¨åº”ç”¨ID
            'EhR', 'EhQ',   # QQæ–‡ä»¶IDå¸¸ç”¨å‰ç¼€
        ]
        
        # æ£€æŸ¥æ˜¯å¦åŒ¹é…ä¸‹è½½é“¾æ¥æ¨¡å¼
        if any(pattern in text for pattern in qq_download_patterns):
            return True
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºURLæ¨¡å¼ï¼ˆä½†ä¸æ˜¯æ­£å¸¸ç½‘å€ï¼‰
        if ('?' in text and '&' in text and '=' in text and 
            len(text) > 50 and 
            not any(domain in text.lower() for domain in ['http://', 'https://', 'www.'])):
            return True
        
        return False
    
    def is_media_content(self, text: str, blob_data: bytes = None) -> bool:
        """æ£€æµ‹æ˜¯å¦ä¸ºåª’ä½“å†…å®¹ï¼ˆå›¾ç‰‡ã€è¡¨æƒ…åŒ…ã€è¯­éŸ³ç­‰ï¼‰"""
        if not text:
            return True
        
        # å¸¸è§çš„åª’ä½“å†…å®¹æ ‡è¯†ç¬¦
        media_indicators = [
            '[å›¾ç‰‡]', '[è¡¨æƒ…]', '[Pic]', '[å›¾]', '[Image]',
            '[è¯­éŸ³]', '[Voice]', '[éŸ³é¢‘]', '[Audio]',
            '[è§†é¢‘]', '[Video]', '[æ–‡ä»¶]', '[File]',
            '[åŠ¨ç”»è¡¨æƒ…]', '[Sticker]', '[è´´çº¸]',
            'data:image/', '.jpg', '.png', '.gif', '.jpeg', '.webp',
            '.mp3', '.wav', '.mp4', '.avi'
        ]
        
        # æ£€æŸ¥æ–‡æœ¬ä¸­æ˜¯å¦åŒ…å«åª’ä½“æ ‡è¯†ç¬¦
        if any(indicator in text for indicator in media_indicators):
            return True
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºQQä¸‹è½½é“¾æ¥
        if self.is_qq_download_link(text):
            return True
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºç¼–ç æ•°æ®
        if self.is_encoded_data(text):
            return True
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«æ–‡ä»¶è·¯å¾„
        if self.contains_file_path(text):
            return True
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å¤§é‡éå¯è¯»å­—ç¬¦ï¼ˆå¯èƒ½æ˜¯ç¼–ç åçš„åª’ä½“æ•°æ®ï¼‰
        non_printable = sum(1 for c in text if not c.isprintable() or ord(c) < 32)
        if len(text) > 10 and non_printable / len(text) > 0.3:
            return True
        
        return False
    
    def determine_message_type(self, content: str, blob_data: bytes) -> str:
        """åˆ¤æ–­æ¶ˆæ¯ç±»å‹"""
        if not content:
            return "system"
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºQQä¸‹è½½é“¾æ¥ï¼ˆæ–‡ä»¶ä¼ è¾“ï¼‰
        if self.is_qq_download_link(content):
            return "file"
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºè¯­éŸ³æ¶ˆæ¯
        if any(indicator in content for indicator in ['[è¯­éŸ³]', '[Voice]', '[éŸ³é¢‘]', '[Audio]']):
            return "voice"
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºè§†é¢‘æ¶ˆæ¯
        if any(indicator in content for indicator in ['[è§†é¢‘]', '[Video]']):
            return "video"
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºæ–‡ä»¶æ¶ˆæ¯
        if any(indicator in content for indicator in ['[æ–‡ä»¶]', '[File]']):
            return "file"
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºå›¾ç‰‡æˆ–è¡¨æƒ…
        if any(indicator in content for indicator in ['[å›¾ç‰‡]', '[è¡¨æƒ…]', '[Pic]', '[å›¾]', '[Image]', '[åŠ¨ç”»è¡¨æƒ…]', '[Sticker]', '[è´´çº¸]']):
            return "image"
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºåª’ä½“å†…å®¹
        if self.is_media_content(content, blob_data):
            return "media"
        
        # é»˜è®¤ä¸ºæ–‡æœ¬æ¶ˆæ¯
        return "text"
    
    def format_timestamp(self, timestamp: int) -> str:
        """æ ¼å¼åŒ–æ—¶é—´æˆ³"""
        try:
            return datetime.fromtimestamp(timestamp).strftime('%Y-%m-%d %H:%M:%S')
        except (ValueError, OSError):
            return ""
    
    def get_all_peers(self) -> List[int]:
        """è·å–æ‰€æœ‰å¯¹è¯å¯¹è±¡çš„QQå·"""
        try:
            self.db_connector.connect()
            peers = self.db_connector.query("SELECT DISTINCT `40030` FROM c2c_msg_table WHERE `40030` IS NOT NULL")
            return [peer[0] for peer in peers]
        except Exception as e:
            logger.error(f"è·å–å¯¹è¯å¯¹è±¡å¤±è´¥: {e}")
            return []
        finally:
            if self.db_connector.conn:
                self.db_connector.conn.close()
    
    def parse_peer_messages(self, peer_qq: int) -> List[Dict[str, Any]]:
        """è§£ææŒ‡å®šå¯¹è±¡çš„æ‰€æœ‰æ¶ˆæ¯"""
        messages = []
        
        try:
            self.db_connector.connect()
            
            # æŸ¥è¯¢è¯¥QQå·çš„æ‰€æœ‰æ¶ˆæ¯
            rows = self.db_connector.query("""
                SELECT `40050`, `40030`, `40033`, `40800`
                FROM c2c_msg_table
                WHERE `40030` = ? AND `40800` IS NOT NULL
                ORDER BY `40050` ASC
            """, (peer_qq,))
            
            ai_qq = config.get("qq_number_ai")
            try:
                ai_qq_int = int(ai_qq) if ai_qq is not None else None
            except (ValueError, TypeError):
                logger.warning(f"AI QQå·é…ç½®æ— æ•ˆ: {ai_qq}")
                ai_qq_int = None
            
            if ai_qq_int is None:
                logger.warning(
                    f"å¤„ç†QQå·{peer_qq}æ—¶ï¼ŒAI QQå·æœªé…ç½®æˆ–æ— æ•ˆï¼Œæ‰€æœ‰æ¶ˆæ¯å°†è¢«æ ‡è®°ä¸ºis_sender=0"
                )
            else:
                logger.info(f"å¤„ç†QQå·{peer_qq}ï¼Œé…ç½®çš„AI QQå·: {ai_qq_int}")
            
            for idx, (timestamp, sender_30, sender_33, blob_data) in enumerate(rows):
                if not blob_data:
                    continue
                
                # æå–æ–‡æœ¬å†…å®¹
                content = self.extract_text_content(blob_data)
                
                # ä½¿ç”¨é›†æˆçš„å¢å¼ºè¿‡æ»¤å™¨è¿›è¡ŒéªŒè¯
                if not content:
                    continue
                    
                # å»é™¤é¦–å°¾ç©ºæ ¼
                content = content.strip()
                
                # ä½¿ç”¨é›†æˆçš„å¢å¼ºè¿‡æ»¤å™¨æ£€æŸ¥æ–‡æœ¬æœ‰æ•ˆæ€§
                if not self.is_enhanced_valid_text(content):
                    logger.debug(f"è·³è¿‡æ— æ•ˆæˆ–ä¹±ç æ–‡æœ¬: {repr(content[:50])}...")
                    continue
                
                # é¢å¤–æ£€æŸ¥åª’ä½“å†…å®¹
                if self.is_media_content(content, blob_data):
                    logger.debug(f"è·³è¿‡åª’ä½“å†…å®¹: {content[:50]}...")
                    continue
                
                # åˆ¤æ–­å‘é€è€…
                # sender_33 æ˜¯æ¶ˆæ¯å‘é€è€…çš„QQå·
                # å¦‚æœ sender_33 == ai_qq_intï¼Œè¯´æ˜AIå‘é€çš„æ¶ˆæ¯ï¼Œ is_sender = 1
                # å¦åˆ™æ˜¯å¯¹æ–¹å‘é€çš„æ¶ˆæ¯ï¼Œ is_sender = 0
                is_sender = 1 if (ai_qq_int is not None and sender_33 == ai_qq_int) else 0
                talker = str(sender_33)
                
                # è°ƒè¯•ä¿¡æ¯ï¼šè¾“å‡ºåˆ¤æ–­é€»è¾‘
                if ai_qq_int is not None:
                    logger.debug(f"å‘é€è€…QQ: {sender_33}, AI_QQ: {ai_qq_int}, is_sender: {is_sender}")
                
                # åˆ¤æ–­æ¶ˆæ¯ç±»å‹
                message_type = self.determine_message_type(content, blob_data)
                
                message = {
                    'id': idx + 1,
                    'MsgSvrID': f"{timestamp}_{sender_33}",
                    'type_name': message_type,
                    'is_sender': is_sender,
                    'talker': talker,
                    'msg': content,
                    'src': '',  # QQæ•°æ®åº“ä¸­åª’ä½“æ–‡ä»¶è·¯å¾„éœ€è¦å•ç‹¬å¤„ç†
                    'CreateTime': self.format_timestamp(timestamp),
                    'room_name': f'QQ_{peer_qq}',
                    'is_forward': 0  # è½¬å‘æ¶ˆæ¯æ£€æµ‹éœ€è¦æ›´å¤æ‚çš„é€»è¾‘
                }
                
                messages.append(message)
        
        except Exception as e:
            logger.error(f"è§£æQQå·{peer_qq}çš„æ¶ˆæ¯å¤±è´¥: {e}")
        
        finally:
            if self.db_connector.conn:
                self.db_connector.conn.close()
        
        return messages
    
    def save_to_csv(self, messages: List[Dict[str, Any]], peer_qq: int):
        """ä¿å­˜æ¶ˆæ¯åˆ°CSVæ–‡ä»¶"""
        if not messages:
            return
        
        # åˆ›å»ºå¯¹è¯å¯¹è±¡ç›®å½•
        peer_dir = self.output_dir / f"QQ_{peer_qq}"
        peer_dir.mkdir(parents=True, exist_ok=True)
        
        # CSVæ–‡ä»¶è·¯å¾„
        csv_file = peer_dir / f"QQ_{peer_qq}_chat.csv"
        
        # CSVå­—æ®µ
        fieldnames = [
            'id', 'MsgSvrID', 'type_name', 'is_sender', 'talker', 
            'msg', 'src', 'CreateTime', 'room_name', 'is_forward'
        ]
        
        try:
            with open(csv_file, 'w', newline='', encoding='utf-8') as csvfile:
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                writer.writeheader()
                writer.writerows(messages)
            
            logger.info(f"æˆåŠŸä¿å­˜ {len(messages)} æ¡æ¶ˆæ¯åˆ° {csv_file}")
        
        except Exception as e:
            logger.error(f"ä¿å­˜CSVæ–‡ä»¶å¤±è´¥: {e}")
    
    def parse_all(self):
        """è§£ææ‰€æœ‰å¯¹è¯æ•°æ®å¹¶è½¬æ¢ä¸ºCSVæ ¼å¼"""
        logger.info("å¼€å§‹è§£æQQèŠå¤©æ•°æ®...")
        
        # è·å–æ‰€æœ‰å¯¹è¯å¯¹è±¡
        peers = self.get_all_peers()
        if not peers:
            logger.warning("æœªæ‰¾åˆ°ä»»ä½•å¯¹è¯æ•°æ®")
            return
        
        logger.info(f"æ‰¾åˆ° {len(peers)} ä¸ªå¯¹è¯å¯¹è±¡")
        
        total_messages = 0
        for peer_qq in peers:
            logger.info(f"æ­£åœ¨å¤„ç†QQå·: {peer_qq}")
            
            # è§£æè¯¥å¯¹è±¡çš„æ¶ˆæ¯
            messages = self.parse_peer_messages(peer_qq)
            
            if messages:
                # ä¿å­˜åˆ°CSV
                self.save_to_csv(messages, peer_qq)
                total_messages += len(messages)
            else:
                logger.warning(f"QQå· {peer_qq} æ²¡æœ‰æœ‰æ•ˆæ¶ˆæ¯")
        
        logger.info(f"è§£æå®Œæˆï¼Œæ€»å…±å¤„ç†äº† {total_messages} æ¡æ¶ˆæ¯")
    
    # ========== å¢å¼ºæ–‡æœ¬è¿‡æ»¤å™¨æ–¹æ³• ==========
    
    def contains_problematic_unicode(self, text: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦åŒ…å«é—®é¢˜Unicodeå­—ç¬¦"""
        for char in text:
            char_code = ord(char)
            for start, end in self.problematic_unicode_ranges:
                if start <= char_code <= end:
                    return True
        return False
    
    def matches_garbage_pattern(self, text: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦åŒ¹é…å·²çŸ¥çš„ä¹±ç æ¨¡å¼"""
        # ç›´æ¥å­—ç¬¦ä¸²åŒ¹é…
        for pattern in self.known_garbage_patterns:
            if pattern in text:
                return True
        
        # æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…  
        for pattern in self.regex_patterns:
            if re.search(pattern, text):
                return True
        
        return False
    
    def has_excessive_special_chars(self, text: str) -> bool:
        """æ£€æŸ¥ç‰¹æ®Šå­—ç¬¦æ¯”ä¾‹æ˜¯å¦è¿‡é«˜"""
        if len(text) <= 2:
            return False
            
        special_chars = sum(1 for c in text if not c.isalnum() and not c.isspace() and ord(c) < 256)
        return special_chars / len(text) > 0.4
    
    def is_enhanced_garbled_text(self, text: str) -> bool:
        """å¢å¼ºçš„ä¹±ç æ£€æµ‹"""
        if not text or not text.strip():
            return True
        
        text = text.strip()
        
        # æ£€æŸ¥å·²çŸ¥ä¹±ç æ¨¡å¼
        if self.matches_garbage_pattern(text):
            return True
        
        # æ£€æŸ¥é—®é¢˜Unicodeå­—ç¬¦
        if self.contains_problematic_unicode(text):
            return True
        
        # æ£€æŸ¥éå¯æ‰“å°å­—ç¬¦
        non_printable = sum(1 for c in text if not c.isprintable())
        if non_printable > 0:
            return True
        
        # æ£€æŸ¥ç‰¹æ®Šå­—ç¬¦æ¯”ä¾‹
        if self.has_excessive_special_chars(text):
            return True
        
        # å¯¹äºçŸ­å­—ç¬¦ä¸²ï¼Œæ›´ä¸¥æ ¼çš„éªŒè¯
        if len(text) <= 10:
            meaningful_chars = sum(1 for c in text if c.isalpha() or c.isdigit() or '\u4e00' <= c <= '\u9fff')
            if meaningful_chars / len(text) < 0.8:
                return True
        
        return False
    
    def is_enhanced_valid_text(self, text: str) -> bool:
        """å¢å¼ºçš„æ–‡æœ¬æœ‰æ•ˆæ€§éªŒè¯"""
        if not text or not text.strip():
            return False
        
        text = text.strip()
        
        # é¦–å…ˆæ£€æŸ¥æ˜¯å¦ä¸ºä¹±ç 
        if self.is_enhanced_garbled_text(text):
            return False
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«æœ‰æ„ä¹‰çš„å­—ç¬¦
        meaningful_chars = sum(1 for c in text if c.isalpha() or c.isdigit() or '\u4e00' <= c <= '\u9fff')
        if meaningful_chars == 0:
            return False
        
        # æ£„æŸ¥å¸¸ç”¨è¯æ±‡
        common_words = [
            # ä¸­æ–‡å¸¸ç”¨è¯ï¼ˆæ‰©å±•ç‰ˆï¼‰
            '\u4f60', '\u6211', '\u4ed6', '\u5979', '\u662f', '\u4e0d', '\u6709', '\u7684', '\u4e86', '\u5728', '\u90fd', '\u53ef\u4ee5', '\u4e5f',
            '\u600e\u4e48', '\u4ec0\u4e48', '\u4e3a\u4ec0\u4e48', '\u5bf9', '\u597d', '\u884c', '\u7684\u786e', '\u771f\u5047', '\u8fd9', '\u90a3',
            '\u592a', '\u70b9', '\u4e86', '\u90a3\u4e2a', '\u8bd5\u8bd5', '\u80fd\u4e0d\u80fd', '\u54e6\u4e0d\u5bf9', '\u6ca1\u4e8b', '\u53ef\u4ee5',
            '\u5f00\u6e90', '\u547d\u4ee4\u884c', '\u52a8\u753b', '\u54c6', '\u8fd9\u4e48\u5927', '\u56e0\u4e3a', '\u5199', '\u7684\u786e',
            # è‹±æ–‡å¸¸ç”¨è¯
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with',
            'how', 'what', 'why', 'when', 'where', 'who', 'yes', 'no', 'ok', 'good', 'bad',
            'idk', 'sketch', 
            # æŠ€æœ¯ç›¸å…³
            'qt5', 'qt', 'pyqt', 'neo', 'flux'
        ]
        
        text_lower = text.lower()
        contains_common_word = any(word.lower() in text_lower for word in common_words)
        
        # å¦‚æœåŒ…å«å¸¸ç”¨è¯æ±‡ï¼Œåˆ™è®¤ä¸ºæ˜¯æœ‰æ„ä¹‰çš„
        if contains_common_word:
            return True
        
        # æœ€åçš„éªŒè¯ï¼šè‡³å°‘3ä¸ªæœ‰æ„ä¹‰å­—ç¬¦ä¸”ç‰¹æ®Šå­—ç¬¦æ¯”ä¾‹ä¸é«˜
        if meaningful_chars >= 3:
            special_chars = sum(1 for c in text if not c.isalnum() and not c.isspace())
            return special_chars / len(text) < 0.3
        
        return False

def main():
    """ä¸»å‡½æ•°"""
    # ä»é…ç½®è·å–QQæ•°æ®åº“è·¯å¾„
    db_path = config.get('qq_db_path')
    if not db_path or not os.path.exists(db_path):
        logger.error("QQæ•°æ®åº“è·¯å¾„æœªé…ç½®æˆ–æ–‡ä»¶ä¸å­˜åœ¨")
        return
    
    # åˆ›å»ºè§£æå™¨å¹¶æ‰§è¡Œè§£æ
    parser = QQParser(db_path)
    parser.parse_all()

if __name__ == "__main__":
    main()